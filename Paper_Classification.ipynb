{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMiqCnrm7iEwov6pswfhUcu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonisGantzos/ICDAR-Paper-Classification/blob/main/Paper_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper Classification Project\n",
        "The goal of this project is to extrac information regarding the title and author(s) from papers and subsequenlty classify them into one of the following categories :\n",
        "- Tables\n",
        "- Classification\n",
        "- Key Information Extraction\n",
        "- Optical Character Recognition\n",
        "- Datasets\n",
        "- Document Layout Understanding\n",
        "- Others\n",
        "\n",
        "After analyzing the abstract and the title for each pdf, we will be adding the following 3 categories to the already established ones, in order to make the classification a bit more specific\n",
        "- Deep Learning Models\n",
        "- Multimodal Document Analysis\n",
        "- Handwriting Recognition"
      ],
      "metadata": {
        "id": "xDSxeSQ4hoi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1 : Retrieval, Visualization and Extraction of our data"
      ],
      "metadata": {
        "id": "nISoMyb4ihkR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OBGUkBGffoVe"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import pymupdf\n",
        "except:\n",
        "  !pip install pymupdf\n",
        "  import pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the file containing the subset\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "data_path=Path(\"/data\")\n",
        "\n",
        "pdf_path=data_path / \"pdf_data\"\n",
        "\n",
        "#if the folder does not exist download it and prepare it\n",
        "if pdf_path.is_dir():\n",
        "  print(f\"{pdf_path} directory already exists\")\n",
        "else:\n",
        "  print(f\"{pdf_path} does not exist, creating directory...\")\n",
        "  pdf_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# Download zip file containing pdf data\n",
        "with open(data_path / \"ICDAR2024_papers.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/Captonomy/assessment/raw/refs/heads/main/ml-engineer/ICDAR2024_papers.zip\")\n",
        "    print(\"Downloading pdf data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "# Unzip file\n",
        "with zipfile.ZipFile(data_path / \"ICDAR2024_papers.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pdf data...\")\n",
        "    zip_ref.extractall(pdf_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnHan8u6Jyfx",
        "outputId": "28232b28-86a3-4e87-85b1-ab69f51edfa1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/data/pdf_data directory already exists\n",
            "Downloading pdf data...\n",
            "Unzipping pdf data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we have to have a good understanding of our dataset before we start building our model\n",
        "import os\n",
        "def walk_through_dir(dir_path) :\n",
        "  #walks thorugh a dir path returning its contents\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(filenames)} pdfs in {dirpath} \")\n",
        "\n",
        "\n",
        "walk_through_dir(pdf_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhDAXVsLK_Dn",
        "outputId": "e2204487-67ac-46e6-d68f-b955a4801095"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 0 pdfs in /data/pdf_data \n",
            "There are 50 pdfs in /data/pdf_data/Data \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list all pdf paths\n",
        "import pathlib\n",
        "pdf_paths=list(pdf_path.glob(\"*/*.pdf\"))\n",
        "print(pdf_paths[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUacDQaDr0NR",
        "outputId": "ec5614d5-f3ee-4661-f8f4-398d805c6e22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PosixPath('/data/pdf_data/Data/0049.pdf'), PosixPath('/data/pdf_data/Data/0044.pdf'), PosixPath('/data/pdf_data/Data/0086.pdf'), PosixPath('/data/pdf_data/Data/0020.pdf'), PosixPath('/data/pdf_data/Data/0089.pdf'), PosixPath('/data/pdf_data/Data/0083.pdf'), PosixPath('/data/pdf_data/Data/0027.pdf'), PosixPath('/data/pdf_data/Data/0045.pdf'), PosixPath('/data/pdf_data/Data/0060.pdf'), PosixPath('/data/pdf_data/Data/0051.pdf')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Iterate through our data\n",
        "Before proceeding witht the extraction lets view the contents of each .pdf file to get an idea of the general shape and look of our data"
      ],
      "metadata": {
        "id": "x__8nVtHsB4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_pdf(pdf_path):\n",
        "  doc = pymupdf.open(pdf_path) # open a document\n",
        "  out = open(\"output.txt\", \"wb\") # create a text output\n",
        "  for page in doc: # iterate the document pages\n",
        "      text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
        "      print(text)\n",
        "      out.write(text) # write text of page\n",
        "      out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
        "      lines = text.splitlines()\n",
        "\n",
        "      # Assuming the title is in the first line and authors in the next few lines\n",
        "      title = lines[0:2]\n",
        "      authors = lines[2:5]\n",
        "      abstract = lines[3:25]\n",
        "\n",
        "      print(f\"Title: {title}\")\n",
        "      print(f\"Authors: {authors}\")\n",
        "      print(f\"Abstract: {abstract}\")\n",
        "\n",
        "  out.close()"
      ],
      "metadata": {
        "id": "ePIMPsRMRckz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#take all pdf paths and add them in a list\n",
        "import random\n",
        "#set seed to control the random result (optional)\n",
        "random.seed(22)\n",
        "#pick one of the pdf paths in the list in random\n",
        "random_pdf_path=random.choice(pdf_paths)\n",
        "#show contents of the pdf file\n",
        "iterate_pdf(random_pdf_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ5dIF86LUnl",
        "outputId": "4cfa077c-5b69-4fcf-89e5-be48634a7442"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Multi-Cell Decoder and Mutual Learning for\\nTable Structure and Character Recognition\\nTakaya Kawakatsui[0000-0003-1285-2748]\\nPreferred Networks, Inc., 1-6-1 Otemachi, Chiyoda, Tokyo, Japan.\\nkat.nii.ac.jp@gmail.com\\nhttps://researchmap.jp/t.kat\\nAbstract. Extracting table contents from documents such as scientific\\npapers and financial reports and converting them into a format that can\\nbe processed by large language models is an important task in knowledge\\ninformation processing. End-to-end approaches, which recognize not only\\ntable structure but also cell contents, achieved performance comparable\\nto state-of-the-art models using external character recognition systems,\\nand have potential for further improvements. In addition, these models\\ncan now recognize long tables with hundreds of cells by introducing local\\nattention. However, the models recognize table structure in one direction\\nfrom the header to the footer, and cell content recognition is performed\\nindependently for each cell, so there is no opportunity to retrieve useful\\ninformation from the neighbor cells. In this paper, we propose a multi-cell\\ncontent decoder and bidirectional mutual learning mechanism to improve\\nthe end-to-end approach. The effectiveness is demonstrated on two large\\ndatasets, and the experimental results show comparable performance to\\nstate-of-the-art models, even for long tables with large numbers of cells.\\nKeywords: Deep Learning, Table Recognition, Transformer, Mutual Learning\\n1\\nIntroduction\\nInformation retrieval technology which provides high-quality knowledge to large\\nlanguage models (LLMs) is attracting attention. Many researchers have worked\\non converting scanned and imaged documents to machine-readable formats such\\nas HTML code [37,42,15] and LaTeX code [3,12]. This initiative has direct and\\nindirect benefits. First, because past literature remains mostly in printed form,\\nit is necessary to convert it into structured electronic documents. This is a direct\\nbenefit. Second, the realization of intelligence which recognizes hidden meanings\\nin the layout of published documents that are intended to be read by humans is\\nimportant from the perspective of human-machine interaction. This is an indirect\\nbenefit.\\nIn this paper, we will focus on table recognition, which includes two types of\\ntasks, namely structure recognition and cell content recognition. A simple table\\nhas horizontal and vertical borders, and each cell contains characters. Complex\\n'\n",
            "Title: [b'Multi-Cell Decoder and Mutual Learning for', b'Table Structure and Character Recognition']\n",
            "Authors: [b'Takaya Kawakatsui[0000-0003-1285-2748]', b'Preferred Networks, Inc., 1-6-1 Otemachi, Chiyoda, Tokyo, Japan.', b'kat.nii.ac.jp@gmail.com']\n",
            "Abstract: [b'Preferred Networks, Inc., 1-6-1 Otemachi, Chiyoda, Tokyo, Japan.', b'kat.nii.ac.jp@gmail.com', b'https://researchmap.jp/t.kat', b'Abstract. Extracting table contents from documents such as scientific', b'papers and financial reports and converting them into a format that can', b'be processed by large language models is an important task in knowledge', b'information processing. End-to-end approaches, which recognize not only', b'table structure but also cell contents, achieved performance comparable', b'to state-of-the-art models using external character recognition systems,', b'and have potential for further improvements. In addition, these models', b'can now recognize long tables with hundreds of cells by introducing local', b'attention. However, the models recognize table structure in one direction', b'from the header to the footer, and cell content recognition is performed', b'independently for each cell, so there is no opportunity to retrieve useful', b'information from the neighbor cells. In this paper, we propose a multi-cell', b'content decoder and bidirectional mutual learning mechanism to improve', b'the end-to-end approach. The effectiveness is demonstrated on two large', b'datasets, and the experimental results show comparable performance to', b'state-of-the-art models, even for long tables with large numbers of cells.', b'Keywords: Deep Learning, Table Recognition, Transformer, Mutual Learning', b'1', b'Introduction']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction\n",
        "We will be using the function below to store all relative metadata (title, authors) from each file, and store it in a dictionary"
      ],
      "metadata": {
        "id": "a83nkkAosd2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_metadata(pdf_paths):\n",
        "  text_metadata = {}\n",
        "  i=0\n",
        "  for pdf_path in pdf_paths:\n",
        "      # Open the PDF file\n",
        "      doc = pymupdf.open(pdf_path) # open a document\n",
        "      out = open(\"output.txt\", \"wb\") # create a text output\n",
        "      for page in doc: # iterate the document pages\n",
        "          text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
        "          print(text)\n",
        "          print(f\"metadata {doc.metadata}\")\n",
        "          out.write(text) # write text of page\n",
        "          out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
        "          lines = text.splitlines()\n",
        "\n",
        "          # Assuming the title is in the first line and authors in the next few lines\n",
        "          title = lines[0:2]\n",
        "          authors = lines[2:5]\n",
        "\n",
        "          # Decode bytes to strings\n",
        "          title = [x.decode('utf-8') if isinstance(x, bytes) else x for x in title]\n",
        "          authors = [x.decode('utf-8') if isinstance(x, bytes) else x for x in authors]\n",
        "\n",
        "          print(f\"Title: {title}\")\n",
        "          print(f\"Authors: {authors}\")\n",
        "\n",
        "      print(f\"Authors: {', '.join(authors)} \\n\")\n",
        "      #add author and title in a dictioanry\n",
        "      text_metadata[i] = {'id':pdf_path.name, 'title': ' '.join(title), 'authors': ', '.join(authors)}\n",
        "      i+=1\n",
        "\n",
        "  return text_metadata"
      ],
      "metadata": {
        "id": "tJRnEmkozNaz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_metadata = get_text_metadata(pdf_paths)\n",
        "text_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_9zF_z3zTf8",
        "outputId": "c46abf89-5f0a-4419-a101-5ea7b6a467e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"Synthesizing Realistic Data for Table\\nRecognition\\nQiyu Houl [0009-0009-4150-9907], Jun wangl [0000-0002-9515-076X] P.A/) Meixuan\\nQiao2, and Lujun Tianl\\n1 iWudao Tech\\nfhougyjwang,tianljlftwudao.tech\\n2 Huazhong University of Science and Technology\\ncliaomeixuanelhust.edu.cn\\nAbstract. To overcome the limitations and challenges of current au-\\ntomatic table data annotation methods and random table data synthe-\\nsis approaches, we propose a novel method for synthesizing annotation\\ndata specifically designed for table recognition. This method utilizes the\\nstructure and content of existing complex tables, facilitating the efficient\\ncreation of tables that closely replicate the authentic styles found in the\\ntarget domain. By leveraging the actual structure and content of tables\\nfrom Chinese financial announcements, we have developed the first ex-\\ntensive table annotation dataset in this domain. We used this dataset\\nto train several recent deep learning-based end-to-end table recognition\\nmodels. Additionally, we have established the inaugural benchmark for\\nreal-world complex tables in the Chinese financial announcement do-\\nmain, using it to assess the performance of models trained on our syn-\\nthetic data, thereby effectively validating our method's practicality and\\neffectiveness. Furthermore, we applied our synthesis method to augment\\nthe FinTabNet dataset, extracted from English financial announcements,\\nby increasing the proportion of tables with multiple spanning cells to in-\\ntroduce greater complexity. Our experiments show that models trained\\non this augmented dataset achieve comprehensive improvements in per-\\nformance, especially in the recognition of tables with multiple spanning\\ncells.\\nKeywords: Table Data Synthesis \\xe2\\x80\\xa2 Data Augmentation \\xe2\\x80\\xa2 Table Recog-\\nnition\\n1\\nIntroduction\\nTables, serving as a vital carrier of data, are prevalent across a wide range of\\ndigital documents. They efficiently store and display data in a compact and lu-\\ncid format, encapsulating an immense volume of valuable information. However,\\nrecognizing the structures of tables within digital documents, such as PDF and\\nimages, and subsequently extracting structured data, present significant chal-\\nlenges due to the complexity and diversity of their structure and style [1]. In\\nrecent years, with the advancement of deep learning, new methodologies have\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Synthesizing Realistic Data for Table', 'Recognition']\n",
            "Authors: ['Qiyu Houl [0009-0009-4150-9907], Jun wangl [0000-0002-9515-076X] P.A/) Meixuan', 'Qiao2, and Lujun Tianl', '1 iWudao Tech']\n",
            "Authors: Qiyu Houl [0009-0009-4150-9907], Jun wangl [0000-0002-9515-076X] P.A/) Meixuan, Qiao2, and Lujun Tianl, 1 iWudao Tech \n",
            "\n",
            "b'1\\nDLAFormer: An End-to-End Transformer For\\nDocument Layout Analysis\\nJiawei Wang1,2,*,t, Kai Hui ,2,*, t , and Qiang Huo2\\nDepartment of EEIS, University of Science and Technology of China, Hefei, China\\n2 Microsoft Research Asia, Beijing, China\\nwangjiawei@mail.ustc.edu.cn, hk970213@mail.ustc.edu.cn,\\nclianghuo@microsoft.com\\nAbstract. Document layout analysis (DLA) is crucial for understand-\\ning the physical layout and logical structure of documents, serving in-\\nformation retrieval, document summarization, knowledge extraction, etc.\\nHowever, previous studies have typically used separate models to address\\nindividual sub-tasks within DLA, including table/figure detection, text\\nregion detection, logical role classification, and reading order prediction.\\nIn this work, we propose an end-to-end transformer-based approach for\\ndocument layout analysis, called DLAFormer, which integrates all these\\nsub-tasks into a single model. To achieve this, we treat various DLA\\nsub-tasks (such as text region detection, logical role classification, and\\nreading order prediction) as relation prediction problems and consoli-\\ndate these relation prediction labels into a unified label space, allowing a\\nunified relation prediction module to handle multiple tasks concurrently.\\nAdditionally, we introduce a novel set of type-wise queries to enhance\\nthe physical meaning of content queries in DETR. Moreover, we adopt\\na coarse-to-fine strategy to accurately identify graphical page objects.\\nExperimental results demonstrate that our proposed DLAFormer out-\\nperforms previous approaches that employ multi-branch or multi-stage\\narchitectures for multiple tasks on two document layout analysis bench-\\nmarks, DocLayNet and Comp-HRDoc.\\nKeywords: Document Layout Analysis \\xe2\\x80\\xa2 Relation Prediction \\xe2\\x80\\xa2 Unified\\nLabel Space\\n1\\nIntroduction\\nDocument layout analysis (DLA) is the process of recovering document physical\\nand logical structures from document images, including physical layout analysis\\nand logical structure analysis [6]. Given input document images, physical lay-\\nout analysis aims at identifying physical homogeneous regions of interest (also\\ncalled page objects), such as graphical page objects like tables, figures, mathe-\\nmatical formulas, and different types of text regions. Logical structure analysis\\n*Work done when Jiawei Wang and Kai Hu were interns at Multi-Modal Interaction\\nGroup, Microsoft Research Asia, Beijing, China.\\ntEqual contribution. Correspondence to wangj iawei@mail . ustc . edu . cn.\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['1', 'DLAFormer: An End-to-End Transformer For']\n",
            "Authors: ['Document Layout Analysis', 'Jiawei Wang1,2,*,t, Kai Hui ,2,*, t , and Qiang Huo2', 'Department of EEIS, University of Science and Technology of China, Hefei, China']\n",
            "Authors: Document Layout Analysis, Jiawei Wang1,2,*,t, Kai Hui ,2,*, t , and Qiang Huo2, Department of EEIS, University of Science and Technology of China, Hefei, China \n",
            "\n",
            "b'Handwritten Document Recognition Using\\nPre-trained Vision Transformers\\nDaniel Parresl [0000-0002-2078-0329] , Dan Aniteii [0000-0001 \\xe2\\x80\\x948288-6009] , and\\nRoberto Paredes1,2 [0000-0002-5192-0021]\\n1 PRHLT Research Center, Universitat Politecnica Valencia, Valencia, Spain\\n{dparres,danitei,rparedes}@prhlt.upv.es\\n2 Valencian Graduate School and Research Network of Artificial Intelligence,\\nCami de Vera s/n, 46022 Valencia, Spain\\nAbstract. Handwritten document recognition (HDR) is a rapidly grow-\\ning field that aims to automate the process of document transcription,\\nstreamlining the traditional stages of segmentation and block recognition\\ninherent in handwritten text recognition systems. However, the efficacy of\\nHDR systems often encounters performance hurdles due to limited data\\navailability, constraining their practical applicability. In addressing this\\nchallenge, our study delves into leveraging pre-trained vision transformer\\nmodels, proposing two distinct architectures. One architecture integrates\\na transformer decoder, while the other relies on a decoder-free connec-\\ntionist temporal classification approach. Consequently, we present a com-\\nprehensive analysis focused on adapting and optimizing weights to en-\\nhance performance efficiently. Our methodology encompasses exploring\\nan optimal blend of hyperparameters through grid search, implementing\\nfreezing strategies, utilizing gradient accumulation, and employing hard\\nnegative mining techniques. This analysis demonstrates that our pro-\\nposed efficient fine-tuning workflow significantly mitigates error rates and\\nenhances inference speed, surpassing state-of-the-art results across most\\nextended HDR benchmark datasets. Specifically, we outperform previous\\nmethods on the READ 2016 single-page and double-page with a word er-\\nror rate (WER) of 11.49% and 11.86%, the RIMES 2009 with a WER of\\n9.58%, and MAURDOR C3 and C4 with a WER of 16.21% and 13.32%.\\nThe results of this study have important implications for the develop-\\nment of HDR systems, where our approach can lead to improved system\\nperformance and enable wider adoption of this technology. Source code is\\npublicly available at https://github.com/dparres/Pretrained-Document-\\nRecognition-Transformers\\nKeywords: Vision Transformers \\xe2\\x80\\xa2 Handwritten Document Recognition\\n\\xe2\\x80\\xa2 Pre-trained Models \\xe2\\x80\\xa2 Connectionist Temporal Classification.\\n1\\nIntroduction\\nIn recent years, handwritten text recognition (HTR) has evolved significantly,\\nmainly due to the application of deep learning techniques. Nowadays, the most\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Handwritten Document Recognition Using', 'Pre-trained Vision Transformers']\n",
            "Authors: ['Daniel Parresl [0000-0002-2078-0329] , Dan Aniteii [0000-0001 —8288-6009] , and', 'Roberto Paredes1,2 [0000-0002-5192-0021]', '1 PRHLT Research Center, Universitat Politecnica Valencia, Valencia, Spain']\n",
            "Authors: Daniel Parresl [0000-0002-2078-0329] , Dan Aniteii [0000-0001 —8288-6009] , and, Roberto Paredes1,2 [0000-0002-5192-0021], 1 PRHLT Research Center, Universitat Politecnica Valencia, Valencia, Spain \n",
            "\n",
            "b\"Improving Automatic Text Recognition with\\nLanguage Models in the PyLaia Open-Source\\nLibrary\\nSolene Tarride[0000-0001-6174-9865]\\n-\\xe2\\x80\\x98,\\n, y oann Schneider, Marie Generali-Lince,\\nMelodie Boillet[0000-0002-0618-7852]\\n-r%ast\\xe2\\x80\\xa2\\n, nien Abadie, and Christopher\\nKermorvant[0000-0002-7508-4080]\\nTEKLIA, Paris, France\\nAbstract. PyLaia is one of the most popular open-source software for\\nAutomatic Text Recognition (ATR), delivering strong performance in\\nterms of speed and accuracy. In this paper, we outline our recent contri-\\nbutions to the PyLaia library, focusing on the incorporation of reliable\\nconfidence scores and the integration of statistical language modeling\\nduring decoding. Our implementation provides an easy way to combine\\nPyLaia with n-grams language models at different levels. One of the high-\\nlights of this work is that language models are completely auto-tuned:\\nthey can be built and used easily without any expert knowledge, and\\nwithout requiring any additional data. To demonstrate the significance of\\nour contribution, we evaluate PyLaia's performance on twelve datasets,\\nboth with and without language modelling. The results show that decod-\\ning with small language models improves the Word Error Rate by 13%\\nand the Character Error Rate by 12% in average. Additionally, we con-\\nduct an analysis of confidence scores and highlight the importance of cal-\\nibration techniques. Our implementation is publicly available in the offi-\\ncial PyLaia repository (https://gitlab.teklia.com/atr/pylaia), and\\ntwelve open-source models are released on Hugging Face'.\\nKeywords: Automatic Text Recognition, Neural Networks, Language\\nModels, Open-source Software\\n1\\nIntroduction\\nThe scope of Automatic Text Recognition (ATR) has expanded significantly in\\nrecent years as deep learning models have become more robust and efficient.\\nThese advances have contributed to the development of easy-to-use web plat-\\nforms, such as Transkribus2, Transkriptorium3, eScriptorium4 or Arkindex5, al-\\nlowing non machine-learning experts to train deep learning models and process\\n1 https : //huggingface . co/collections/Teklia/pylaia-65f 16e9ae0aa03690e9e9f80\\n2 https://readcoop.eu/transkribus/\\n3 http://www.transkriptorium.com/\\n4 https://escriptorium.fr/\\n5 https://doc.arkindex.org/\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Improving Automatic Text Recognition with', 'Language Models in the PyLaia Open-Source']\n",
            "Authors: ['Library', 'Solene Tarride[0000-0001-6174-9865]', '-‘,']\n",
            "Authors: Library, Solene Tarride[0000-0001-6174-9865], -‘, \n",
            "\n",
            "b'Global-SEG: Text Semantic Segmentation Based\\non Global Semantic Pair Relations\\nWenjun Sunl [0009-0002-7857-8737] , Hanh Thi Hong\\nTran1,2,3 [0000-0002-5993-1630] Carlos-Emiliano\\nGonzalez-Gallardol [0000-0002-0787-2990] , mithael\\n1 [0000-0002-0123-439X] ,\\n1 [0000-0001-6160-3356]\\nCoustaty\\nand Antoine Doucet\\n1 University of La Rochelle, L3i, La Rochelle, France\\n2 Jo2ef Stefan International Postgraduate School, Ljubljana, Slovenia\\n3 Jo2ef Stefan Institute, Ljubljana, Slovenia\\nffirstname.lastname,thi.tran,carlos.gonzalez,gallardol@univ-lr.fr\\nAbstract. Text semantic segmentation is a crucial task in language un-\\nderstanding, as subsequent natural language processing tasks often re-\\nquire cohesive semantic blocks. This paper introduces a new perspective\\non this task by utilizing global semantic pair relations from both token-\\nand sentence-level language models. This approach addresses the limi-\\ntations of prior work, which concentrated solely on individual semantic\\nunits like sentences. Our model processes both local and global levels of\\nsentence semantics via encoders and then combines the semantics ob-\\ntained at each stage into a semantic embedding matrix. This matrix is\\nthen fed through a convolutional neural network and finally used as input\\nthrough another encoder. This process enables the identification of se-\\nmantic segmentation boundaries by describing the relationships of global\\nsemantic pairs. Furthermore, we utilize semantic embeddings from large\\nlanguage models and consider the positional information of text within\\nthe document to assess their efficacy in augmenting semantics. We test\\nour model with both contemporary and historical corpora, and the re-\\nsults demonstrate that our approach outperforms benchmarks on each\\ndataset.\\nKeywords: Text semantic segmentation \\xe2\\x80\\xa2 Semantic pair relation \\xe2\\x80\\xa2 His-\\ntorical documents\\n1\\nIntroduction\\nText semantic segmentation consists of analyzing the semantic relationships be-\\ntween sentences or paragraphs based on the input text and next dividing them\\ninto coherent semantic blocks [15,28]. An example is shown in Figure 1. By\\nidentifying semantics between different blocks, this task can assist in numerous\\ndownstream natural language processing (NLP) tasks, such as dialogue analysis\\n[31] and automatic summarization [20].\\nResearchers have proposed several models to address this task [1,19,18], all\\nof which perform single-sentence semantic analysis on text based on the output\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Global-SEG: Text Semantic Segmentation Based', 'on Global Semantic Pair Relations']\n",
            "Authors: ['Wenjun Sunl [0009-0002-7857-8737] , Hanh Thi Hong', 'Tran1,2,3 [0000-0002-5993-1630] Carlos-Emiliano', 'Gonzalez-Gallardol [0000-0002-0787-2990] , mithael']\n",
            "Authors: Wenjun Sunl [0009-0002-7857-8737] , Hanh Thi Hong, Tran1,2,3 [0000-0002-5993-1630] Carlos-Emiliano, Gonzalez-Gallardol [0000-0002-0787-2990] , mithael \n",
            "\n",
            "b\"Impression-CLIP: Contrastive Shape-Impression\\nEmbedding for Fonts\\nYugo Kubota, Daichi Haraguchi[0000-0002-3109-9053], and\\nSeiichi Uchida[\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0-0001-8592-7566]\\nKyushu University, Fukuoka, Japan yugo . kubota@human . ait . kyushu-u . ac . j p\\nAbstract. Fonts convey different impressions to readers. These impres-\\nsions often come from the font shapes. However, the correlation between\\nfonts and their impression is weak and unstable because impressions\\nare subjective. To capture such weak and unstable cross-modal correla-\\ntion between font shapes and their impressions, we propose Impression-\\nCLIP, which is a novel machine-learning model based on CLIP (Con-\\ntrastive Language-Image Pre-training). By using the CLIP-based model,\\nfont image features and their impression features are pulled closer, and\\nfont image features and unrelated impression features are pushed apart.\\nThis procedure realizes co-embedding between font image and their im-\\npressions. In our experiment, we perform cross-modal retrieval between\\nfonts and impressions through co-embedding. The results indicate that\\nImpression-CLIP achieves better retrieval accuracy than the state-of-the-\\nart method. Additionally, our model shows the robustness to noise and\\nmissing tags.\\nKeywords: Contrastive Embedding \\xe2\\x80\\xa2 Font style \\xe2\\x80\\xa2 Impression.\\n1\\nIntroduction\\nThe shape of machine-printed letters varies depending on fonts. Even for the\\nsame letter 'A,' there are huge shape variations. For example, when printing 'A'\\nusing a serif font, its diagonal strokes often end with a short decorative line called\\na serif. Nowadays, such huge shape variations are available as font collections.\\nTypographic experts (or even non-experts) carefully choose appropriate fonts\\nfor their products.\\nDifferent fonts give different impressions to readers. Fig. 1 shows several font\\nexamples and their impression tags at MyFonts.com. Interestingly, these simple\\nshapes (i.e., stroke contours) with neither photo-realistic nor color elements can\\ngive various impressions. Moreover, we get such impressions from detailed shape\\nstructures independent of the whole shape of the letter itself (e.g., 'Al. Namely,\\nan impression from the letter 'A' does not come from its basic structure with\\ntwo diagonal strokes and a horizontal stroke but from finer structures, such as\\nlocal contour curvature, stroke thickness, corner shapes, serif, etc.\\nDependency between font shapes and their impressions will not be random;\\nin other words, they have a mutual correlation. Understanding the correlation\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Impression-CLIP: Contrastive Shape-Impression', 'Embedding for Fonts']\n",
            "Authors: ['Yugo Kubota, Daichi Haraguchi[0000-0002-3109-9053], and', 'Seiichi Uchida[°°°°-0001-8592-7566]', 'Kyushu University, Fukuoka, Japan yugo . kubota@human . ait . kyushu-u . ac . j p']\n",
            "Authors: Yugo Kubota, Daichi Haraguchi[0000-0002-3109-9053], and, Seiichi Uchida[°°°°-0001-8592-7566], Kyushu University, Fukuoka, Japan yugo . kubota@human . ait . kyushu-u . ac . j p \n",
            "\n",
            "b\"Towards End-to-End Semi-Supervised Table Detection\\nwith Semantic Aligned Matching Transformer\\nTahira Shehzadi*1,2,3[0000-0002-7052-979X] Shalini Sarode1,3 [0009-0007-9968-4068],\\nDidier Stricker1'2'3, and Muhammad Zeshan Afza11,2,3[0000-0002-0536-6867]\\n1 Department of Computer Science, Technical University of Kaiserslautern, 67663, Germany\\n2 Mindgarage, Technical University of Kaiserslautern, 67663, Germany\\n3 German Research Institute for Artificial Intelligence (DFKI), 67663 Kaiserslautern, Germany\\nItahira.shehzadi@dfki.del\\nAbstract. Table detection within document images is a crucial task in document\\nprocessing, involving the identification and localization of tables. Recent strides in\\ndeep learning have substantially improved the accuracy of this task, but it still heav-\\nily relies on large labeled datasets for effective training. Several semi-supervised ap-\\nproaches have emerged to overcome this challenge, often employing CNN-based de-\\ntectors with anchor proposals and post-processing techniques like non-maximal sup-\\npression (NMS). However, recent advancements in the field have shifted the focus\\ntowards transformer-based techniques, eliminating the need for NMS and empha-\\nsizing object queries and attention mechanisms. Previous research has focused on\\ntwo key areas to improve transformer-based detectors: refining the quality of object\\nqueries and optimizing attention mechanisms. However, increasing object queries\\ncan introduce redundancy, while adjustments to the attention mechanism can in-\\ncrease complexity. To address these challenges, we introduce a semi-supervised ap-\\nproach employing SAM-DETR, a novel approach for precise alignment between ob-\\nject queries and target features. Our approach demonstrates remarkable reductions\\nin false positives and substantial enhancements in table detection performance, par-\\nticularly in complex documents characterized by diverse table structures. This work\\nprovides more efficient and accurate table detection in semi-supervised settings.\\nKeywords: Semi-Supervised Learning \\xe2\\x80\\xa2 Detection Transformer \\xe2\\x80\\xa2 SAM-DETR \\xe2\\x80\\xa2 Ta-\\nble Analysis \\xe2\\x80\\xa2 Table Detection.\\n1\\nIntroduction\\nDocument analysis has been the fundamental task in various workflow pipelines[1,2], with\\ndocument summarization as its core task. The essential task in document analysis is iden-\\ntifying graphical objects like tables, figures, and text paragraphs. Previously, this task\\nwas carried out manually by analyzing the documents, understanding their contents, and\\nsummarizing them. However, the number of documents that need to be analyzed has dras-\\ntically increased, and manual inspection is impossible. The growing number of documents\\nled businesses to use more efficient and reliable automated methods. Optical character\\nrecognition(OCR) [3,4] and rule-based table detection approaches[5,6,7] are classical ap-\\nproaches for visual summarization. These methods perform well for documents with highly\\nstructured layouts because they are rule-based[5,6,7]. However, they struggle to adapt to\\nvarying and newer table designs, such as borderless tables. These limitations has shifted the\\nresearch focus to developing techniques using deep learning [8,9,10,11]. These methods show\\nsignificant improvements over traditional approaches [12], precisely detecting tables in doc-\\numents irrespective of their structure. This advancement provides a notable improvement\\nin document analysis and visual summarization.\\nDeep learning methods [13,14,15,16,17,18] eliminate handcrafted rules and excel at gen-\\neralizing problems. However, their reliance on large amounts of labeled data for training\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Towards End-to-End Semi-Supervised Table Detection', 'with Semantic Aligned Matching Transformer']\n",
            "Authors: ['Tahira Shehzadi*1,2,3[0000-0002-7052-979X] Shalini Sarode1,3 [0009-0007-9968-4068],', \"Didier Stricker1'2'3, and Muhammad Zeshan Afza11,2,3[0000-0002-0536-6867]\", '1 Department of Computer Science, Technical University of Kaiserslautern, 67663, Germany']\n",
            "Authors: Tahira Shehzadi*1,2,3[0000-0002-7052-979X] Shalini Sarode1,3 [0009-0007-9968-4068],, Didier Stricker1'2'3, and Muhammad Zeshan Afza11,2,3[0000-0002-0536-6867], 1 Department of Computer Science, Technical University of Kaiserslautern, 67663, Germany \n",
            "\n",
            "b\"UniVIE: A Unified Label Space Approach to\\nVisual Information Extraction from Form-like\\nDocuments\\nKai Hul'2'*'t , Jiawei Wang1,2,*, Weihong Line,*,\\nZhuoyao Zhong2'*, Lei Sung'*, and Qiang Huo2\\n1 University of Science and Technology of China, Hefei, China\\n2 Microsoft Research Asia, Beijing, China\\nhk970213@mail.ustc.edu.cn, wangjiawei@mail.ustc.edu.cn,\\n1wher1996@outlook.com, zhuoyao.zhong@gmail.com,\\nray_ustc@163.com,\\ngianghuo@microsoft.com\\nAbstract. Existing methods for Visual Information Extraction (VIE)\\nfrom form-like documents typically fragment the process into separate\\nsubtasks, such as key information extraction, key-value pair extraction,\\nand choice group extraction. However, these approaches often overlook\\nthe hierarchical structure of form documents, including hierarchical key-\\nvalue pairs and hierarchical choice groups. To address these limitations,\\nwe present a new perspective, refraining VIE as a relation prediction\\nproblem and unifying labels of different tasks into a single label space.\\nThis unified approach allows for the definition of various relation types\\nand effectively tackles hierarchical relationships in form-like documents.\\nIn line with this perspective, we present UniVIE, a unified model that\\naddresses the VIE problem comprehensively. UniVIE functions using a\\ncoarse-to-fine strategy. It initially generates tree proposals through a\\nTree Proposal Network, which are subsequently refined into hierarchical\\ntrees by a Relation Decoder module. To enhance the relation prediction\\ncapabilities of UniVIE, we incorporate two novel tree constraints into\\nthe Relation Decoder: a Tree Attention Mask and a Tree Level Embed-\\nding. Extensive experimental evaluations on both our in-house dataset\\nHierForms and a publicly available dataset SIBR, substantiate that our\\nmethod achieves state-of-the-art results, underscoring the effectiveness\\nand potential of our unified approach in advancing the field of VIE.\\nKeywords: Visual Information Extraction \\xe2\\x80\\xa2 Relation Prediction \\xe2\\x80\\xa2 Uni-\\nfied Label Space.\\n*Work done when Kai Hu and Jiawei Wang were interns, Weihong Lin, Zhuoyao\\nZhong and Lei Sun were full-time employees in Multi-Modal Interaction Group, Mi-\\ncrosoft Research Asia, Beijing, China.\\ntCorrespondence to: Kai Hu at hk970213@mail.ustc .edu.cn.\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['UniVIE: A Unified Label Space Approach to', 'Visual Information Extraction from Form-like']\n",
            "Authors: ['Documents', \"Kai Hul'2'*'t , Jiawei Wang1,2,*, Weihong Line,*,\", \"Zhuoyao Zhong2'*, Lei Sung'*, and Qiang Huo2\"]\n",
            "Authors: Documents, Kai Hul'2'*'t , Jiawei Wang1,2,*, Weihong Line,*,, Zhuoyao Zhong2'*, Lei Sung'*, and Qiang Huo2 \n",
            "\n",
            "b'Multi-Cell Decoder and Mutual Learning for\\nTable Structure and Character Recognition\\nTakaya Kawakatsui[0000-0003-1285-2748]\\nPreferred Networks, Inc., 1-6-1 Otemachi, Chiyoda, Tokyo, Japan.\\nkat.nii.ac.jp@gmail.com\\nhttps://researchmap.jp/t.kat\\nAbstract. Extracting table contents from documents such as scientific\\npapers and financial reports and converting them into a format that can\\nbe processed by large language models is an important task in knowledge\\ninformation processing. End-to-end approaches, which recognize not only\\ntable structure but also cell contents, achieved performance comparable\\nto state-of-the-art models using external character recognition systems,\\nand have potential for further improvements. In addition, these models\\ncan now recognize long tables with hundreds of cells by introducing local\\nattention. However, the models recognize table structure in one direction\\nfrom the header to the footer, and cell content recognition is performed\\nindependently for each cell, so there is no opportunity to retrieve useful\\ninformation from the neighbor cells. In this paper, we propose a multi-cell\\ncontent decoder and bidirectional mutual learning mechanism to improve\\nthe end-to-end approach. The effectiveness is demonstrated on two large\\ndatasets, and the experimental results show comparable performance to\\nstate-of-the-art models, even for long tables with large numbers of cells.\\nKeywords: Deep Learning, Table Recognition, Transformer, Mutual Learning\\n1\\nIntroduction\\nInformation retrieval technology which provides high-quality knowledge to large\\nlanguage models (LLMs) is attracting attention. Many researchers have worked\\non converting scanned and imaged documents to machine-readable formats such\\nas HTML code [37,42,15] and LaTeX code [3,12]. This initiative has direct and\\nindirect benefits. First, because past literature remains mostly in printed form,\\nit is necessary to convert it into structured electronic documents. This is a direct\\nbenefit. Second, the realization of intelligence which recognizes hidden meanings\\nin the layout of published documents that are intended to be read by humans is\\nimportant from the perspective of human-machine interaction. This is an indirect\\nbenefit.\\nIn this paper, we will focus on table recognition, which includes two types of\\ntasks, namely structure recognition and cell content recognition. A simple table\\nhas horizontal and vertical borders, and each cell contains characters. Complex\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Multi-Cell Decoder and Mutual Learning for', 'Table Structure and Character Recognition']\n",
            "Authors: ['Takaya Kawakatsui[0000-0003-1285-2748]', 'Preferred Networks, Inc., 1-6-1 Otemachi, Chiyoda, Tokyo, Japan.', 'kat.nii.ac.jp@gmail.com']\n",
            "Authors: Takaya Kawakatsui[0000-0003-1285-2748], Preferred Networks, Inc., 1-6-1 Otemachi, Chiyoda, Tokyo, Japan., kat.nii.ac.jp@gmail.com \n",
            "\n",
            "b\"Source-Free Domain Adaptation for Optical\\nMusic Recognition\\nAdrian Rose1161, Eliseo Fuentes-Martinezi , Maria Alfaro-Contreras1 , David\\nRizo1'2, and Jorge Calvo-Zaragozal\\n1 Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain\\n2 Instituto Superior de Ensefianzas Artisticas de la Comunidad Valenciana, Spain\\nAbstract. This work addresses the problem of Domain Adaptation\\n(DA) in the context of stafflevel end-to-end Optical Music Recognition.\\nSpecifically, we consider a source-free DA approach to adapt a given\\ntrained model to a new collection\\xe2\\x80\\x94an extremely useful scenario for pre-\\nserving musical heritage. The method involves re-training the pre-trained\\nmodel to align the statistics stored from the original data in normal-\\nization layers with those of the new collection, while also including a\\nregularization mechanism to prevent the model from converging to un-\\ndesirable solutions. Unlike conventional DA techniques, this approach is\\nvery efficient and practical, as it only requires the pre-trained model and\\nunlabeled data from the new collection, without relying on data from the\\noriginal training collections (i.e., source-free). Evaluation of diverse music\\ncollections in Mensural notation and a synthetic-to-real scenario of com-\\nmon Western modern notation demonstrates consistent improvements\\nover the baseline (no DA), often with remarkable relative improvements.\\nKeywords: Optical Music Recognition\\n\\xe2\\x80\\xa2\\nDomain Adaptation\\n\\xe2\\x80\\xa2\\nSource-Free Domain Adaptation\\n1\\nIntroduction\\nOptical Music Recognition (OMR), the task analogous to Optical Character\\nRecognition or Handwritten Text Recognition for music documents, holds great\\nimportance in expediting the transcription of music heritage into digital formats.\\nOMR offers a scalable and efficient alternative to the tedious manual transcrip-\\ntion process, further aggravated by the complexities of music score editors.\\nIn recent years, there has been a growing interest in developing models for\\nOMR, driven largely by advances in Deep Learning. These innovations have\\ncatalyzed the emergence of end-to-end neural approaches capable of directly re-\\ncovering the sequence of musical symbols within entire units (typically staves).\\nHowever, despite the success of this approach, the need for sufficient labeled\\ndata to train the neural network is evident. This necessity underscores a broader\\nchallenge in the field: the development of a universal OMR system. This pursuit\\nis hindered by several factors such as the diversity of music notation formats\\nand the presence of graphical variations in documents. These factors necessitate\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Source-Free Domain Adaptation for Optical', 'Music Recognition']\n",
            "Authors: ['Adrian Rose1161, Eliseo Fuentes-Martinezi , Maria Alfaro-Contreras1 , David', \"Rizo1'2, and Jorge Calvo-Zaragozal\", '1 Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain']\n",
            "Authors: Adrian Rose1161, Eliseo Fuentes-Martinezi , Maria Alfaro-Contreras1 , David, Rizo1'2, and Jorge Calvo-Zaragozal, 1 Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain \n",
            "\n",
            "b\"LMTextSpotter: Towards Better Scene Text\\nSpotting with Language Modeling in Transformer\\nXin Xia1'2, Guodong Ding2, and Siyuan Lie\\n1 College of Computer Science and Technology, Zhejiang University\\n2 HiThink RoyalFlush Information Network Co. Ltd., China\\nfxiaxin,dingguodong2,1isiyuan2Iftyhexin.com\\nAbstract. End-to-end text spotting aims to build a unified framework\\nfor scene text detection and recognition. Recently, the DETR-like frame-\\nwork has attracted great attention. However, most of such studies merely\\nfocused on promoting the collaboration between two sub-tasks, without\\nconsidering the importance of linguistic information in scene text. In\\nthis paper, we propose a novel end-to-end text spotting model, termed\\nas LMTextSpotter, which introduces language modeling capability into\\nthe DETR-like text spotting framework. Specifically, we add an extra\\nLM recognition branch besides the detection and commonly used CTC\\nrecognition branch in most existing frameworks, which is trained with\\nthe cross-entropy loss in autoregression manner. We backtrack the viterbi\\npath given by CTC recognition branch to get an alignment between the\\nqueries and characters, so as to train LM recognition branch without\\nrelying on the character level annotations. After training phase, the LM\\nrecognition branch can predict the character classes independently of the\\nCTC recognition branch. Besides, we exploit the position priors provided\\nby rotated bounding boxes and adopt a task-specific query strategy for\\nfurther improving the performance. Comparison results demonstrate the\\nsuperiority of our LMTextSpotter over previous state-of-the-art methods.\\nAblation studies also prove the effectiveness of each component.\\nKeywords: Scene Text Spotting \\xe2\\x80\\xa2 Deformable Attention \\xe2\\x80\\xa2 CTC-based\\nCharacter Recognition \\xe2\\x80\\xa2 Language Modeling.\\n1\\nIntroduction\\nText Spotting aims to build a end-to-end scene text detection and recognition\\nframework, which raises wide concern due to its extensive application in image\\nretrieval, automatic driving, and intelligent transportation [4]. Early works [16,\\n43, 32] regard text recognition as character classification, without concerning the\\nlinguistic information naturally contained in the text. However, such approaches\\nusually face a significant degradation in performance with insufficient visual\\ninformation provided, e.g., images taken in harsh conditions. Some works [17, 15,\\n38, 24] turned the attention to language modeling and thus made a step forward.\\nHowever, most existing approaches treat text detection and recognition as two\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['LMTextSpotter: Towards Better Scene Text', 'Spotting with Language Modeling in Transformer']\n",
            "Authors: [\"Xin Xia1'2, Guodong Ding2, and Siyuan Lie\", '1 College of Computer Science and Technology, Zhejiang University', '2 HiThink RoyalFlush Information Network Co. Ltd., China']\n",
            "Authors: Xin Xia1'2, Guodong Ding2, and Siyuan Lie, 1 College of Computer Science and Technology, Zhejiang University, 2 HiThink RoyalFlush Information Network Co. Ltd., China \n",
            "\n",
            "b'Binarizing Documents\\nby Leveraging both Space and Frequency\\nFabio Quattrini[0009-0004-3244-6186], Vittorio Pippip009-000l-7365-6348]\\nSilvia Cascianelli[0000-0001-7885-6050], and Rita Cucchiara[0003-0002-2239-283X]\\nUniversity of Modena and Reggio Emilia, Modena, Italy\\nIname.surnamel@unimore.it\\nAbstract. Document Image Binarization is a well-known problem in\\nDocument Analysis and Computer Vision, although it is far from being\\nsolved. One of the main challenges of this task is that documents gener-\\nally exhibit degradations and acquisition artifacts that can greatly vary\\nthroughout the page. Nonetheless, even when dealing with a local patch\\nof the document, taking into account the overall appearance of a wide\\nportion of the page can ease the prediction by enriching it with seman-\\ntic information on the ink and background conditions. In this respect,\\napproaches able to model both local and global information have been\\nproven suitable for this task. In particular, recent applications of Vision\\nTransformer (ViT)-based models, able to model short and long-range de-\\npendencies via the attention mechanism, have demonstrated their supe-\\nriority over standard Convolution-based models, which instead struggle\\nto model global dependencies. In this work, we propose an alternative so-\\nlution based on the recently introduced Fast Fourier Convolutions, which\\novercomes the limitation of standard convolutions in modeling global in-\\nformation while requiring fewer parameters than ViTs. We validate the\\neffectiveness of our approach via extensive experimental analysis consid-\\nering different types of degradations.\\nKeywords: Document Enhancement\\n\\xe2\\x80\\xa2 Document Image Binarization\\n\\xe2\\x80\\xa2\\nFast Fourier Convolution.\\n1\\nIntroduction\\nDocuments with good visual quality are easier to handle when performing Doc-\\nument Analysis tasks, which is why the problems and approaches to Document\\nEnhancement have long been explored. However, this remains a challenging\\ntask, especially for old and highly damaged documents, for which automati-\\ncally achieving good visual quality can benefit efficient content understanding.\\nDue to preservation conditions and time, these documents exhibit various types\\nof degradations, such as bleed-through ink, ink oxidation, faded ink, smears,\\nscratches, stains, and creases in the paper. Moreover, the digitalization pro-\\ncess can introduce artifacts due to image compression and unideal acquisition\\nconditions such as uneven illumination, contrast variation, and blur [68]. A typ-\\nical Document Enhancement task is Document Binarization, which consists in\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Binarizing Documents', 'by Leveraging both Space and Frequency']\n",
            "Authors: ['Fabio Quattrini[0009-0004-3244-6186], Vittorio Pippip009-000l-7365-6348]', 'Silvia Cascianelli[0000-0001-7885-6050], and Rita Cucchiara[0003-0002-2239-283X]', 'University of Modena and Reggio Emilia, Modena, Italy']\n",
            "Authors: Fabio Quattrini[0009-0004-3244-6186], Vittorio Pippip009-000l-7365-6348], Silvia Cascianelli[0000-0001-7885-6050], and Rita Cucchiara[0003-0002-2239-283X], University of Modena and Reggio Emilia, Modena, Italy \n",
            "\n",
            "b\"1\\n3\\nA Hybrid Approach for Document Layout\\nAnalysis in Document images\\nTahira Shehzadi*1,2,3[0000-0002-7052-979X]7 Didier Stricker1,2,37 and\\nMuhammad Zeshan Afza11,2,3 [0000-0002-0536-6867]\\nDepartment of Computer Science, Technical University of Kaiserslautern, Germany\\n2 Mindgarage, Technical University of Kaiserslautern, Germany\\nGerman Research Institute for Artificial Intelligence (DFKI), 67663 Kaiserslautern,\\nGermany\\nftahira.shehzadi@dfki.del\\nAbstract. Document layout analysis involves understanding the ar-\\nrangement of elements within a document. This paper navigates the\\ncomplexities of understanding various elements within document images,\\nsuch as text, images, tables, and headings. The approach employs an\\nadvanced Transformer-based object detection network as an innovative\\ngraphical page object detector for identifying tables, figures, and dis-\\nplayed elements. We introduce a query encoding mechanism to provide\\nhigh-quality object queries for contrastive learning, enhancing efficiency\\nin the decoder phase. We also present a hybrid matching scheme that\\nintegrates the decoder's original one-to-one matching strategy with the\\none-to-many matching strategy during the training phase. This approach\\naims to improve the model's accuracy and versatility in detecting vari-\\nous graphical elements on a page. Our experiments on PubLayNet, Do-\\ncLayNet, and PubTables benchmarks show that our approach outper-\\nforms current state-of-the-art methods. It achieves an average precision\\nof 97.3% on PubLayNet, 81.6% on DocLayNet, and 98.6% on PubTa-\\nbles, demonstrating its superior performance in layout analysis. These\\nadvancements not only enhance the conversion of document images into\\neditable and accessible formats but also streamline information retrieval\\nand data extraction processes.\\nKeywords: Detection Transformer \\xe2\\x80\\xa2 Document Layout Analysis \\xe2\\x80\\xa2 Graph-\\nical object detection\\n1\\nIntroduction\\nSystems for Document Intelligence (DI) is essential in enhancing the efficiency of\\nautomating large-scale document processing tasks, primarily focusing on extract-\\ning and understanding content within these documents. These systems are piv-\\notal in key business intelligence operations such as document retrieval, text recog-\\nnition, and content categorization, which rely heavily on extracting information\\nand transforming documents into a structured, machine-readable format. This\\nprocess seamlessly integrates the information extracted into further document\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['1', '3']\n",
            "Authors: ['A Hybrid Approach for Document Layout', 'Analysis in Document images', 'Tahira Shehzadi*1,2,3[0000-0002-7052-979X]7 Didier Stricker1,2,37 and']\n",
            "Authors: A Hybrid Approach for Document Layout, Analysis in Document images, Tahira Shehzadi*1,2,3[0000-0002-7052-979X]7 Didier Stricker1,2,37 and \n",
            "\n",
            "b'Dynamic Relation Transformer for Contextual\\nText Block Detection\\nJiawei Wang1\\'3\\'*\\'t, Shunchi Zhang2,3\\'*\\'t, Kai Hul\\'3\\'*\\'t,\\nChixiang Ma\", Zhuoyao Zhong\", Lei Sun\", and Qiang Huo3\\n1 University of Science and Technology of China\\n2 Xi\\'an Jiaotong University\\n3 Microsoft Research Asia\\nAbstract. Contextual Text Block Detection (CTBD) is the task of\\nidentifying coherent text blocks in complex natural scenes. Previous\\nmethodologies have treated CTBD as either a visual relation extrac-\\ntion problem from the perspective of computer vision or as a sequence\\nmodeling problem from the perspective of natural language processing.\\nWe introduce a new framework that frames CTBD as a graph gener-\\nation problem. This methodology consists of two essential procedures:\\nidentifying individual text units as graph nodes and discerning the se-\\nquential reading order relationships among these units as graph edges.\\nLeveraging the cutting-edge capabilities of DQ-DETR for node detec-\\ntion, our framework innovates further by integrating a novel mechanism,\\na Dynamic Relation Transformer (DRFormer), dedicated to edge gen-\\neration. DRFormer incorporates a dual interactive transformer decoder\\nthat manages a dynamic graph structure refinement process. Through\\nthis iterative process, the model gradually enhances the accuracy of the\\ngraph, ultimately resulting in improved precision in detecting contex-\\ntual text blocks. Comprehensive experimental evaluations conducted on\\nboth SCUT-CTW-Context and ReCTS-Context datasets substantiate\\nthat our method achieves state-of-the-art results, underscoring the effec-\\ntiveness and potential of our graph generation framework in advancing\\nthe field of CTBD.\\nKeywords: Graph Generation\\n\\xe2\\x80\\xa2 Scene Text Detection\\n\\xe2\\x80\\xa2 Text Region\\nDetection\\n1\\nIntroduction\\nContextual Text Block Detection (CTBD) [39] aims to detect contextual text\\nblocks (CTBs) within natural scenes, which are aggregates of one or more inte-\\ngral text units, such as characters, words, or text-lines, arranged in their natural\\n*Equal contribution. Correspondence to wangj iawei@mail.ustc .edu.cn.\\ntThis work was done when Jiawei Wang, Shunchi Zhang, Kai Hu were interns\\nand Chixiang Ma, Zhuoyao Zhong, Lei Sun were full-time employees in Multi-Modal\\nInteraction Group, Microsoft Research Asia, Beijing, China.\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Dynamic Relation Transformer for Contextual', 'Text Block Detection']\n",
            "Authors: [\"Jiawei Wang1'3'*'t, Shunchi Zhang2,3'*'t, Kai Hul'3'*'t,\", 'Chixiang Ma\", Zhuoyao Zhong\", Lei Sun\", and Qiang Huo3', '1 University of Science and Technology of China']\n",
            "Authors: Jiawei Wang1'3'*'t, Shunchi Zhang2,3'*'t, Kai Hul'3'*'t,, Chixiang Ma\", Zhuoyao Zhong\", Lei Sun\", and Qiang Huo3, 1 University of Science and Technology of China \n",
            "\n",
            "b'Multimodal Adaptive Inference for Document\\nImage Classification with Anytime Early Exiting\\nOmar Hamedl, Souhail Bakkali4 [0000-0001-9383-3842] , Matthew\\nBlaschko2 [0000-0002-2640 \\xe2\\x80\\x94181X]\\nSien Moens2 [0000-0002-3732-9323]\\nJordy Van\\nLandeghem2,3 [0000-0002-9838-3024]\\n1\\nMulti-Exit Multi-modal Architecture\\nDocument Image\\n1\\nPmehify\\n2 1 Exit Placement\\nPassible\\n1 Microsoft\\nomarhamed@micro sof t . com\\n2 KU Leuven, Belgium\\n3 Contract.fit, Brussels, Belgium\\n4 L3i - La Rochelle Universite, La Rochelle, France\\nText Modality\\nLayout Modality\\nVision Modality\\n,\\nExit Classifier\\nRamp\\nGate\\nModality Exits\\n1,.1) Training Strategies\\nSubgraphs Weighting\\nEntropy Regularization\\nWeighted-Entropy\\nEncoder Exits\\nFig. 1: Illustration of the proposed experimental methodology of a multi-modal multi-\\nexit architecture for efficient document image classification. Every step highlights de-\\nsign choices that are benchmarked for achieving Pareto efficiency.\\nAbstract. This work addresses the need for a balanced approach be-\\ntween performance and efficiency in scalable production environments\\nfor visually-rich document understanding (VDU) tasks. Currently, there\\nis a reliance on large document foundation models that offer advanced\\ncapabilities but come with a heavy computational burden. In this paper,\\nwe propose a multimodal early exit (EE) model design that incorporates\\nvarious training strategies, exit layer types and placements5. Our goal\\nis to achieve a Pareto-optimal balance between predictive performance\\nand efficiency for multimodal document image classification. Through a\\n5 Code is available at https: //github . com/Jordy- VL/mult i -modal - early- exit\\nTop Classifier\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Multimodal Adaptive Inference for Document', 'Image Classification with Anytime Early Exiting']\n",
            "Authors: ['Omar Hamedl, Souhail Bakkali4 [0000-0001-9383-3842] , Matthew', 'Blaschko2 [0000-0002-2640 —181X]', 'Sien Moens2 [0000-0002-3732-9323]']\n",
            "Authors: Omar Hamedl, Souhail Bakkali4 [0000-0001-9383-3842] , Matthew, Blaschko2 [0000-0002-2640 —181X], Sien Moens2 [0000-0002-3732-9323] \n",
            "\n",
            "b'MOoSE: Multi-Orientation Sharing Experts for\\nOpen-set Scene Text Recognition\\nChang Liu, Simon Corbille, and Elisa H Barney Smith\\nMachine Learning Group, Lulea University of Technology, Sweden\\nchang.liu@ltu.se,\\nsimon.corbille@associated.ltu.se,\\nelisa.barney@ltu.se\\nAbstract. Open-set text recognition, which aims to address both novel\\ncharacters and previously seen ones, is one of the rising subtopics in\\nthe text recognition field. However, the current open-set text recognition\\nsolutions only focuses on horizontal text, which fail to model the real-\\nlife challenges posed by the variety of writing directions in real-world\\nscene text. Multi-orientation text recognition, in general, faces challenges\\nfrom diverse image aspect ratios, significant imbalance in data amount,\\nand domain gaps between orientations. In this work, we first propose a\\nMulti-Oriented Open-Set Text Recognition task (MOOSTR) to model\\nthe challenges of both novel characters and writing direction variety. We\\nthen propose a Multi-Orientation Sharing Experts (MOoSE) framework\\nas a strong baseline solution. MOoSE uses a mixture-of-experts scheme\\nto alleviate the domain gaps between orientations, while exploiting com-\\nmon structural knowledge among experts to alleviate the data scarcity\\nthat some experts face. The proposed MOoSE framework is validated\\nby ablative experiments, and also tested for feasibility on an existing\\nopen-set text recogntion benchmark. Code, models, and documents are\\navailable at: https : //github .com/lancercat/Moose/\\nKeywords: Open-set text recognition, multi-orientation text recognition, in-\\ncremental learning\\n1\\nIntroduction\\nThe open-set text recognition task, as an emerging subfield of text recognition,\\naims to address the presence of novel characters and contextual biases introduced\\nby the use of multiple language scripts and language evolution [1]. Specifically,\\nfor novel characters the task first requires the model to spot samples that include\\n\"out-of-set\" characters for human inspection. Then the model needs to incremen-\\ntally acquire the capability to recognize these characters without retraining, by\\nregistering the side-information [2, 3]. Open-set text recognition is well-suited\\nto scene text recognition scenerios, where characters from a new script can be\\nfrequently encountered as in the Korean text image in Fig. 1 which includes text\\nin 4 languages (Korean, English, Chinese, and Japanese) in one single scene.\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['MOoSE: Multi-Orientation Sharing Experts for', 'Open-set Scene Text Recognition']\n",
            "Authors: ['Chang Liu, Simon Corbille, and Elisa H Barney Smith', 'Machine Learning Group, Lulea University of Technology, Sweden', 'chang.liu@ltu.se,']\n",
            "Authors: Chang Liu, Simon Corbille, and Elisa H Barney Smith, Machine Learning Group, Lulea University of Technology, Sweden, chang.liu@ltu.se, \n",
            "\n",
            "b'More and Less: Enhancing Abundance and\\nRefining Redundancy for Text-Prior-Guided\\nScene Text Image Super-Resolution\\nWei Yang1,200, Yihong Luo3G, Mayire\\n1,r) and Askar Hamdulla1,26\\n1 School of Computer Science and Technology, Xinjiang University, Urumqi 830017,\\nChina\\n2 Xinjiang Key Laboratory of Signal Detection and Processing, Urumqi 830017,\\nChina\\n3 School of Measurement and Communication Engineering, Harbin University of\\nScience and Technology, Harbin 150080, China.\\netom611@stu.xju.edu.cn,\\nluoylh@l63.com,\\nmayire401@xju.edu.cn,\\naskar@xju.edu.cn\\nAbstract. Scene text image super-resolution (STISR) aims to enhance\\nlow-resolution text images, boosting downstream text recognition tasks.\\nRecent STISR models leverage text recognizer for prior information,\\nachieving superior performance via a novel strategy. However, we observe\\nabundant erroneous prior information from the low-resolution (LR) text\\nimages processed by the text recognizer, which can mislead text recon-\\nstruction when fused with image features. Therefore, we propose a novel\\nsequential residual blocks, termed sequence refinement blocks, to refine\\nthe merged features of text images and text priors during the recon-\\nstruction of LR images. Additionally, regarding the widespread problem\\nof ignoring the contextual semantic information in the shallow features\\nof text images in the STISR, We introduce a multi-scale feature module\\nto supplement the fine-grained and coarse-grained information required\\nin the reconstruction of LR text images, which can well resolve infor-\\nmation loss and generate more accurate super-resolution text images.\\nOur proposed method consistently outperforms baselines employing text\\nrecognizers ASTER, MORAN, and CRNN by 1-2% on TextZoom, and\\nachieves impressive gains of 4-5% on the challenging hard subset when\\nleveraging multi-modal recognizers like ABINet and MATRN. The gen-\\neralization experiments on scene text recognition datasets demonstrate\\noptimal 5-8% performance improvements over the baselines.\\nKeywords: super-resolution \\xe2\\x80\\xa2 text prior \\xe2\\x80\\xa2 text recognition\\n1\\nIntroduction\\nIn recent years, with the advent of industrial applications such as autonomous\\ndriving and license plate recognition, scene text-related tasks, including scene\\n* Corresponding author\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['More and Less: Enhancing Abundance and', 'Refining Redundancy for Text-Prior-Guided']\n",
            "Authors: ['Scene Text Image Super-Resolution', 'Wei Yang1,200, Yihong Luo3G, Mayire', '1,r) and Askar Hamdulla1,26']\n",
            "Authors: Scene Text Image Super-Resolution, Wei Yang1,200, Yihong Luo3G, Mayire, 1,r) and Askar Hamdulla1,26 \n",
            "\n",
            "b'End to End Table Transformer\\nYun Young Choi*[0000-0002-6017-258X], Taehoon Kim* [0009-0004-4676-8784]\\nNamwook Kim0\\xc2\\xb0\\xc2\\xb09-0006-2348-490017 Taehee Lee[\\xc2\\xb0\\xc2\\xb0\\xc2\\xb09-0006-7701-2335]\\nand\\nSeongho Joe[\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0-0003-1419-9930]\\nSamsung SDS, Seoul 05510, South Korea\\nAbstract. Table extraction (TE) task in document images is important\\nin deep learning for conveying structured information. TE was decom-\\nposed into three subtasks: table detection (TD), table structure recog-\\nnition (TSR), and functional analysis (FA). Most of previous research\\nfocused on developing models specifically tailored to each of these tasks,\\nleading to challenges in computational cost, model size, and performance\\nlimitations. Transformer-based object detection models are being suc-\\ncessfully applied to TE subtasks, yet they face inherent challenges due\\nto the one-to-one set matching approach for detecting objects. This ap-\\nproach assigns only a few queries as positive samples, diminishing the\\ntraining efficacy of these samples and leading to a performance bot-\\ntleneck. Therefore, prior research in the object detection field has in-\\ntroduced modifications to the Detection Transformer (DETR), adding\\nadditional queries and training schemes that improve performance. In\\nthis work, we introduce the End-to-end Table Transformer (ETT), a\\nspecialized transformer-based object detection model designed for high-\\nperforming TE from document images with single model. Our model\\ncomprises three key components: a backbone, the Deformable DETR\\n(DDETR) model, and the novel layout analysis module with table layout\\nloss. This layout analysis module leverages explicit relationships between\\ntable objects to enhance the table extraction task performance in multi\\ntables in images. We conduct rigorous experiments to assess the efficacy\\nof our proposed model against table extraction benchmark datasets, com-\\nparing it with other DETR variants, including vanilla DETR, DDETR,\\nand H-DETR. Empirical evaluations highlight that our model efficiently\\nsecures state-of-the-art results in TE task.\\nKeywords: Document analysis systems \\xe2\\x80\\xa2 Document image processing \\xe2\\x80\\xa2\\nPhysical and logical layout analysis \\xe2\\x80\\xa2 Tables and charts.\\n1\\nIntroduction\\nTable extraction (TE) in document images is an important application of com-\\nputer vision for analyzing and summarizing structured information. Previous\\napproaches divided the TE task into three subtasks: table detection (TD) for\\ndetecting table area in single document page; table structure recognition (TSR)\\n* equal contribution, correspondence to Seongho Joe(drizzle.choOsarnsung.corn)\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['End to End Table Transformer', 'Yun Young Choi*[0000-0002-6017-258X], Taehoon Kim* [0009-0004-4676-8784]']\n",
            "Authors: ['Namwook Kim0°°9-0006-2348-490017 Taehee Lee[°°°9-0006-7701-2335]', 'and', 'Seongho Joe[°°°°-0003-1419-9930]']\n",
            "Authors: Namwook Kim0°°9-0006-2348-490017 Taehee Lee[°°°9-0006-7701-2335], and, Seongho Joe[°°°°-0003-1419-9930] \n",
            "\n",
            "b\"Deep Learning-Driven Innovative Model for\\nGenerating Functional Knowledge Units\\nPan Qiangangl, Hu Yahongl*, Xie Youbai2'3, Meng Xianghui2, and Zhang\\nYilun2\\n1 Zhejiang University of Technology, Hangzhou, Zhejiang, China\\n2 Shanghai Jiaotong University, Shanghai, China\\n3 Xi'an Jiaotong University, Xi'an, Shaanxi, China\\n1308176817@qq. com, huyahong@zjut . edu . cn, fyl:mie ochmengl@sj tu . edu. cn.\\nzhangyi1un902@163 . com\\nAbstract. Design science research shows that existing knowledge is the\\nbasis for product design. The functional knowledge unit is the most basic\\nknowledge to describe the functional design knowledge. Nowadays, the\\nacquisition of functional units is mainly manual, which is time-consuming\\nand labor-intensive. Functional knowledge integration is an effective way\\nto achieve innovation design, yet the insufficient functional units can-\\nnot effectively support the integration. To address the above issue, this\\npaper proposes a named-entity recognition (NER) model called Bound-\\nary Perception NER (BP-NER). From the product manual, BP-NER\\ncan automatically extract information necessary to describe the func-\\ntional unit. The model leverages entity boundary information to predict\\nentity classification labels and incorporates semantically-rich character-\\nlevel feature information. BP-NER also introduces FocalLoss function\\nto solve the problem of label imbalance. Experiments on the functional\\nunit dataset demonstrate the effectiveness of the proposed model. Com-\\npared with the baseline model BERT-BiLSTM-CRF, BP-NER increases\\nthe overall label prediction accuracy by 5.05%, and the average Fl-score\\nimprovement is 32.8% for entities CIN,COT,DIN,DOT and ENY.\\nKeywords: Natural Language Processing \\xe2\\x80\\xa2 Named Entity Recognition\\n\\xe2\\x80\\xa2 Product innovative Design \\xe2\\x80\\xa2 Perception Awareness.\\n1\\nIntroduction\\nImproving product quality and enhancing innovation capability are key factors\\nfor enterprise development. When designers rely solely on their own or their\\nteam's knowledge to design, the resulting design proposals are easily confined\\nwithin a limited knowledge framework, making it difficult for the products to be\\ninnovative [1-3]. In general, the design of new products consists of conceptual\\ndesign, structural design, and process design. Conceptual design plays a crucial\\nrole in product quality, and more and more research is being conducted in this\\narea.\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Deep Learning-Driven Innovative Model for', 'Generating Functional Knowledge Units']\n",
            "Authors: [\"Pan Qiangangl, Hu Yahongl*, Xie Youbai2'3, Meng Xianghui2, and Zhang\", 'Yilun2', '1 Zhejiang University of Technology, Hangzhou, Zhejiang, China']\n",
            "Authors: Pan Qiangangl, Hu Yahongl*, Xie Youbai2'3, Meng Xianghui2, and Zhang, Yilun2, 1 Zhejiang University of Technology, Hangzhou, Zhejiang, China \n",
            "\n",
            "b'Privacy-Aware Document Visual Question\\nAnswering\\nRuben Tito\", Khanh Nguyen\\', Marlon Tobaben*2, Raouf Kerkouche3,\\nMohamed Ali Souibguil, Kangsoo June, Joonas Ja110, Vincent Poulain\\nD\\'Andecy5, Aurelie Josephs, Lei Kang1, Ernest Valvenyl, Antti Honkela2,\\nMario Fritz3, and Dimosthenis Karatzas1\\n1 Computer Vision Center, Universitat Autonoma de Barcelona, Spain\\n2 University of Helsinki, Finland\\n3 CISPA Helmholtz Center for Information Security, Germany\\n4 French Institute for Research in Computer Science and Automation (INRIA),\\nFrance\\n5 Yooz, France\\nrperez@cvc.uab.cat\\nknguyen@cvc.uab.cat\\nmarlon.tobaben@helsinki.fi\\nAbstract. Document Visual Question Answering (DocVQA) has quickly\\ngrown into a central task of document understanding. But despite the\\nfact that documents contain sensitive or copyrighted information, none\\nof the current DocVQA methods offers strong privacy guarantees.\\nIn this work, we explore privacy in the domain of DocVQA for the first\\ntime, highlighting privacy issues in state of the art multi-modal LLM\\nmodels used for DocVQA, and explore possible solutions.\\nSpecifically, we focus on invoice processing as a realistic document under-\\nstanding scenario, and propose a large scale DocVQA dataset comprising\\ninvoice documents and associated questions and answers. We employ a\\nfederated learning scheme, that reflects the real-life distribution of doc-\\numents in different businesses, and we explore the use case where the\\ndata of the invoice provider is the sensitive information to be protected.\\nWe demonstrate that non-private models tend to memorise, a behaviour\\nthat can lead to exposing private information. We then evaluate baseline\\ntraining schemes employing federated learning and differential privacy\\nin this multi-modal scenario, where the sensitive information might be\\nexposed through either or both of the two input modalities: vision (doc-\\nument image) or language (OCR tokens).\\nFinally, we design attacks exploiting the memorisation effect of the model,\\nand demonstrate their effectiveness in probing a representative DocVQA\\nmodels.\\nKeywords: DocVQA \\xe2\\x80\\xa2 Federated Learning \\xe2\\x80\\xa2 Differential Privacy\\n1\\nIntroduction\\nAutomatic document processing enables the vast majority of daily interactions\\nwith and between institutions. From a research viewpoint, document understand-\\n* These authors contributed equally to this work.\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Privacy-Aware Document Visual Question', 'Answering']\n",
            "Authors: ['Ruben Tito\", Khanh Nguyen\\', Marlon Tobaben*2, Raouf Kerkouche3,', 'Mohamed Ali Souibguil, Kangsoo June, Joonas Ja110, Vincent Poulain', \"D'Andecy5, Aurelie Josephs, Lei Kang1, Ernest Valvenyl, Antti Honkela2,\"]\n",
            "Authors: Ruben Tito\", Khanh Nguyen', Marlon Tobaben*2, Raouf Kerkouche3,, Mohamed Ali Souibguil, Kangsoo June, Joonas Ja110, Vincent Poulain, D'Andecy5, Aurelie Josephs, Lei Kang1, Ernest Valvenyl, Antti Honkela2, \n",
            "\n",
            "b'A region-based approach for layout analysis of\\nmusic score images in scarce data scenarios\\nFrancisco J. Castellanos[0000-0001-9949-5522] Juan P.\\nMartinez-Esteso[0009-0006-3246-3781] Alejandro\\nGalan-Cuenca[\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0-0002-5799-6270] , and Antonio Javier\\n\\xe2\\x80\\x940003-3148-6886]\\nGallegoPm\\nUniversity Institute for Computing Research, University of Alicante, 03690, Spain\\nf cast ellanos@dls i ua . es. {juan . mart inez 11 , a . galanl@ua . es,\\njgallego@dlsi .ua. es\\nAbstract. This work presents a novel region-based layout analysis (LA)\\nmethod for Optical Music Recognition (OMR) systems, aimed at over-\\ncoming the data scarcity challenge. Contemporary OMR techniques,\\ngrounded in machine learning principles, have a critical requirement: a\\nlabeled dataset for training. This presents a practical challenge due to\\nthe extensive manual effort required, coupled with the fact that the avail-\\nability of suitable data for creating training sets is not always guaran-\\nteed. Unlike other approaches, our method focuses on adapting the train-\\ning and sample extraction processes within an existing neural network\\nframework. Our approach incorporates a labeled data-driven oversam-\\npling technique, a masking layer to enable training with partial labeling,\\nand an adaptive scaling process to improve results for varying score sizes.\\nThrough comprehensive experimentation, we established the minimal la-\\nbeled data necessary for an effective model and demonstrated that our\\nmethod could achieve a performance comparable with the state-of-the-\\nart with just 8 to 32 labeled samples. The implications of our research\\nextend beyond improving LA, providing a scalable and practical solution\\nfor digitizing and preserving musical documents.\\nKeywords:\\n\\xe2\\x80\\xa2 Few-shot Learning \\xe2\\x80\\xa2 Deep Learning \\xe2\\x80\\xa2 Layout Analysis\\n\\xe2\\x80\\xa2\\nObject Detection \\xe2\\x80\\xa2 Optical Music Recognition.\\n1\\nIntroduction\\nThe digitization process\\xe2\\x80\\x94understood as the conversion of physical documents\\ninto digital format\\xe2\\x80\\x94has been a fundamental advancement for the preservation\\nof historical manuscripts of our heritage. This process not only improves accessi-\\nbility but also plays a critical role in the protection and conservation of cultural\\nheritage [24]. However, it is important to note that for these documents to be\\ntruly accessible, it is not sufficient to simply transform them into a digital image;\\nit is necessary to transcribe their content into a structured format that enables\\naccess, search, and automatic processing of the content. This task is addressed\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['A region-based approach for layout analysis of', 'music score images in scarce data scenarios']\n",
            "Authors: ['Francisco J. Castellanos[0000-0001-9949-5522] Juan P.', 'Martinez-Esteso[0009-0006-3246-3781] Alejandro', 'Galan-Cuenca[°°°°-0002-5799-6270] , and Antonio Javier']\n",
            "Authors: Francisco J. Castellanos[0000-0001-9949-5522] Juan P., Martinez-Esteso[0009-0006-3246-3781] Alejandro, Galan-Cuenca[°°°°-0002-5799-6270] , and Antonio Javier \n",
            "\n",
            "b'Multi-task Learning for License Plate\\nRecognition in Unconstrained Scenarios\\nZhen-Lun Mot, Song-Lu Chenl, Qi Liu1, Feng Chen2, and Xu-Cheng Yinl\\n1 University of Science and Technology Beijing, Beijing, China\\n{songluchen,xuchengyin}Oustb.edu.cn\\n2 EEasy Technology Company Ltd., Zhuhai, China\\nAbstract. The recognition of license plates in natural scenes often face\\nchallenges such as multi-directional and multi-line variations. Addition-\\nally, previous studies have treated license plate detection and recognition\\nas separate tasks, resulting in inefficiencies and error accumulation. To\\naddress these challenges, we propose an end-to-end method for license\\nplate detection and recognition using multi-task learning. Firstly, we in-\\ntroduce two parallel branches to detect the horizontal bounding box and\\nthe four corners of the license plate, enabling multi-directional license\\nplate detection in a multi-task manner. The outputs from these branches\\nare combined to enhance recognition accuracy. Secondly, we propose to\\nextract global features to perceive character layout and utilize reading\\norder to spatially attend to characters for recognizing multi-line license\\nplates. Finally, we combine detection and recognition using the same\\nbackbone, with the detection branch based on multiple deep layers and\\nthe recognition branch based on multiple shallow layers, thereby con-\\nstructing an end-to-end detection and recognition network. Comparative\\nexperiments on CCPD and RodoSol datasets validate that our method\\nsignificantly outperforms state-of-the-art methods, particularly in sce-\\nnarios involving multi-directional and multi-line license plates.\\nKeywords: License plate recognition\\n\\xe2\\x80\\xa2 Multi-task\\n\\xe2\\x80\\xa2 Multi-directional\\n\\xe2\\x80\\xa2\\nMulti-line \\xe2\\x80\\xa2 End-to-end.\\n1\\nIntroduction\\nAutomatic License Plate Recognition (ALPR) is a crucial technology in intelli-\\ngent transportation systems, essential for traffic monitoring, electronic toll col-\\nlection (ETC), and smart parking lots [8, 27, 9]. While effective in specific oper-\\national contexts, ALPR faces challenges in unconstrained scenarios, as shown in\\nFig. 1. Recognizing multi-directional and multi-line license plates poses signifi-\\ncant obstacles for conventional ALPR systems.\\nAs depicted in Fig. 2(a), previous license plate detectors [18, 10, 30, 4] have\\nshown limitations in effectively handling multi-directional license plates due to\\ntheir inherent restriction to detecting only the horizontal bounding box. Con-\\nsequently, this limitation can introduce significant background noises into sub-\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Multi-task Learning for License Plate', 'Recognition in Unconstrained Scenarios']\n",
            "Authors: ['Zhen-Lun Mot, Song-Lu Chenl, Qi Liu1, Feng Chen2, and Xu-Cheng Yinl', '1 University of Science and Technology Beijing, Beijing, China', '{songluchen,xuchengyin}Oustb.edu.cn']\n",
            "Authors: Zhen-Lun Mot, Song-Lu Chenl, Qi Liu1, Feng Chen2, and Xu-Cheng Yinl, 1 University of Science and Technology Beijing, Beijing, China, {songluchen,xuchengyin}Oustb.edu.cn \n",
            "\n",
            "b\"A Real-Time Scene Uyghur Text Detection\\nNetwork Based on Feature Complementation\\nMayire Ibrayim1,2[0000-0002-8766-0647], Mengmeng Chen1,2[0000-0003-4604-0051],\\nAskar Ham.dulla2,3[0000-0002-2321-308X], Jianjun Kang1,2 , and Chunhu\\nZhang1,2[0009-0002-2564-0998]\\n1\\nCollege of Information Science and Engineering, Xinjiang University, Urumqi,\\n830046, Xinjiang, China\\n2 Xinjiang Key Laboratory of Signal Detection, Xinjiang University, Urumqi, 830046,\\nXinjiang, China\\n3 Academy of Future Technology, Xinjiang University, Urumqi, 830046, Xinjiang,\\nChina\\nAbstract. Text detection in complex background images is a challeng-\\ning task. With the national emphasis on the culture of various ethnic\\ngroups, carrying out the task of detecting Uyghur in natural scene images\\nis of great significance to the intelligent information technology industry\\nand economic construction. In fact, current text detection applications in\\ncomplex scenes have large models, and sluggish detection rates, and are\\nchallenging to implement on mobile devices. Uyghur text has unique writ-\\ning characteristics that lead to low detection results. To address these\\nproblems, we propose a feature-complementation-based text detection\\nframework for real-time scenes (FC-Net). The network uses a deeply\\nseparable residual convolutional network (DResNet) for feature extrac-\\ntion, which lowers the model's parameter count and boosts the detector's\\ndetection speed. Secondly, a feature enhancement network based on spa-\\ntial feature attention (SFA-FEN) is included in to obtain multi-target\\ninformation by expanding the range of perceptual fields and enhanc-\\ning the robustness of small-scale Uyghur. The experimental results show\\nthat FC-Net can maintain the advantages of a lightweight network while\\nmaintaining high accuracy, effectively reducing model parameters, and\\nspeeding up model detection. The robustness and real-time performance\\nare achieved not only on the Uyghur dataset but also on an arbitrary\\ntext dataset.\\nKeywords: Lightweight\\n\\xe2\\x80\\xa2 Scene text detection\\n\\xe2\\x80\\xa2 Feature complementa-\\ntion \\xe2\\x80\\xa2 Sensory field \\xe2\\x80\\xa2 FC-Net.\\n1\\nIntroduction\\nStudying Uyghur scene texts is of great significance to the intelligent information\\ntechnology industry and economic development in western my country. However,\\nthe exploration of Uyghur text in scene text detection tasks is still in the early\\nstages of development, so we study the detection of Uyghur text in scene images.\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['A Real-Time Scene Uyghur Text Detection', 'Network Based on Feature Complementation']\n",
            "Authors: ['Mayire Ibrayim1,2[0000-0002-8766-0647], Mengmeng Chen1,2[0000-0003-4604-0051],', 'Askar Ham.dulla2,3[0000-0002-2321-308X], Jianjun Kang1,2 , and Chunhu', 'Zhang1,2[0009-0002-2564-0998]']\n",
            "Authors: Mayire Ibrayim1,2[0000-0002-8766-0647], Mengmeng Chen1,2[0000-0003-4604-0051],, Askar Ham.dulla2,3[0000-0002-2321-308X], Jianjun Kang1,2 , and Chunhu, Zhang1,2[0009-0002-2564-0998] \n",
            "\n",
            "b'SAGHOG: Self-Supervised Autoencoder for\\nGenerating HOG Features for Writer Retrieval\\nMarco PeerO, Florian Klebere, and Robert Sablatnigwo\\nComputer Vision Lab, TU Wien\\n{mpeer, kleber,\\nsabl@cvlAuwien.ac.at\\nCode:http://github.com/marco-peer/icdar24\\nAbstract. This paper introduces SAGHOG, a self-supervised pretraining\\nstrategy for writer retrieval using HOG features of the binarized input\\nimage. Our preprocessing involves the application of the Segment Any-\\nthing technique to extract handwriting from various datasets, ending up\\nwith about 24k documents, followed by training a vision transformer\\non reconstructing masked patches of the handwriting. SAGHOG is then\\nfinetuned by appending NetRVLAD as an encoding layer to the pre-\\ntrained encoder. Evaluation of our approach on three historical datasets,\\nHistorical-WI, HisFrag20, and GRK-Papyri, demonstrates the effective-\\nness of SAGHOG for writer retrieval. Additionally, we provide ablation\\nstudies on our architecture and evaluate un- and supervised finetuning.\\nNotably, on HisFrag20, SAGHOG outperforms related work with a mAP\\nof 57.2 % - a margin of 11.6 % to the current state of the art, showcas-\\ning its robustness on challenging data, and is competitive on even small\\ndatasets, e.g. GRK-Papyri, where we achieve a Top-1 accuracy of 58.0 %.\\nKeywords: Writer Retrieval \\xe2\\x80\\xa2 Self-Supervised Learning \\xe2\\x80\\xa2 Masked Au-\\ntoencoder \\xe2\\x80\\xa2 Document Analysis.\\n1\\nIntroduction\\nWriter Retrieval (WR) is the task of locating documents written by the same\\nauthor within a dataset, achieved by identifying similarities in handwriting [18].\\nThis task is particularly valuable for historians and paleographers, allowing them\\nto track individuals or social groups across various historical periods [9]. Ad-\\nditionally, WR is used for recognizing documents with unknown authors and\\nuncovering similarities within such documents [6].\\nWR methods usually consist of multiple steps, such as sampling of handwrit-\\ning by applying interest point detection, a neural network for feature extraction,\\nand feature encoding such as NetVLAD, followed by aggregation of the encoded\\nfeatures [29]. Although those methods work for large datasets such as Historical-\\nWI [14], WR still lacks performance for datasets that contain less handwriting or\\nnoise such as degradation, best exemplified by HisFrag20 [32]. While approaches\\ntrained on full fragments currently work best [26], experiments show that those\\nmethods do infer features from the background, not necessarily related on the\\nactual handwriting [30].\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['SAGHOG: Self-Supervised Autoencoder for', 'Generating HOG Features for Writer Retrieval']\n",
            "Authors: ['Marco PeerO, Florian Klebere, and Robert Sablatnigwo', 'Computer Vision Lab, TU Wien', '{mpeer, kleber,']\n",
            "Authors: Marco PeerO, Florian Klebere, and Robert Sablatnigwo, Computer Vision Lab, TU Wien, {mpeer, kleber, \n",
            "\n",
            "b\"ConClue: Conditional Clue Extraction for\\nMultiple Choice Question Answering*\\nWangli Yang1, Jie Yangl*, Wanqing Li', and Yi Guo2\\n1 School of Computing and Information Technology,\\nUniversity of Wollongong, Australia\\n{wangli , j iey , wanqing}@uow . edu . au\\n2 School of Computer, Data and Mathematical Sciences,\\nWestern Sydney University, Australia\\ny . guo@westernsydney . edu . au\\nAbstract. The task of Multiple Choice Question Answering (MCQA)\\naims to identify the correct answer from a set of candidates, given a back-\\nground passage and an associated question. Considerable research efforts\\nhave been dedicated to addressing this task, leveraging a diversity of se-\\nmantic matching techniques to estimate the alignment among the answer,\\npassage, and question. However, key challenges arise as not all sentences\\nfrom the passage contribute to the question answering, while only a few\\nsupporting sentences (clues) are useful. Existing clue extraction methods\\nsuffer from inefficiencies in identifying supporting sentences, relying on\\nresource-intensive algorithms, pseudo labels, or overlooking the semantic\\ncoherence of the original passage. Addressing this gap, this paper intro-\\nduces a novel extraction approach, termed Conditional Clue extractor\\n(ConClue), for MCQA. ConClue leverages the principles of Conditional\\nOptimal Transport to effectively identify clues by transporting the se-\\nmantic meaning of one or several words (from the original passage) to\\nselected words (within identified clues), under the prior condition of the\\nquestion and answer. Empirical studies on several competitive bench-\\nmarks consistently demonstrate the superiority of our proposed method\\nover different traditional approaches, with a substantial average improve-\\nment of 1.1-2.5 absolute percentage points in answering accuracy.\\nKeywords: Multiple Choice Question Answering \\xe2\\x80\\xa2 Optimal Transport\\n\\xe2\\x80\\xa2 Clue Extraction \\xe2\\x80\\xa2 Machine Reading Comprehension\\n1\\nIntroduction\\nMultiple Choice Question Answering (MCQA), a critical task in Machine Read-\\ning Comprehension (MRC), remains a central focus in Natural Language Pro-\\ncessing (NLP) research. The task necessitates the ability to identify the correct\\nanswer from a candidate set, in response to a given passage and question [9,19].\\n* Corresponding author. This work is partially supported by the Australian Research\\nCouncil Discovery Project (DP210101426), the Australian Research Council Linkage\\nProject (LP200201035), AEGiS Advance Grant(888/008/268, University of Wollon-\\ngong), and Telstra-UOW Hub for AIOT Solutions Seed Funding (2024).\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['ConClue: Conditional Clue Extraction for', 'Multiple Choice Question Answering*']\n",
            "Authors: [\"Wangli Yang1, Jie Yangl*, Wanqing Li', and Yi Guo2\", '1 School of Computing and Information Technology,', 'University of Wollongong, Australia']\n",
            "Authors: Wangli Yang1, Jie Yangl*, Wanqing Li', and Yi Guo2, 1 School of Computing and Information Technology,, University of Wollongong, Australia \n",
            "\n",
            "b'Progressive Evolution from Single-Point to\\nPolygon for Scene Text\\nLinger Denglt, Mingxin Huang2t, Xudong Xie1, Yuliang Liul*, Lianwen Jin2,\\nand Xiang Bail\\n1 Huazhong University of Science and Technology, WuHan, China\\n2 South China University of Technology, GuangZhou, China\\nlingerdeng20230163.com\\nylliu@hust.edu.cn\\nAbstract. The advancement of text shape representations towards com-\\npactness has enhanced text detection and spotting performance, but at\\na high annotation cost. Current models use single-point annotations to\\nreduce costs, yet they lack sufficient localization information for down-\\nstream applications. To overcome this limitation, we introduce Point2Pol-\\nygon, which can efficiently transform single-points into compact poly-\\ngons. Our method uses a coarse-to-fine process, starting with creating\\nand selecting anchor points based on recognition confidence, then ver-\\ntically and horizontally refining the polygon using recognition informa-\\ntion to optimize its shape. We demonstrate the accuracy of the gener-\\nated polygons through extensive experiments: 1) By creating polygons\\nfrom ground truth points, we achieved an accuracy of 82.0% on ICDAR\\n2015; 2) In training detectors with polygons generated by our method,\\nwe attained 86% of the accuracy relative to training with ground truth\\n(GT); 3) Additionally, the proposed Point2Polygon can be seamlessly\\nintegrated to empower single-point spotters to generate polygons. This\\nintegration led to an impressive 82.5% accuracy for the generated poly-\\ngons. It is worth mentioning that our method relies solely on synthetic\\nrecognition information, eliminating the need for any manual annotation\\nbeyond single points.\\nKeywords: Text Representations \\xe2\\x80\\xa2 Text Detection \\xe2\\x80\\xa2 Single Point \\xe2\\x80\\xa2 Recog-\\nnition Information.\\n1\\nIntroduction\\nIn recent years, the evolution of text shape representation has significantly en-\\nhanced text detection and spotting, progressing from managing horizontal and\\nmulti-oriented text to adeptly handling arbitrarily shaped text. However, polyg-\\nonal annotation brings expensive annotation costs and limits the acquisition of\\nlarge-scale annotated data, constraining the overall generalization of the model.\\ntEqual contribution.\\n* Corresponding author.\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Progressive Evolution from Single-Point to', 'Polygon for Scene Text']\n",
            "Authors: ['Linger Denglt, Mingxin Huang2t, Xudong Xie1, Yuliang Liul*, Lianwen Jin2,', 'and Xiang Bail', '1 Huazhong University of Science and Technology, WuHan, China']\n",
            "Authors: Linger Denglt, Mingxin Huang2t, Xudong Xie1, Yuliang Liul*, Lianwen Jin2,, and Xiang Bail, 1 Huazhong University of Science and Technology, WuHan, China \n",
            "\n",
            "b\"Visual Prompt Learning for Chinese Handwriting\\nRecognition\\nGang yao1,2 [0000-0001-9311-0337], Ning Ding1,2 [0000-0001-5808-8795]7 Tianqi\\nZhao1,2 [0009-0001-0433-1662], Kemeng Zhao1,2 [0009-0006-3539-5163], pei\\nTang1,2[0000-0002\\xe2\\x80\\x94 1176-0086] 7 Yao Ta03 [0009-0007-2666-8315]7 and Liangrui\\nPeng1 ,20:21) [0000-0001-7793-1039]\\n1 Department of Electronic Engineering, Tsinghua University, Beijing, China\\n2 Beijing National Research Center for Information Science and Technology,\\nTsinghua University, Beijing, China\\n3 Huawei Noah's Ark Lab, Shenzhen, China\\n{yg19,dn22,zhaotq20,zkm23,tp21}0mails.tsinghua.edu.cn\\ntaoyao2Ohuawei.com\\npenglr@tsinghua.edu.cn\\nAbstract. Recognizing Chinese handwriting in unconstrained scenar-\\nios remains a challenging task due to wide variations in writing styles\\nand imaging conditions. Recently, prompt learning in natural language\\nprocessing has shown success in leveraging context awareness in various\\ndomains. This paper explores the idea of incorporating visual prompt\\nlearning into an encoder-decoder model for handwriting recognition. For\\nthe encoder, multi-scale meta prompts are incorporated to utilize con-\\ntextual information in internal feature representations. For the decoder,\\nadditional character-level visual prompts are used along with the em-\\nbeddings of previously predicted text to guide the decoding process. Ex-\\nperiments conducted on the SCUT-HCCDoc, SCUT-EPT and CASIA-\\nHWDB Chinese handwriting datasets validate the effectiveness of the\\nproposed methods.\\nKeywords: handwriting recognition \\xe2\\x80\\xa2 visual prompt learning \\xe2\\x80\\xa2 encoder-\\ndecoder model\\n1\\nIntroduction\\nIn recent years, there have been increasing demands to recognize Chinese hand-\\nwriting in low-quality images captured by mobile phones in unconstrained sce-\\nnarios. In addition to large character set and variations of writing styles, different\\nillumination, viewpoints, and degradation in imaging process bring more chal-\\nlenges for handwriting recognition.\\nDeep learning based handwriting recognition methods generally fall into two\\ncategories, connectionist temporal classification (CTC) based methods [18] and\\nattention mechanism based methods [2]. Recently, Transformer-based methods\\nwith self-attention mechanism [11,12,15,32,24] have shown promising perfor-\\nmance for handwriting tasks.\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Visual Prompt Learning for Chinese Handwriting', 'Recognition']\n",
            "Authors: ['Gang yao1,2 [0000-0001-9311-0337], Ning Ding1,2 [0000-0001-5808-8795]7 Tianqi', 'Zhao1,2 [0009-0001-0433-1662], Kemeng Zhao1,2 [0009-0006-3539-5163], pei', 'Tang1,2[0000-0002— 1176-0086] 7 Yao Ta03 [0009-0007-2666-8315]7 and Liangrui']\n",
            "Authors: Gang yao1,2 [0000-0001-9311-0337], Ning Ding1,2 [0000-0001-5808-8795]7 Tianqi, Zhao1,2 [0009-0001-0433-1662], Kemeng Zhao1,2 [0009-0006-3539-5163], pei, Tang1,2[0000-0002— 1176-0086] 7 Yao Ta03 [0009-0007-2666-8315]7 and Liangrui \n",
            "\n",
            "b'Self-supervised Pre-training of Text Recognizers\\nMartin Kimi [0000-0001\\xe2\\x80\\x946853-0508] and Michal Hradigi [0000-0002-6364-129X]\\nFaculty of Information Technology, Brno University of Technology,\\nBrno, Czech Republic\\n{ikiss,hradis}@fit.vutbr.cz\\nAbstract. In this paper, we investigate self-supervised pre-training meth-\\nods for document text recognition. Nowadays, large unlabeled datasets\\ncan be collected for many research tasks, including text recognition, but\\nit is costly to annotate them. Therefore, methods utilizing unlabeled data\\nare researched. We study self-supervised pre-training methods based on\\nmasked label prediction using three different approaches \\xe2\\x80\\x94 Feature Quan-\\ntization, VQ-VAE, and Post-Quantized AE. We also investigate joint-\\nembedding approaches with VICReg and NT-Xent objectives, for which\\nwe propose an image shifting technique to prevent model collapse where\\nit relies solely on positional encoding while completely ignoring the input\\nimage. We perform our experiments on historical handwritten (Bentham)\\nand historical printed datasets mainly to investigate the benefits of the\\nself-supervised pre-training techniques with different amounts of anno-\\ntated target domain data. We use transfer learning as strong baselines.\\nThe evaluation shows that the self-supervised pre-training on data from\\nthe target domain is very effective, but it struggles to outperform transfer\\nlearning from closely related domains. This paper is one of the first re-\\nsearches exploring self-supervised pre-training in document text recogni-\\ntion, and we believe that it will become a cornerstone for future research\\nin this area. We made our implementation of the investigated methods\\npublicly available at https://github.com/DCGM/pero-pretraining.\\nKeywords: Self-supervised learning \\xe2\\x80\\xa2 Text Recognition \\xe2\\x80\\xa2 Pre-training \\xe2\\x80\\xa2\\nOCR \\xe2\\x80\\xa2 HTR.\\n1\\nIntroduction\\nNowadays, large models are an integral part of machine learning. Representatives\\nof such models are, for example, language models GPT-3 [10] or LLaMa-2 [35],\\nthe ASR model Whisper [31], or generative models such as Stable Diffusion [32]\\nor DALLE-3 [9]. In order for these models to generalize, a large amount of train-\\ning data is needed. However, obtaining such large datasets with annotations is\\nvery expensive either in terms of cost or time. This also applies to text recogni-\\ntion and especially to handwritten text recognition.\\nAt the same time, relatively large datasets without annotations are available\\nand approaches based on semi-supervised or self-supervised learning are able\\nto utilize this data. Semi-supervised approaches are mostly based on pseudo-\\nlabeling [26, 21, 22, 39] (also self-training) or consistency regularization [8, 25, 1].\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Self-supervised Pre-training of Text Recognizers', 'Martin Kimi [0000-0001—6853-0508] and Michal Hradigi [0000-0002-6364-129X]']\n",
            "Authors: ['Faculty of Information Technology, Brno University of Technology,', 'Brno, Czech Republic', '{ikiss,hradis}@fit.vutbr.cz']\n",
            "Authors: Faculty of Information Technology, Brno University of Technology,, Brno, Czech Republic, {ikiss,hradis}@fit.vutbr.cz \n",
            "\n",
            "b'Drawing the Line: Deep Segmentation for\\nExtracting Art from Ancient Etruscan Mirrors*\\nRafael SterzingerC, Simon BrennerO, and Robert Sablatnige\\nComputer Vision Lab, TU Wien, Vienna, AUT\\nIfirstname.lastnamel@tuwien.ac.at\\nAbstract. Etruscan mirrors constitute a significant category within Etr-\\nuscan art and, therefore, undergo systematic examinations to obtain in-\\nsights into ancient times. A crucial aspect of their analysis involves the\\nlabor-intensive task of manually tracing engravings from the backside.\\nAdditionally, this task is inherently challenging due to the damage these\\nmirrors have sustained, introducing subjectivity into the process. We ad-\\ndress these challenges by automating the process through photometric-\\nstereo scanning in conjunction with deep segmentation networks which,\\nhowever, requires effective usage of the limited data at hand. We ac-\\ncomplish this by incorporating predictions on a per-patch level, and\\nvarious data augmentations, as well as exploring self-supervised learn-\\ning. Compared to our baseline, we improve predictive performance w.r.t.\\nthe pseudo-F-Measure by around 16%. When assessing performance on\\ncomplete mirrors against a human baseline, our approach yields quantita-\\ntive similar performance to a human annotator and significantly outper-\\nforms existing binarization methods. With our proposed methodology,\\nwe streamline the annotation process, enhance its objectivity, and reduce\\noverall workload, offering a valuable contribution to the examination of\\nthese historical artifacts and other non-traditional documents.\\nKeywords: Binarization\\n\\xe2\\x80\\xa2 Photometric Stereo \\xe2\\x80\\xa2 Image Segmentation\\n\\xe2\\x80\\xa2\\nLimited Data \\xe2\\x80\\xa2 Etruscan Art \\xe2\\x80\\xa2 Cultural Heritage\\n1\\nIntroduction\\nComprising more than 3,000 identified specimens, Etruscan hand mirrors repre-\\nsent one of the most extensive categories in Etruscan art. These ancient artworks,\\nwhich date from the second half of the sixth century to the first century B.C.E.,\\nare predominantly made from bronze. On the front, they feature a highly pol-\\nished surface acting as a mirror, whereas the back side typically depicts engraved\\nand/or chased linear figurative illustrations of Greek mythology which may be\\nsupplemented by inscriptions in Etruscan [9]. An exemplary mirror showcasing\\nsuch drawings is depicted in Fig. 1.\\n* We gratefully acknowledge funding from the Austrian Science Fund/Osterreichischer\\nWissenschaftsfonds (FWF) under grant agreement No. P 33721. Additionally, we\\nthank Ms. Sindy Kluge for her time and expertise in annotating the mirrors.\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Drawing the Line: Deep Segmentation for', 'Extracting Art from Ancient Etruscan Mirrors*']\n",
            "Authors: ['Rafael SterzingerC, Simon BrennerO, and Robert Sablatnige', 'Computer Vision Lab, TU Wien, Vienna, AUT', 'Ifirstname.lastnamel@tuwien.ac.at']\n",
            "Authors: Rafael SterzingerC, Simon BrennerO, and Robert Sablatnige, Computer Vision Lab, TU Wien, Vienna, AUT, Ifirstname.lastnamel@tuwien.ac.at \n",
            "\n",
            "b'Revisiting N-Gram Models: Their Impact in\\nModern Neural Networks for Handwritten Text\\nRecognition\\nSolene Tarride[0000-0001-6174-9865] and Christopher\\nKermorvant[0000-0002-7508-4080]\\nTEKLIA, Paris, France\\nAbstract. In recent advances in automatic text recognition (ATR),\\ndeep neural networks have demonstrated the ability to implicitly cap-\\nture language statistics, potentially reducing the need for traditional lan-\\nguage models. This study directly addresses whether explicit language\\nmodels, specifically n-gram models, still contribute to the performance\\nof state-of-the-art deep learning architectures in the field of handwriting\\nrecognition. We evaluate two prominent neural network architectures,\\nPyLaia [23] and DAN [8], with and without the integration of explicit\\nn-gram language models. Our experiments on three datasets - IAM [19],\\nRIMES [11], and NorHand v2 [2] - at both line and page level, investi-\\ngate optimal parameters for n-gram models, including their order, weight,\\nsmoothing methods and tokenization level. The results show that incor-\\nporating character or subword n-gram models significantly improves the\\nperformance of ATR models on all datasets, challenging the notion that\\ndeep learning models alone are sufficient for optimal performance. In par-\\nticular, the combination of DAN with a character language model out-\\nperforms current benchmarks, confirming the value of hybrid approaches\\nin modern document analysis systems.\\nKeywords: Handwritten Text Recognition, Neural Networks, Statisti-\\ncal Language Modeling, Tokenization\\n1\\nIntroduction\\nBefore the era of deep neural networks, handwriting recognition systems [13,5,1],\\nderived from automatic speech recognition, usually combined an optical model,\\ndesigned to generate character hypotheses from the image, and a language model,\\nresponsible for reevaluating these hypotheses using language statistics. The in-\\ntroduction of the statistical language model had significantly reduced the tran-\\nscription error rates.\\nAs deep neural network performance drastically improved, the need for sta-\\ntistical language models was less prevalent. The rise of transformers and their\\nimplicit language modeling capacities [8,17,3] eclipsed even more the need for\\nexplicit language models for Automatic Text Recognition (ATR). Today, a key\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Revisiting N-Gram Models: Their Impact in', 'Modern Neural Networks for Handwritten Text']\n",
            "Authors: ['Recognition', 'Solene Tarride[0000-0001-6174-9865] and Christopher', 'Kermorvant[0000-0002-7508-4080]']\n",
            "Authors: Recognition, Solene Tarride[0000-0001-6174-9865] and Christopher, Kermorvant[0000-0002-7508-4080] \n",
            "\n",
            "b'Learning to Kern: Set-wise Estimation of\\nOptimal Letter Space\\nKei Nakatsuru and Seiichi Uchida[\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0-0001-8592-7566]\\nKyushu University, Fukuoka, Japan\\nuchida@ait.kyushu-u.ac.jp\\nAbstract. Kerning is the task of setting appropriate horizontal spaces\\nfor all possible letter pairs of a certain font. One of the difficulties of kern-\\ning is that the appropriate space differs for each letter pair. Therefore, for\\na total of 52 capital and small letters, we need to adjust 52 x 52 = 2704\\ndifferent spaces. Another difficulty is that there is neither a general pro-\\ncedure nor criterion for automatic kerning; therefore, kerning is still done\\nmanually or with heuristics. In this paper, we tackle kerning by proposing\\ntwo machine-learning models, called pairwise and set-wise models. The\\nformer is a simple deep neural network that estimates the letter space\\nfor two given letter images. In contrast, the latter is a transformer-based\\nmodel that estimates the letter spaces for three or more given letter\\nimages. For example, the set-wise model simultaneously estimates 2704\\nspaces for 52 letter images for a certain font. Among the two models, the\\nset-wise model is not only more efficient but also more accurate because\\nits internal self-attention mechanism allows for more consistent kerning\\nfor all letters. Experimental results on about 2500 Google fonts and their\\nquantitative and qualitative analyses show that the set-wise model has\\nan average estimation error of only about 5.3 pixels when the average\\nletter space of all fonts and letter pairs is about 115 pixels.\\nKeywords: kerning\\n\\xe2\\x80\\xa2 letter spacing\\n\\xe2\\x80\\xa2 machine learning.\\n1\\nIntroduction\\nKerning, or letter spacing, is the micro-typographic task of adjusting the horizon-\\ntal space between adjacent letters[10]. For monospaced fonts (like most Chinese\\nfonts), the space is easily determined so that all letters occupy the same horizon-\\ntal space. In contrast, kerning is a very important operation for the proportional\\nfonts of the Latin alphabet letters. Fig. 1(a) shows the effect of kerning. Without\\nkerning (that is, by monospacing), the letters do not appear to form the word\\n\"UNIVERSITY,\" especially due to an excessive space around \\'I\\' Various cognitive\\npsychology research on reading has revealed that kerning is important for better\\nreadability [1, 4]. Therefore, kerning is not a trivial task for type designers and\\nis beneficial for readers.\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Learning to Kern: Set-wise Estimation of', 'Optimal Letter Space']\n",
            "Authors: ['Kei Nakatsuru and Seiichi Uchida[°°°°-0001-8592-7566]', 'Kyushu University, Fukuoka, Japan', 'uchida@ait.kyushu-u.ac.jp']\n",
            "Authors: Kei Nakatsuru and Seiichi Uchida[°°°°-0001-8592-7566], Kyushu University, Fukuoka, Japan, uchida@ait.kyushu-u.ac.jp \n",
            "\n",
            "b\"ClusterTabNet: Supervised clustering method\\nfor table detection and table structure\\nrecognition\\nMarek Polewczykl and Marco Spinacil\\nSAP Business AT {marek . polewczyk , marco . spinaci }@sap . com\\nAbstract. Table detection and recognition consists of locating tables\\nwithin a given document and identifying the exact location of its pieces,\\nsuch as rows, columns, and headers. We present a novel deep-learning-\\nbased method' to cluster words in documents which we apply to detect\\nand recognize tables given the OCR output. We interpret table structure\\nbottom-up as a graph of relations between pairs of words (belonging to\\nthe same row, column, header, as well as to the same table) and use a\\ntransformer encoder model to predict its adjacency matrix. We demon-\\nstrate the performance of our method on the PubTables-1M dataset in-\\ntroduced in [18] as well as PubTabNet and FinTabNet datasets. Com-\\npared to the current state-of-the-art detection methods such as DETR [2]\\nand Faster R-CNN [16], our method achieves similar or better accuracy,\\nwhile requiring a significantly smaller model.\\nKeywords: Table detection \\xe2\\x80\\xa2 Table recognition \\xe2\\x80\\xa2 Supervised clustering\\n\\xe2\\x80\\xa2 Transformers\\n1\\nIntroduction\\nThe problem of detecting tables in documents and recognizing their structure is\\na long-standing application of machine learning techniques, as it is a necessary\\nstep towards automatically understanding the contents of a document. Early\\nmethods for example [20,11] involved heuristic-based approaches to detect tables\\nwithin a page and recognize its contents; but due to the variety of styles of both\\ntables and of the documents they appear in, these approaches cannot generalize\\narbitrarily. Therefore, recently a new wave of deep learning-based approaches\\nhas emerged.\\nMost of these approaches take the document image as input and directly deal\\nwith table detection and recognition as an object detection task. For instance,\\nthey apply off-the-shelf state-of-the-art algorithms such as DETR [2] or Faster R-\\nCNN [16] to the raw image. This approach, however, has several disadvantages:\\nfirst of all, the models tend to be large, and their application is expensive since\\n1 The code is released at https://github.com/SAP-samples/clustertabnet\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['ClusterTabNet: Supervised clustering method', 'for table detection and table structure']\n",
            "Authors: ['recognition', 'Marek Polewczykl and Marco Spinacil', 'SAP Business AT {marek . polewczyk , marco . spinaci }@sap . com']\n",
            "Authors: recognition, Marek Polewczykl and Marco Spinacil, SAP Business AT {marek . polewczyk , marco . spinaci }@sap . com \n",
            "\n",
            "b'A Multiclass Imbalanced Dataset Classification\\nof Symbols from Piping and Instrumentation\\nDiagrams*\\nLaura Jamiesonl , Carlos Francisco Moreno-Garciai[0000-0001 \\xe2\\x80\\x947218-9023] , and\\nEyad Elyanl [0000-0002-8342-9026]\\nRobert Gordon University, Aberdeen, Scotland AB10 7QB, UK,\\nCorresponding author email: c.moreno-garciaargu.ac.uk\\nAbstract. Engineering diagrams provide rich source of information and\\nare widely used across different industries. Recent years have seen grow-\\ning research interest in developing solutions for processing and analysing\\nthese diagrams using wide range of image-processing and computer vi-\\nsion techniques. In this paper, we first, present a new multiclass im-\\nbalanced dataset of symbols extracted from Piping and Instrumentation\\nDiagrams (P&IDs). The dataset contains 7,728 instances representing 48\\ndifferent types of engineering symbols and it is considered the first of its\\nkind in the research community. Second, we present a new method for\\nhandling multiclass imbalance classification based on class decomposi-\\ntion by means of unsupervised machine learning methods. Experiments\\nusing Convolutional Neural Networks showed that using class decompo-\\nsition significantly improves the classification performance that can be\\nachieved, without causing information loss, as it is the case with other\\nclass imbalance data sampling approaches.\\nKeywords: Piping and Instrumentation Diagrams\\n\\xe2\\x80\\xa2 Class Imbalance\\n\\xe2\\x80\\xa2\\nConvolutional Neural Networks\\n1\\nIntroduction\\nEngineering diagrams are used in a wide range of industries including electronics\\n[5], architecture [25] and the oil industry [9]. It is common for engineering dia-\\ngrams to be stored in an undigitised format [1], [23] and consequently, extracting\\ndata from these diagrams is a significant task. Piping and Instrumentation Di-\\nagrams (P&IDs) are one type of complex engineering diagram. P&IDs are used\\nto show equipment, connectors and associated technical details. A section of a\\nP&ID is shown in Figure 1. Extracting information from undigitised P&IDs can\\nbe a very time-consuming task, even for engineering specialists [27]. For example,\\nthe task of identifying the locations of one equipment type in a set of undigitised\\nP&IDs requires exhaustive manual search through each P&ID in the set [31].\\n* Supported by DNV.\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['A Multiclass Imbalanced Dataset Classification', 'of Symbols from Piping and Instrumentation']\n",
            "Authors: ['Diagrams*', 'Laura Jamiesonl , Carlos Francisco Moreno-Garciai[0000-0001 —7218-9023] , and', 'Eyad Elyanl [0000-0002-8342-9026]']\n",
            "Authors: Diagrams*, Laura Jamiesonl , Carlos Francisco Moreno-Garciai[0000-0001 —7218-9023] , and, Eyad Elyanl [0000-0002-8342-9026] \n",
            "\n",
            "b'Analysis of the Calibration of Handwriting Text\\nRecognition Models\\nEric Ayllon[\\xc2\\xb0\\xc2\\xb0\\xc2\\xb09-0005-7468-1712], Francisco J. Castellanos[0000-0001-9949-5522],\\nand Jorge Calvo-Zaragoza[0000-0003-3183-2232]\\nPattern Recognition and Artificial Intelligence Group, University of Alicante, Spain\\nAbstract. Current neural approaches for Handwritten Text Recogni-\\ntion (HTR) have proven to be successful in many settings, but their per-\\nformance can be unpredictable when facing new data. In this context,\\nit is essential to correctly estimate an approximate error of the target\\npredictions. To achieve this, the model must be well calibrated, meaning\\nthat the confidence values are sufficiently representative of the expected\\naccuracy. Calibration is a crucial aspect in practical applications of HTR,\\nbut the topic remains overly underexplored. In this paper, we fill this gap\\nby studying calibration in state-of-the-art HTR models, along with spe-\\ncific techniques for this purpose. We perform thorough experiments on\\nseveral datasets, both in a classic setting and in cross-collection scenar-\\nios. Our results report interesting conclusions about the calibration of\\nHTR, highlighting their strengths, weaknesses, and the extent to which\\nthe considered strategies improve results.\\nKeywords: Handwritten Text Recognition \\xe2\\x80\\xa2 Calibration \\xe2\\x80\\xa2 Convolutional\\nRecurrent Neural Network\\n1\\nIntroduction\\nThere exist countless handwritten documents scattered worldwide that exist only\\nin physical form. Many of these documents are currently inaccessible because\\nthey lack a digitized version. While the task of digitization seems straightfor-\\nward for humans, merely storing images is not enough; a transcription process\\nis also necessary to make the content really accessible and browsable through\\nanalysis and search algorithms [14]. Although transcription can be manually per-\\nformed, this is not feasible on a large-scale basis due to the huge demand for\\nhuman resources. The development of Handwritten Text Recognition (HTR) sys-\\ntems aims to automate this process with minimal human intervention, thereby\\nreducing costs for obtaining high-quality transcripts.\\nAdvancements in deep learning have led to a growing trend in using neural\\nnetworks to tackle HTR [22]. Their performance has been proven in several set-\\ntings, yielding high accuracy [2, 27, 31, 18]. However, while these algorithms may\\nseem suitable for performing HTR, they are not error-free [25]. In a real-world\\nscenario, identifying those samples with correct predictions or those for which\\nthe model might have provided errors can be crucial to achieving a complete and\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Analysis of the Calibration of Handwriting Text', 'Recognition Models']\n",
            "Authors: ['Eric Ayllon[°°°9-0005-7468-1712], Francisco J. Castellanos[0000-0001-9949-5522],', 'and Jorge Calvo-Zaragoza[0000-0003-3183-2232]', 'Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain']\n",
            "Authors: Eric Ayllon[°°°9-0005-7468-1712], Francisco J. Castellanos[0000-0001-9949-5522],, and Jorge Calvo-Zaragoza[0000-0003-3183-2232], Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain \n",
            "\n",
            "b\"One-shot Transformer-based Framework for\\nVisually-Rich Document Understanding\\nHuynh Vu The' [0000-0002-8980-283x]\\xc2\\xae, Van Pham Hoail [0009-0006-8867-9899]\\n1\\nand Jeff yang1[0009-0007-6847-0075]\\nCinnamon AI\\nhttps://cinnamon.is\\nAbstract. There is a growing need for efficient entity extraction (EE)\\nfrom business documents. While recent EE models have shown good ac-\\ncuracy for a variety of document templates, fine-tuning these models and\\nacquiring additional training data can be expensive. To address this prob-\\nlem, we propose a novel template-based system for the EE task which\\ndoes not require model fine-tuning for new entities and templates. The\\nsystem includes two one-shot transformer-based models: one for template\\nrecognition and the other for entity recognition. The document recogni-\\ntion model (OTDC) achieves high accuracy (over 93%) on more than 200\\ntemplates of public and private datasets. The entity recognition model\\n(OTER) outperforms recent zero-shot models with regards to the full set\\nof labeled entities in the public SROIE datasets. We have also gathered\\nand annotated the public RVL-CDIP and invoice datasets to showcase\\nthe generalization of our OTER models for the EE task across a wide\\nrange of document templates, containing both single and multiple-region\\nfields.\\nKeywords: Entity extraction \\xe2\\x80\\xa2 Visually-Rich document understanding\\n\\xe2\\x80\\xa2 One-shot approaches.\\n1\\nIntroduction\\nBusinesses handle a large number of Visually-Rich Documents (VrDs) in physi-\\ncal formats on a daily basis. Working with the physical documents can be time-\\nconsuming, which is why digital transformation processes are implemented to\\nimprove the efficiency of document-related tasks. The EE task from digital doc-\\numents has been gaining attention due to its potential applications in organizing,\\narchiving, and indexing a variety of documents. The methods for the EE task\\ncan be divided into two main categories: template-free and template-based ap-\\nproaches.\\nTemplate-free approaches [37, 39, 35, 23, 36,15, 32,16] for structured informa-\\ntion extraction (IE) remove the burden of template preparation in the training\\nphase. Although good results have been achieved, these approaches still face\\nchallenges related to data availability and high training and labeling costs. In\\nreal-world scenarios, sufficient training data is not always available while some\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['One-shot Transformer-based Framework for', 'Visually-Rich Document Understanding']\n",
            "Authors: [\"Huynh Vu The' [0000-0002-8980-283x]®, Van Pham Hoail [0009-0006-8867-9899]\", '1', 'and Jeff yang1[0009-0007-6847-0075]']\n",
            "Authors: Huynh Vu The' [0000-0002-8980-283x]®, Van Pham Hoail [0009-0006-8867-9899], 1, and Jeff yang1[0009-0007-6847-0075] \n",
            "\n",
            "b\"AltChart: Enhancing VLM-based Chart\\nSummarization Through Multi-Pretext Tasks\\nOmar Moured1'2, Jiaming Zhang1,2,*, M. Saquib Sarfrazl, and Rainer\\nStiefelhagen1'2\\n1 CV:HCI lab, Karlsruhe Institute of Technology, Germany.\\n2 ACCESSOKIT, Karlsruhe Institute of Technology, Germany.\\nffirstname.lastnamel@kit.edu\\nhttps://cvhci.anthropomatik.kit.edu/\\nAbstract. Chart summarization is a crucial task for blind and visu-\\nally impaired individuals as it is their primary means of accessing and\\ninterpreting graphical data. Crafting high-quality descriptions is chal-\\nlenging because it requires precise communication of essential details\\nwithin the chart without vision perception. Many chart analysis meth-\\nods, however, produce brief, unstructured responses that may contain\\nsignificant hallucinations, affecting their reliability for blind people. To\\naddress these challenges, this work presents three key contributions: (1)\\nWe introduce the AltChart dataset, comprising 10,000 real chart images,\\neach paired with a comprehensive summary that features long-context,\\nand semantically rich annotations. (2) We propose a new method for\\npretraining Vision-Language Models (VLMs) to learn fine-grained chart\\nrepresentations through training with multiple pretext tasks, yielding a\\nperformance gain with (-2.5%. (3) We conduct extensive evaluations of\\nfour leading chart summarization models, analyzing how accessible their\\ndescriptions are. Our dataset and codes are publicly available on our\\nproject page: https://github.com/moured/AltChart.\\nKeywords: Alternative Text \\xe2\\x80\\xa2 Text Semantics \\xe2\\x80\\xa2 Pretext Tasks\\n1\\nIntroduction\\nWe can find charts everywhere in newspapers, in scientific documents, and in\\nmany applications that track personal data. Charts provide compact and coher-\\nent information visualization and are often used to obtain significant insights\\nfrom complex data. Given the importance and diverse use cases for charts, it's\\nessential to guarantee equal access for a broad audience. This includes the 253\\nmillion Blind and Visually Impaired (BVI) individuals [1], who are often given\\nno choice in understanding charts [22].\\nVisualizations are accessible to individuals with blindness through two meth-\\nods: tactile modality, where data is rendered on printed materials with varied el-\\nevations [38,39,37]. However, the primary means by which they use is via textual\\n* corresponding author\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['AltChart: Enhancing VLM-based Chart', 'Summarization Through Multi-Pretext Tasks']\n",
            "Authors: [\"Omar Moured1'2, Jiaming Zhang1,2,*, M. Saquib Sarfrazl, and Rainer\", \"Stiefelhagen1'2\", '1 CV:HCI lab, Karlsruhe Institute of Technology, Germany.']\n",
            "Authors: Omar Moured1'2, Jiaming Zhang1,2,*, M. Saquib Sarfrazl, and Rainer, Stiefelhagen1'2, 1 CV:HCI lab, Karlsruhe Institute of Technology, Germany. \n",
            "\n",
            "b\"The Socface Project: Large-Scale Collection,\\nProcessing, and Analysis of a Century of French\\nCensuses\\nMelodie Boilletl [0000-0002-0618-7852] , Solene Tarridei [0000-0001-6174-9865] ,\\nYoann Schneiders, Bastien Abadiel, Lionel Kesztenbaum2, and Christopher\\nKermorvantl [0000-0002-7508-4080]\\n1 TEKLIA, Paris, France\\nboillet@teklia.com\\n2 Institut National d'Etudes Demographiques (INED) and Paris School of Economics\\n(PSE), France\\nlionel.kesztenbaum@ined.fr\\nAbstract. This paper presents a complete processing workflow for ex-\\ntracting information from French census lists from 1836 to 1936. These\\nlists contain information about individuals living in France and their\\nhouseholds. We aim at extracting all the information contained in these\\ntables using automatic handwritten table recognition. At the end of the\\nSocface project, in which our work is taking place, the extracted in-\\nformation will be redistributed to the departmental archives, and the\\nnominative lists will be freely available to the public, allowing anyone to\\nbrowse hundreds of millions of records. The extracted data will be used\\nby demographers to analyze social change over time, significantly im-\\nproving our understanding of French economic and social structures. For\\nthis project, we developed a complete processing workflow: large-scale\\ndata collection from French departmental archives, collaborative anno-\\ntation of documents, training of handwritten table text and structure\\nrecognition models, and mass processing of millions of images.\\nWe present the tools we have developed to easily collect and process\\nmillions of pages. We also show that it is possible to process such a wide\\nvariety of tables with a single table recognition model that uses the image\\nof the entire page to recognize information about individuals, categorize\\nthem and automatically group them into households. The entire process\\nhas been successfully used to process the documents of a departmental\\narchive, representing more than 450,000 images.\\nKeywords: Handwritten table recognition \\xe2\\x80\\xa2 Large-scale data collection\\n\\xe2\\x80\\xa2 Collaborative annotation.\\n1\\nThe Socface project\\nThe Socface projects involves archivists, demographers, and computer scientists\\nworking together to analyze French census documents and extract information\\n1 https://socface.site.ined.fr/\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['The Socface Project: Large-Scale Collection,', 'Processing, and Analysis of a Century of French']\n",
            "Authors: ['Censuses', 'Melodie Boilletl [0000-0002-0618-7852] , Solene Tarridei [0000-0001-6174-9865] ,', 'Yoann Schneiders, Bastien Abadiel, Lionel Kesztenbaum2, and Christopher']\n",
            "Authors: Censuses, Melodie Boilletl [0000-0002-0618-7852] , Solene Tarridei [0000-0001-6174-9865] ,, Yoann Schneiders, Bastien Abadiel, Lionel Kesztenbaum2, and Christopher \n",
            "\n",
            "b'WikiDT: Visual-based Table Recognition and Question\\nAnswering Dataset\\nHui Shit, Yusheng Xie2, Luis Goncalves3, Sicun Gaol, and Jishen Zhao1\\n1 University of California San Diego\\n2 Amazon AGI\\n3 AWS AI Labs\\n{hshi,\\nsicung,\\njzhao}@ucsd.edu,\\n{yushx,\\nluisgonc}@amazon.com\\nAbstract. Companies and organizations grapple with the daily burden of docu-\\nment processing. As manual handling is tedious and error-prune, automating this\\nprocess is a significant goal. In response to this demand, research on table extrac-\\ntion and information extraction from scanned documents in gaining increasing\\ntraction. These extractions are fulfilled by machine learning models that require\\nlarge-scale and realistic datasets for development. However, despite the clear need,\\nacquiring high-quality and comprehensive dataset can be costly. In this work, we\\nintroduce the WikiDT, a TableVQA dataset with hierarchical labels for model\\ndiagnosis and potentially benefit the research on sub-tasks, e.g. table recognition.\\nThis dataset boasts a massive collection of 70,919 images paired with a diverse set\\nof 159,905 tables, providing an extensive corpus for tacking question-answering\\ntasks. The creation of WikiDT is by extending the existing non-synthetic QA\\ndatasets, with a fully automated process with verified heuristics and manual qual-\\nity inspections, and therefore minimizes labeling effort and human errors. A novel\\nfocus of WikiDT and its design goal is to answer questions that require locating the\\ntarget information fragment and in-depth reasoning, given web-style document im-\\nages. We established the baseline performance on the TableVQA, table extraction,\\nand table retrieval task with recent state-of-the-art models. The results illustrate\\nthat WikiDT is yet solved by the existing models that work moderately well on\\nother VQA tasks, and also introduce advanced challenges on table extraction.\\n1\\nIntroduction\\nQuestion answering is widely accepted as an AI-completeness task, while visual question\\nanswering (VQA) is an alternative to the visual Turing test [23]. VQA tasks, which\\nrequire the integration of natural language and image understanding, attract tremendous\\ninterest from both computer vision and natural language processing communities. In\\ngeneral, VQA tasks can test a wide range of knowledge and inference skills, provided\\nthey can be related to information within an image. While knowledge in the wild world\\nis extensive, VQA tasks typically bound the domain of the images and questions to make\\nthe task practical. General VQA tasks restrict the image domain to daily-life images\\nwith commonly seen objects, such as GQA[ 15] and VQA-v2[12]; Scene-Text VQAs\\nconfine their questions to text information on the images; and document VQAs ask\\nquestions on the image-formed documents, for example, OCR-VQA[26], DocVQA[25],\\nand VisualMRC[41].\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['WikiDT: Visual-based Table Recognition and Question', 'Answering Dataset']\n",
            "Authors: ['Hui Shit, Yusheng Xie2, Luis Goncalves3, Sicun Gaol, and Jishen Zhao1', '1 University of California San Diego', '2 Amazon AGI']\n",
            "Authors: Hui Shit, Yusheng Xie2, Luis Goncalves3, Sicun Gaol, and Jishen Zhao1, 1 University of California San Diego, 2 Amazon AGI \n",
            "\n",
            "b'Font Impression Estimation in the Wild\\nKazuki Kitajima, Daichi Haraguchi[0000-0002-3109-9053]7 and\\nSeiichi Uchida[\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0\\xc2\\xb0-0001-8592-7566]\\nKyushu University, Fukuoka, Japan\\nuchida@ait.kyushu-u.ac.jp\\nAbstract. This paper addresses the challenging task of estimating font\\nimpressions from real font images. We use a font dataset with annota-\\ntion about font impressions and a convolutional neural network (CNN)\\nframework for this task. However, impressions attached to individual\\nfonts are often missing and noisy because of the subjective characteristic\\nof font impression annotation. To realize stable impression estimation\\neven with such a dataset, we propose an exemplar-based impression esti-\\nmation approach, which relies on a strategy of ensembling impressions of\\nexemplar fonts that are similar to the input image. In addition, we train\\nCNN with synthetic font images that mimic scanned word images so\\nthat CNN estimates impressions of font images in the wild. We evaluate\\nthe basic performance of the proposed estimation method quantitatively\\nand qualitatively. Then, we conduct a correlation analysis between book\\ngenres and font impressions on real book cover images; it is important\\nto note that this analysis is only possible with our impression estimation\\nmethod. The analysis reveals various trends in the correlation between\\nthem \\xe2\\x80\\x94 this fact supports a hypothesis that book cover designers care-\\nfully choose a font for a book cover considering the impression given by\\nthe font.\\nKeywords: Font impression \\xe2\\x80\\xa2 Impression estimation \\xe2\\x80\\xa2 Book covers.\\n1\\nIntroduction\\nEach font has its own style and gives various impressions according to the style.\\nFig. 1 shows examples of fonts and their impression tags. These examples are\\npicked up from a website 1, where font designers freely attach these impression\\ntags. It is noteworthy that each font has multiple and diverse impression tags.\\nFor example, in Fig. 1, the font tresci 11a has diverse tags, such as calligraphy,\\nelegant, wedding, etc. These impression tags are sometimes words describing\\nfont shapes directly (such as Sans-serif) rather than words representing abstract\\nideas, subjective feelings, and sensations. Throughout this paper, all of these\\ntags are referred to as impression tags without discrimination, unless there is\\nconfusion.2\\n1 https://www.1001freefonts.com/\\n2 In fact, discrimination of impression and non-impression tags is an interesting re-\\nsearch topic by itself. The difference between them is very ambiguous. For example,\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Font Impression Estimation in the Wild', 'Kazuki Kitajima, Daichi Haraguchi[0000-0002-3109-9053]7 and']\n",
            "Authors: ['Seiichi Uchida[°°°°-0001-8592-7566]', 'Kyushu University, Fukuoka, Japan', 'uchida@ait.kyushu-u.ac.jp']\n",
            "Authors: Seiichi Uchida[°°°°-0001-8592-7566], Kyushu University, Fukuoka, Japan, uchida@ait.kyushu-u.ac.jp \n",
            "\n",
            "b\"GraphMLLM: A Graph-based Multi-level\\nLayout Language-independent Model for\\nDocument Understanding\\nHe-Sen Dai1'2, Xiao-Hui Lie('), Fei Yin2, Xudong Yana, Shuqi Mei3, and\\nCheng-Lin Liu1'2\\n1 School of Artificial Intelligence,\\nUniversity of Chinese Academy of Sciences, Beijing, 100049, China\\n2 State Key Laboratory of Multimodal Artificial Intelligence Systems,\\nInstitute of Automation of Chinese Academy of Sciences, Beijing, 100190, China\\n3 T Lab, Tencent Map,\\nTencent Technology (Beijing) Co., Ltd., Beijing, 100193, China\\ndaihesen20@mails.ucas.ac.cn,\\nfxiaohui.li,\\nfyin,\\nliuclI@nlpr.ia.ac.cn,\\n{owenyan,\\nshawnmelIftencent.com\\nAbstract. Self-supervised multi-modal document pre-training for docu-\\nment knowledge learning shows superiority in various downstream tasks.\\nHowever, due to the diversity of document languages and structures,\\nthere is still room to better model various document layouts while effi-\\nciently utilizing the pre-trained language models. To this goal, this pa-\\nper proposes a Graph-based Multi-level Layout Language-independent\\nModel (GraphMLLM) which uses dual-stream structure to explore tex-\\ntual and layout information separately and cooperatively. Specifically,\\nGraphMLLM consists of a text stream which uses off-the-shelf pre-trained\\nlanguage model to explore textual semantics and a layout stream which\\nuses multi-level graph neural network (GNN) to model hierarchical page\\nlayouts. Through the cooperation of the text stream and layout stream,\\nGraphMLLM can model multi-level page layouts more comprehensively\\nand improve the performance of language-independent document pre-\\ntrained model. Experimental results show that compared with previ-\\nous state-of-the-art methods, GraphMLLM yields higher performance on\\ndownstream visual information extraction (VIE) tasks after pre-training\\non less documents. Code and model will be available at https : //github .\\ncom/HSDai/GraphMLLM.\\nKeywords: Visual information extraction\\n\\xe2\\x80\\xa2 Self-supervised pre-training\\n\\xe2\\x80\\xa2 Multi-level page layouts.\\n1\\nIntroduction\\nAs an important task in Visual Document Understanding (VDU), Visual Infor-\\nmation Extraction (VIE) focuses on automated information extraction through\\nSemantic Entity Recognition (SER) and Relationship Extraction (RE) from\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['GraphMLLM: A Graph-based Multi-level', 'Layout Language-independent Model for']\n",
            "Authors: ['Document Understanding', \"He-Sen Dai1'2, Xiao-Hui Lie('), Fei Yin2, Xudong Yana, Shuqi Mei3, and\", \"Cheng-Lin Liu1'2\"]\n",
            "Authors: Document Understanding, He-Sen Dai1'2, Xiao-Hui Lie('), Fei Yin2, Xudong Yana, Shuqi Mei3, and, Cheng-Lin Liu1'2 \n",
            "\n",
            "b'Light-Weight Multi-Modality Feature Fusion\\nNetwork for Visually-Rich Document\\nUnderstanding\\nJeff yangi [0009-0007-6847-0075] , Huynh Vu Thei [0000-0002-8980-283X] 124 and\\nHai Luu Tu.& [0009-0008-7524-951X]\\nCinnamon AI\\nhttps://cinnamon.is\\nAbstract. Entity extraction (EE) is an important task in visually-rich\\ndocument understanding (VrDU) which leverages multi-modal features\\nof text, layout, and image. Recent transformer-based architectures en-\\nable an effective fusion of these features, showing great performance on\\nthe EE task. However, these models are heavy, leading to substantially\\nhigh training cost and low inference speed. Thus, we propose a light-\\nweight transformer-based model (named LMFFN) with a novel layout-\\nself-attention layout-aware multi-modal fusion mechanism that allows an\\nefficient entity extraction. Specifically, the proposed framework uses just\\na simple pre-training objective coupled with an effective batch imple-\\nmentation. In addition, no constraints are required with regard to the\\ninput sequence length or the reading order. This relaxation gives our\\nmodel an advantage when it comes to camera and skewed documents, as\\nwe observed a 7% Fl-score improvement when we compared our model\\nto previous SOTA models on camera data. Evaluation results of three\\npublic datasets (CORD, SROIE, and XFUND) show that our proposed\\narchitecture achieves competitive performance compared to recent SOTA\\nmodels while having 5 to 10 times fewer parameters.\\n1\\nIntroduction\\nVisually-rich documents (VrDs) are tightly connected to our daily life from in-\\nsurance, logistic, finance to travel and leisure. As they contain lots of useful\\ninformation, there are demands for archiving, indexing or structuring document\\ninformation in form of digital format. In visually-rich documents understand-\\ning (VrDU), documents of various formats (PDF, images, etc.) are digitized,\\nstructured and organized automatically, which plays a crucial role in developing\\ndigital transformation processes in daily business operation.\\nThe VrDU tasks such as semantic entity extraction and entity linking [18] are\\nbuilt upon the inherent complexities of co-existed multi-modal features. Hence,\\nan effective feature learning and fusion of multi-modal data play an important\\nrole to achieve good performance. Another challenge of VrDU tasks is the lack\\nof data. While there is a variety of document templates, some of them are\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Light-Weight Multi-Modality Feature Fusion', 'Network for Visually-Rich Document']\n",
            "Authors: ['Understanding', 'Jeff yangi [0009-0007-6847-0075] , Huynh Vu Thei [0000-0002-8980-283X] 124 and', 'Hai Luu Tu.& [0009-0008-7524-951X]']\n",
            "Authors: Understanding, Jeff yangi [0009-0007-6847-0075] , Huynh Vu Thei [0000-0002-8980-283X] 124 and, Hai Luu Tu.& [0009-0008-7524-951X] \n",
            "\n",
            "b'Mining and Analyzing Statistical Information\\nfrom Untranscribed Form Images\\n1,2 (E) [0000-0002-5541 \\xe2\\x80\\x948231]\\nJose Andresl,2(\\xc2\\xae) [0000-0002-5541-8231] , Alejandro H.\\nToselli1,2 [0000-0001-6955-9249] , and Enrique Vida11,2 [0000-0003-4579-5196]\\n1 PRHLT Research Center\\nUniversitat Politecnica de Valencia, Spain\\n{joanmo2,ahector,evidal}Qprhlt.upv.es\\ntranSkriptorium AI, Valencia, Spain\\n{joanmo,ahector,evidal}@transkriptorium.com\\nAbstract. Large amounts of historical structured documents are avail-\\nable in archives around the world. Here, we are interested in automat-\\nically extracting statistical information of selected attributes contained\\nin these documents. To this end, we propose a pipeline relying on prob-\\nabilistic indexing and machine learning to mine and analyze relevant in-\\nformation contained in historical form images. These ideas are assessed\\non a large collection of images containing almost 295 000 forms with re-\\nsults showing the adequateness of the proposed methods to perform the\\nbig-data analytic task considered.\\nKeywords: Probabilistic Indexing, Big Data, Structured Documents,\\nHandwritten Text Recognition\\n1\\nIntroduction\\nBig-data statistical information analysis is a common practice today to mine\\nknowledge from large sets of documents [4,27] that are available in electronic\\nform. Of particular interest are documents structured into tables or forms [14].\\nIn tables, column and/or row headers provide \"key\" or \"attribute\" clues, the\\n\"values\" of which lay in the corresponding columns or rows, respectively. In\\nforms, attributes are typically described by pre-printed text and values are typed\\nor handwritten, geometrically next to them. In this work we focus on forms.\\nPaper forms are generally scanned into high-resolution digital images which is\\na convenient procedure to help preserve the information, since the paper support\\ndecays over time and is difficult to safeguard in good condition. Digital text\\nimages additionally offer the possibility of applying automated techniques such as\\nOCR to convert the image strokes representing characters into actual electronic\\ntext. However, as just mentioned, forms are typically hybrid documents. The pre-\\nprinted parts are repetitive across documents and generally easy to recognize.\\nNevertheless, typewritten text can be elusive to OCR [11] because of degraded\\nsupport, low-quality typewriters, eroded types, or poor carbon copying. And, of\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Mining and Analyzing Statistical Information', 'from Untranscribed Form Images']\n",
            "Authors: ['1,2 (E) [0000-0002-5541 —8231]', 'Jose Andresl,2(®) [0000-0002-5541-8231] , Alejandro H.', 'Toselli1,2 [0000-0001-6955-9249] , and Enrique Vida11,2 [0000-0003-4579-5196]']\n",
            "Authors: 1,2 (E) [0000-0002-5541 —8231], Jose Andresl,2(®) [0000-0002-5541-8231] , Alejandro H., Toselli1,2 [0000-0001-6955-9249] , and Enrique Vida11,2 [0000-0003-4579-5196] \n",
            "\n",
            "b\"Geometric-aware Control in Diffusion Model for\\nHandwritten Chinese Font Generation\\nyao1,2 [0000-0001-9311-033/\\n1,2 [0009-0006-3539-5163]\\nGang\\nKemeng Zhao\\n1,2 [0009-0007-2676-2360f,\\nDing1'2 [0000-0001-5808-8795]\\n:\\n\\xe2\\x80\\xa2\\nChengyu Deng\\nNing7 Tianqi\\nzhao1,2 [0009-0001-0433-1662] , Yao Tao3 [0009-0007-2666-8315] , and Liangrui\\npeng1,2 0E0[0000-0001-7793-1039]\\n1 Department of Electronic Engineering, Tsinghua University, Beijing, China\\n2 Beijing National Research Center for Information Science and Technology,\\nTsinghua University, Beijing, China\\n3 Huawei Noah's Ark Lab, Shenzhen, China\\n{yg19,zkm23,deng-cy20,dn22,zhaotq20}@mails.tsinghua.edu.cn\\ntaoyao2@huawei.com\\npenglr@tsinghua.edu.cn\\nAbstract. With the progress of artificial intelligence generated content\\n(AIGC) technologies, style-controlled image generation methods based\\non diffusion models have shown promising performance on different tasks\\nincluding Chinese font generation. However, most of the current diffu-\\nsion model-based Chinese font generation methods focus on generating\\nprinted Chinese character images and pay insufficient attention to the ge-\\nometric characteristics of character images with variations in handwriting\\nstyle. This paper investigates incorporating geometric-aware control into\\ndiffusion models to generate target character images with new content\\ntemplates and given style references. First, a deformable attention mech-\\nanism is utilized in the content aggregation process to adapt to variations\\nin handwritten character structure. Second, the edge contour of the new\\ncontent template is incorporated into the style control process. Third,\\nthe geometric information such as the corner points of a target charac-\\nter is utilized to weight the image reconstruction loss function for better\\ncontrol of character shape. The effectiveness of the proposed method for\\nhandwritten Chinese font generation is validated on the CASIA-HWDB\\n1.0/1.1 Chinese handwriting dataset.\\nKeywords: handwritten font generation \\xe2\\x80\\xa2 diffusion model \\xe2\\x80\\xa2 geometric-\\naware control.\\n1\\nIntroduction\\nHandwritten Chinese font generation is an important artificial intelligence gen-\\nerated content (AIGC) technique that can benefit many applications, such as\\nhandwriting imitation, ancient character restoration and data augmentation for\\nhandwriting recognition, etc. Compared with printed font types, the style vari-\\nations in handwritten Chinese font images are more diverse, which brings great\\nchallenges to style control in the generation process.\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Geometric-aware Control in Diffusion Model for', 'Handwritten Chinese Font Generation']\n",
            "Authors: ['yao1,2 [0000-0001-9311-033/', '1,2 [0009-0006-3539-5163]', 'Gang']\n",
            "Authors: yao1,2 [0000-0001-9311-033/, 1,2 [0009-0006-3539-5163], Gang \n",
            "\n",
            "b'Test Time Augmentation as a Defense Against\\nAdversarial Attacks on Online Handwriting*\\nYoh Yamashita and Brian Kenji Iwana[ww-0002-5146-6818]\\nGraduate School of Information Science and Electrical Engineering\\nKyushu University, Fukuoka, Japan\\nyoh.yamashita@human.ait.kyushu-u.ac.jp,\\niwana@ait.kyushu-u.ac.jp\\nAbstract. Neural networks have been shown to be weak against adver-\\nsarial attacks. This study examines the effects of adversarial attacks on\\nonline handwritten characters and proposes a method to defend against\\nsuch attacks. In order to make temporal neural networks more robust to\\nadversarial attacks, we propose using Test Time Augmentation (TTA).\\nTTA combines the predictions of transformed inputs with a trained clas-\\nsifier. We adapt TTA and propose its usage to make temporal neural\\nnetworks more robust to adversarial attacks. The proposed method is\\nevaluated using online handwritten characters and against four state-of-\\nthe-art adversarial attacks. We demonstrate that the nontraditional use\\nof TTA can be used to protect against these attacks for almost no cost.\\nKeywords: Online Handwriting \\xe2\\x80\\xa2 Adversarial Attacks \\xe2\\x80\\xa2 Defense\\n1\\nIntroduction\\nOnline handwriting recognition is the process of converting handwritten input,\\ntypically captured using a stylus or touchscreen device, into digital text. In this\\nway, online handwriting consists of a time series made of coordinates, strokes, or\\nother features. This is in contrast to offline handwriting recognition which uses\\nstatic images of handwritten text. Online handwriting recognition is important\\ndue to its applications in various domains, such as handwriting input on touch-\\nscreens, biometric authentication and signature verification systems, and smart\\nwhiteboards.\\nNotably, online handwriting recognition and temporal neural networks have\\na close history. Deep neural networks have had widespread successes in pattern\\nrecognition [35] and time series classification [41], including online handwriting\\nrecognition. For example, early uses of novel neural networks have been used\\nfor online handwriting in a variety of languages, such as English [10, 17], Chi-\\nnese [38], Japanese [32], Arabic [30], Devanagari [21], Mongolian [42], etc. Also,\\nonline handwriting was one of the early uses of Connectionist Temporal Classi-\\nfication (CTC) [10]. Today, most state-of-the-art online handwriting recognition\\nsystems incorporate neural networks [8].\\n* This work was partially supported by MEXT-Japan (Grant No. 23K16949).\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Test Time Augmentation as a Defense Against', 'Adversarial Attacks on Online Handwriting*']\n",
            "Authors: ['Yoh Yamashita and Brian Kenji Iwana[ww-0002-5146-6818]', 'Graduate School of Information Science and Electrical Engineering', 'Kyushu University, Fukuoka, Japan']\n",
            "Authors: Yoh Yamashita and Brian Kenji Iwana[ww-0002-5146-6818], Graduate School of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan \n",
            "\n",
            "b\"CREPE: Coordinate-Aware End-to-End\\nDocument Parser\\nYamato Okamoto1,2* t [0009-0009-2153-3782]\\nYoungmin Baek1,2*$ [0000-0001-7001-4641]\\nGeewook Kiml [0009-0001-6713-3858] , Ryota Nakao1,2 [0009-0001 \\xe2\\x80\\x946692 \\xe2\\x80\\x946952]\\nDongHyun Kim]. [0000-0001-9033-5231] , Moon Bin Yimi [0000-0002-7272-2198] ,\\n1 [0000-0002-8509-9163] ,\\n1 [0000-0003-4962-8977]\\nSeunghyun Park\\nand Bado Lee\\n1 NAVER Cloud AI, Seongnam-si, Gyeonggi-do, Korea\\nokamoto.yamato.w15@kyoto-u.jp\\nfyoungmin.baek,\\ngw.kim,\\ndong.hyun}@navercorp.com\\n{moonbin.yim,\\nseung.park,\\nbado.lee}@navercorp.com\\n2 LINE WORKS, Shibuya-city, Toyko, Japan\\nnakao.ryota@line-works.com\\nAbstract. In this study, we formulate an OCR-free sequence genera-\\ntion model for visual document understanding (VDU). Our model not\\nonly parses text from document images but also extracts the spatial\\ncoordinates of the text based on the multi-head architecture. Named as\\nCoordinate-aware End-to-end Document Parser (CREPE), our method\\nuniquely integrates these capabilities by introducing a special token for\\nOCR text, and token-triggered coordinate decoding. We also proposed\\na weakly-supervised framework for cost-efficient training, requiring only\\nparsing annotations without high-cost coordinate annotations. Our ex-\\nperimental evaluations demonstrate CREPE's state-of-the-art performances\\non document parsing tasks. Beyond that, CREPE's adaptability is fur-\\nther highlighted by its successful usage in other document understanding\\ntasks such as layout analysis, document visual question answering, and\\nso one. CREPE's abilities including OCR and semantic parsing not only\\nmitigate error propagation issues in existing OCR-dependent methods, it\\nalso significantly enhance the functionality of sequence generation mod-\\nels, ushering in a new era for document understanding studies.\\nKeywords: Visual Document Understanding. Document Parsing. Doc-\\nument Information Extraction. Optical Character Recognition. End-to-\\nEnd Transformer. Weakly Supervised Learning.\\n* Equal contribution.\\nt Current Affiliation: CyberAgent, Inc.\\nt Corresponding author.\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['CREPE: Coordinate-Aware End-to-End', 'Document Parser']\n",
            "Authors: ['Yamato Okamoto1,2* t [0009-0009-2153-3782]', 'Youngmin Baek1,2*$ [0000-0001-7001-4641]', 'Geewook Kiml [0009-0001-6713-3858] , Ryota Nakao1,2 [0009-0001 —6692 —6952]']\n",
            "Authors: Yamato Okamoto1,2* t [0009-0009-2153-3782], Youngmin Baek1,2*$ [0000-0001-7001-4641], Geewook Kiml [0009-0001-6713-3858] , Ryota Nakao1,2 [0009-0001 —6692 —6952] \n",
            "\n",
            "b'Typographic Text Generation with Off-the-Shelf\\nDiffusion Model\\nKhayTze Peong, Seiichi Uchida[0000-0001-8592-7566], and Daichi\\n\\xe2\\x80\\x940002-3109-9053]\\nHaraguchiP\\xc2\\xb0\\xc2\\xb0\\nKyushu University, Fukuoka, Japan\\nuchida@ait.kyushu-u.ac.jp\\nAbstract. Recent diffusion-based generative models show promise in\\ntheir ability to generate text images, but limitations in specifying the\\nstyles of the generated texts render them insufficient in the realm of\\ntypographic design. This paper proposes a typographic text generation\\nsystem to add and modify text on typographic designs while specifying\\nfont styles, colors, and text effects. The proposed system is a novel combi-\\nnation of two off-the-shelf methods for diffusion models, ControlNet and\\nBlended Latent Diffusion. The former functions to generate text images\\nunder the guidance of edge conditions specifying stroke contours. The\\nlatter blends latent noise in Latent Diffusion Models (LDM) to add ty-\\npographic text naturally onto an existing background. We first show that\\ngiven appropriate text edges, ControlNet can generate texts in specified\\nfonts while incorporating effects described by prompts. We further in-\\ntroduce text edge manipulation as an intuitive and customizable way to\\nproduce texts with complex effects such as \"shadows\" and \"reflections\".\\nFinally, with the proposed system, we successfully add and modify texts\\non a predefined background while preserving its overall coherence.\\nKeywords: Text generation\\n\\xe2\\x80\\xa2 diffusion models\\n\\xe2\\x80\\xa2 image blending.\\n1\\nIntroduction\\nTypography is a fundamental element in textual communication. It helps convey\\na message, evokes specific emotions, and guides the reader through the content.\\nEffective typographic design encompasses a thoughtful combination of carefully\\nchosen font styles, text colors, and harmonizing backgrounds to ensure that the\\nmessage is clear, legible, and easily understood. Even subtle changes in typog-\\nraphy may lead to significant alterations in how one perceives a design.\\nFor instance, the text \"Goodnight Already\" on the book cover in Fig. 1 (Orig-\\ninal) provides insight into the content, with the chosen display font style con-\\nveying a playful impression that informs the reader about the genre of the book.\\nConversely, a change in font style in Fig. 1 (c) may evoke a scary impression,\\neven though the content remains the same. Furthermore, adding eye-catching ef-\\nfects to text is also useful to emphasize certain text, as demonstrated in Figs. 1\\n(d) and (e). These effects may cause readers to unconsciously pay more attention\\nto these texts than flat texts without any effect.\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Typographic Text Generation with Off-the-Shelf', 'Diffusion Model']\n",
            "Authors: ['KhayTze Peong, Seiichi Uchida[0000-0001-8592-7566], and Daichi', '—0002-3109-9053]', 'HaraguchiP°°']\n",
            "Authors: KhayTze Peong, Seiichi Uchida[0000-0001-8592-7566], and Daichi, —0002-3109-9053], HaraguchiP°° \n",
            "\n",
            "b\"Reading Order Independent Metrics for\\nInformation Extraction in Handwritten\\nDocuments\\nDavid Villanova-Aparisii* [0000-0003-2301-6673] , Solene\\nTarride2* [0000-0001 -6174-9865] , Carlos-D.\\nMartinez-Hinarejosl [0000-0002-6139-2891] , Veronica\\nRomero3 [0000-0002-1721-5732] , Christopher Kermorvant2 [0000-0002-7508-4080]\\nand Moises Pastor-Gadeal\\n1 PRHLT Research Center, Universitat Politecnica de Valencia, Cami de Vera, s/n,\\nValencia 46021, Spain\\n2 TEKLIA, Paris, France\\n3 Departament d'Informatica, Universitat de Valencia, Valencia 46010, Spain\\nAbstract. Information Extraction processes in handwritten documents\\ntend to rely on obtaining an automatic transcription and performing\\nNamed Entity Recognition (NER) over such transcription. For this rea-\\nson, in publicly available datasets, the performance of the systems is usu-\\nally evaluated with metrics particular to each dataset. Moreover, most\\nof the metrics employed are sensitive to reading order errors. Therefore,\\nthey do not reflect the expected final application of the system and in-\\ntroduce biases in more complex documents. In this paper, we propose\\nand publicly release a set of reading order independent metrics tailored\\nto Information Extraction evaluation in handwritten documents. In our\\nexperimentation, we perform an in-depth analysis of the behavior of the\\nmetrics to recommend what we consider to be the minimal set of metrics\\nto evaluate a task correctly.\\nKeywords: Information Extraction \\xe2\\x80\\xa2 Evaluation Metrics \\xe2\\x80\\xa2 Reading Or-\\nder \\xe2\\x80\\xa2 Full Page Recognition \\xe2\\x80\\xa2 End-to-End Model\\n1\\nIntroduction\\nInformation Extraction (IE) refers to the identification of parts of digital text\\nthat contain specific knowledge. In particular, the task of Named Entity Recog-\\nnition (NER) [20,7] aims to tag parts of the text that contain specific seman-\\ntic information with their corresponding categories. Traditionally, the approach\\nused to address this task on both handwritten or typeset documents involves a\\nfirst step of automatic transcription of the document, followed by tagging of the\\nresulting digital text [31,21,1]. Coupled approaches that solve the task in one\\nstep have also seen successful application in recent years [2,3,28] at solving the\\n* Authors contributed equally to this work.\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Reading Order Independent Metrics for', 'Information Extraction in Handwritten']\n",
            "Authors: ['Documents', 'David Villanova-Aparisii* [0000-0003-2301-6673] , Solene', 'Tarride2* [0000-0001 -6174-9865] , Carlos-D.']\n",
            "Authors: Documents, David Villanova-Aparisii* [0000-0003-2301-6673] , Solene, Tarride2* [0000-0001 -6174-9865] , Carlos-D. \n",
            "\n",
            "b'Weakly Supervised Training for\\nHologram Verification in Identity Documents\\nGlen Pouliquen1\\'20, Guillaume Chironle, Joseph Chazalon2G,\\nThierry Geraud20, and Ahmad Montaser Awalle\\n1 IDnow AI & ML Center of Excellence, Cesson-Sevigne, France\\nname.surname@idnow.io\\n2 EPITA Research Lab. (LRE), Le Kremlin-Bicetre, France\\nname.surname@epita.fr\\nAbstract. We propose a method to remotely verify the authenticity of\\nOptically Variable Devices (OVDs), often referred to as \"holograms\",\\nin identity documents. Our method processes video clips captured with\\nsmartphones under common lighting conditions, and is evaluated on two\\npublic datasets: MIDV-HOLO and MIDV-2020. Thanks to a weakly-\\nsupervised training, we optimize a feature extraction and decision pipeline\\nwhich achieves a new leading performance on MIDV-HOLO, while main-\\ntaining a high recall on documents from MIDV-2020 used as attack sam-\\nples. It is also the first method, to date, to effectively address the photo\\nreplacement attack task, and can be trained on either genuine samples,\\nattack samples, or both for increased performance. By enabling to verify\\nOVD shapes and dynamics with very little supervision, this work opens\\nthe way towards the use of massive amounts of unlabeled data to build ro-\\nbust remote identity document verification systems on commodity smart-\\nphones. Code is available at https : //github com/EPITAResearchLab/\\npouliquen.24.icdar.\\nKeywords: Know Your Consumer (KYC) \\xe2\\x80\\xa2 Identity Documents \\xe2\\x80\\xa2 Holo-\\ngram Verification \\xe2\\x80\\xa2 Weakly Supervised Learning \\xe2\\x80\\xa2 Contrastive Loss\\n1\\nIntroduction\\nOften called KYC (Know Your Customer), remotely verifying the authenticity\\nof identity documents is a critical point for building online trust. This is an\\nincreasingly regulated process which relies on identity documents, among other\\nproofs, to establish the link between an online identity and a real state-backed\\none. This linking usually requires checking that the document is original and\\nwas not altered. The photography of the bearer is of paramount importance\\nhere to ensure that the user of a remote system is the intended one. Optically\\nvariable devices (OVDs), commonly referred to as \"holograms\" and illustrated\\nin Figure 2, are powerful tools to secure physical documents in line with the\\nrecommendation of the EU council [14], among others. Built using elaborated\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['Weakly Supervised Training for', 'Hologram Verification in Identity Documents']\n",
            "Authors: [\"Glen Pouliquen1'20, Guillaume Chironle, Joseph Chazalon2G,\", 'Thierry Geraud20, and Ahmad Montaser Awalle', '1 IDnow AI & ML Center of Excellence, Cesson-Sevigne, France']\n",
            "Authors: Glen Pouliquen1'20, Guillaume Chironle, Joseph Chazalon2G,, Thierry Geraud20, and Ahmad Montaser Awalle, 1 IDnow AI & ML Center of Excellence, Cesson-Sevigne, France \n",
            "\n",
            "b'A Historical Handwritten Dataset for Ethiopic\\nOCR with Baseline Models and Human-level\\nPerformance\\nBirhanu Hailu Belays [0009-0000-6709-7773] , Isabelle\\nGuyon1,2,3[0000-0002-9266-1783] , Tadele Mengiste4 [0009-0006-7739-2462] ,\\nBezawork Tilahun4[0009-0007-7360-3231] , Marcus LiwiCki5 [0000-0003-4029-6574] ,\\n4 [0000-0003-0392-9088] ,\\n1 [0000-0002-8992-8192]\\nTesfa Tegegne\\nand Romain Egele\\n1 LISN, Universite Paris-Saclay, France\\n2 Google Brain, USA\\n3 ChaLearn, USA\\n4 Bahir Dar University, Ethiopia\\n5 Lulea University of Technology, Sweden\\nbirhanu-hailu.belay@upsaclay . fr. guyon@chalearn.org\\nmarcus .liwicki@ltu. se. romain.egele@inria.fr\\nItadele .mengiste ,bezawork .tilahun,tesf a.tegegnel@bdu. edu. et\\nAbstract. This paper introduces a new OCR dataset for historical\\nhandwritten Ethiopic script, characterized by a unique syllabic writ-\\ning system, low-resource availability, and complex orthographic diacrit-\\nics. The dataset consists of roughly 80,000 annotated text-line images\\nfrom 1700 pages of 18th to 20th century documents, including a train-\\ning set with text-line images from the 19th to 20th century and two\\ntest sets. One is distributed similarly to the training set with nearly\\n6,000 text-line images, and the other contains only images from the\\n18th century manuscripts, with around 16,000 images. The former test\\nset allows us to check baseline performance in the classical IID setting\\n(Independently and Identically Distributed), while the latter addresses\\na more realistic setting in which the test set is drawn from a differ-\\nent distribution than the training set (Out-Of-Distribution or 00D).\\nMultiple annotators labeled all text-line images for the HHD-Ethiopic\\ndataset, and an expert supervisor double-checked them. We assessed\\nhuman-level recognition performance and compared it with state-of-the-\\nart (SOTA) OCR models using the Character Error Rate (CER) and\\nNormalized Edit Distance (NED) metrics. Our results show that the\\nmodel performed comparably to human-level recognition on the 18th\\ncentury test set and outperformed humans on the IID test set. However,\\nthe unique challenges posed by the Ethiopic script, such as detecting\\ncomplex diacritics, still present difficulties for the models. Our baseline\\nevaluation and dataset will encourage further research on Ethiopic script\\nrecognition. The dataset and source code can be accessed at https :\\n//github.com/bdu-birhanu/HHD-Ethiopic.\\nKeywords: Historical Ethiopic script \\xe2\\x80\\xa2 Human-level recognition perfor-\\nmance \\xe2\\x80\\xa2 HHD-Ethiopic \\xe2\\x80\\xa2 Normalized edit distance \\xe2\\x80\\xa2 Text recognition\\n'\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['A Historical Handwritten Dataset for Ethiopic', 'OCR with Baseline Models and Human-level']\n",
            "Authors: ['Performance', 'Birhanu Hailu Belays [0009-0000-6709-7773] , Isabelle', 'Guyon1,2,3[0000-0002-9266-1783] , Tadele Mengiste4 [0009-0006-7739-2462] ,']\n",
            "Authors: Performance, Birhanu Hailu Belays [0009-0000-6709-7773] , Isabelle, Guyon1,2,3[0000-0002-9266-1783] , Tadele Mengiste4 [0009-0006-7739-2462] , \n",
            "\n",
            "b\"GDP: Generic Document Pretraining to\\nImprove Document Understanding\\nAkkshita Trivedil [0009-0007-6344-8581] , Akarsh Upadhyayl [0009-0003-6474-4500]\\nRudrabha Mukhopadhyay2 [0009-0000-6628-7065] , and Santanu\\nChaudhury1 [0000-0002-5488-7773]\\n1 Indian Institute of Technology Jodhpur,India\\n{trivedi.2,\\nupadhyay.4}@iitj.ac.in,schaudhury@gmail.com\\n2 International Institute of Information Technology, Hyderabad, India\\nradrabha.m@research.iiit.ac.in\\nAbstract. In this paper, we propose a novel pretraining approach for\\ndocument analysis that advances beyond conventional methods. The ap-\\nproach, called the GDPerformer, trains a suite of unique architectures\\nto predict both masked OCR tokens and masked OCR bounding boxes,\\nfostering the network to learn document semantics such as structure and\\nlanguage. Our experiments with GDPerformervl and GDPerformerv2\\nshow enhanced performance on various downstream tasks, including Se-\\nmantic Entity Recognition and Extraction and Multi-Modal Document\\nClassification with minimal task-specific data and generalization to a\\nwide range of documents. Furthermore, our pretrained features exhibit\\nrobustness in handling noisy documents and can be easily extended to\\nmultiple languages. Our experiments indicate that the proposed pretrain-\\ning strategy requires only 50K document images, making it particularly\\nbeneficial for low-resource languages.\\nKeywords: Pretraining \\xe2\\x80\\xa2 Document Analysis \\xe2\\x80\\xa2 Masked Language Mod-\\neling \\xe2\\x80\\xa2 Document Transformers \\xe2\\x80\\xa2 Document Imaging \\xe2\\x80\\xa2 Deep Learning \\xe2\\x80\\xa2\\nMasked Layout Modeling\\n1\\nIntroduction\\nPrinted documentation has been critical to human civilization for centuries, serv-\\ning as a means of recording and conveying information. From ancient clay tablets\\nto today's books, printed documents have played a vital role in preserving and\\ndisseminating knowledge. With the advent of the printing press, mass produc-\\ntion of printed materials became possible, leading to an explosion of information.\\nPrinted documents continue to play a crucial role in daily life, providing news,\\nknowledge, and entertainment. But with the increasing digitization of data, the\\nability to automatically extract information from document images has become\\nmore important. Using AI and machine learning algorithms, document analysis\\nplays a critical role in this, enabling the understanding of even unstructured doc-\\nument images, such as handwritten notes. It is, therefore, essential to improve\\ndocument analysis algorithms.\\n\"\n",
            "metadata {'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': '', 'encryption': None}\n",
            "Title: ['GDP: Generic Document Pretraining to', 'Improve Document Understanding']\n",
            "Authors: ['Akkshita Trivedil [0009-0007-6344-8581] , Akarsh Upadhyayl [0009-0003-6474-4500]', 'Rudrabha Mukhopadhyay2 [0009-0000-6628-7065] , and Santanu', 'Chaudhury1 [0000-0002-5488-7773]']\n",
            "Authors: Akkshita Trivedil [0009-0007-6344-8581] , Akarsh Upadhyayl [0009-0003-6474-4500], Rudrabha Mukhopadhyay2 [0009-0000-6628-7065] , and Santanu, Chaudhury1 [0000-0002-5488-7773] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'id': '0049.pdf',\n",
              "  'title': 'Synthesizing Realistic Data for Table Recognition',\n",
              "  'authors': 'Qiyu Houl [0009-0009-4150-9907], Jun wangl [0000-0002-9515-076X] P.A/) Meixuan, Qiao2, and Lujun Tianl, 1 iWudao Tech'},\n",
              " 1: {'id': '0044.pdf',\n",
              "  'title': '1 DLAFormer: An End-to-End Transformer For',\n",
              "  'authors': 'Document Layout Analysis, Jiawei Wang1,2,*,t, Kai Hui ,2,*, t , and Qiang Huo2, Department of EEIS, University of Science and Technology of China, Hefei, China'},\n",
              " 2: {'id': '0086.pdf',\n",
              "  'title': 'Handwritten Document Recognition Using Pre-trained Vision Transformers',\n",
              "  'authors': 'Daniel Parresl [0000-0002-2078-0329] , Dan Aniteii [0000-0001 —8288-6009] , and, Roberto Paredes1,2 [0000-0002-5192-0021], 1 PRHLT Research Center, Universitat Politecnica Valencia, Valencia, Spain'},\n",
              " 3: {'id': '0020.pdf',\n",
              "  'title': 'Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source',\n",
              "  'authors': 'Library, Solene Tarride[0000-0001-6174-9865], -‘,'},\n",
              " 4: {'id': '0089.pdf',\n",
              "  'title': 'Global-SEG: Text Semantic Segmentation Based on Global Semantic Pair Relations',\n",
              "  'authors': 'Wenjun Sunl [0009-0002-7857-8737] , Hanh Thi Hong, Tran1,2,3 [0000-0002-5993-1630] Carlos-Emiliano, Gonzalez-Gallardol [0000-0002-0787-2990] , mithael'},\n",
              " 5: {'id': '0083.pdf',\n",
              "  'title': 'Impression-CLIP: Contrastive Shape-Impression Embedding for Fonts',\n",
              "  'authors': 'Yugo Kubota, Daichi Haraguchi[0000-0002-3109-9053], and, Seiichi Uchida[°°°°-0001-8592-7566], Kyushu University, Fukuoka, Japan yugo . kubota@human . ait . kyushu-u . ac . j p'},\n",
              " 6: {'id': '0027.pdf',\n",
              "  'title': 'Towards End-to-End Semi-Supervised Table Detection with Semantic Aligned Matching Transformer',\n",
              "  'authors': \"Tahira Shehzadi*1,2,3[0000-0002-7052-979X] Shalini Sarode1,3 [0009-0007-9968-4068],, Didier Stricker1'2'3, and Muhammad Zeshan Afza11,2,3[0000-0002-0536-6867], 1 Department of Computer Science, Technical University of Kaiserslautern, 67663, Germany\"},\n",
              " 7: {'id': '0045.pdf',\n",
              "  'title': 'UniVIE: A Unified Label Space Approach to Visual Information Extraction from Form-like',\n",
              "  'authors': \"Documents, Kai Hul'2'*'t , Jiawei Wang1,2,*, Weihong Line,*,, Zhuoyao Zhong2'*, Lei Sung'*, and Qiang Huo2\"},\n",
              " 8: {'id': '0060.pdf',\n",
              "  'title': 'Multi-Cell Decoder and Mutual Learning for Table Structure and Character Recognition',\n",
              "  'authors': 'Takaya Kawakatsui[0000-0003-1285-2748], Preferred Networks, Inc., 1-6-1 Otemachi, Chiyoda, Tokyo, Japan., kat.nii.ac.jp@gmail.com'},\n",
              " 9: {'id': '0051.pdf',\n",
              "  'title': 'Source-Free Domain Adaptation for Optical Music Recognition',\n",
              "  'authors': \"Adrian Rose1161, Eliseo Fuentes-Martinezi , Maria Alfaro-Contreras1 , David, Rizo1'2, and Jorge Calvo-Zaragozal, 1 Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain\"},\n",
              " 10: {'id': '0042.pdf',\n",
              "  'title': 'LMTextSpotter: Towards Better Scene Text Spotting with Language Modeling in Transformer',\n",
              "  'authors': \"Xin Xia1'2, Guodong Ding2, and Siyuan Lie, 1 College of Computer Science and Technology, Zhejiang University, 2 HiThink RoyalFlush Information Network Co. Ltd., China\"},\n",
              " 11: {'id': '0007.pdf',\n",
              "  'title': 'Binarizing Documents by Leveraging both Space and Frequency',\n",
              "  'authors': 'Fabio Quattrini[0009-0004-3244-6186], Vittorio Pippip009-000l-7365-6348], Silvia Cascianelli[0000-0001-7885-6050], and Rita Cucchiara[0003-0002-2239-283X], University of Modena and Reggio Emilia, Modena, Italy'},\n",
              " 12: {'id': '0025.pdf',\n",
              "  'title': '1 3',\n",
              "  'authors': 'A Hybrid Approach for Document Layout, Analysis in Document images, Tahira Shehzadi*1,2,3[0000-0002-7052-979X]7 Didier Stricker1,2,37 and'},\n",
              " 13: {'id': '0043.pdf',\n",
              "  'title': 'Dynamic Relation Transformer for Contextual Text Block Detection',\n",
              "  'authors': 'Jiawei Wang1\\'3\\'*\\'t, Shunchi Zhang2,3\\'*\\'t, Kai Hul\\'3\\'*\\'t,, Chixiang Ma\", Zhuoyao Zhong\", Lei Sun\", and Qiang Huo3, 1 University of Science and Technology of China'},\n",
              " 14: {'id': '0094.pdf',\n",
              "  'title': 'Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting',\n",
              "  'authors': 'Omar Hamedl, Souhail Bakkali4 [0000-0001-9383-3842] , Matthew, Blaschko2 [0000-0002-2640 —181X], Sien Moens2 [0000-0002-3732-9323]'},\n",
              " 15: {'id': '0069.pdf',\n",
              "  'title': 'MOoSE: Multi-Orientation Sharing Experts for Open-set Scene Text Recognition',\n",
              "  'authors': 'Chang Liu, Simon Corbille, and Elisa H Barney Smith, Machine Learning Group, Lulea University of Technology, Sweden, chang.liu@ltu.se,'},\n",
              " 16: {'id': '0092.pdf',\n",
              "  'title': 'More and Less: Enhancing Abundance and Refining Redundancy for Text-Prior-Guided',\n",
              "  'authors': 'Scene Text Image Super-Resolution, Wei Yang1,200, Yihong Luo3G, Mayire, 1,r) and Askar Hamdulla1,26'},\n",
              " 17: {'id': '0055.pdf',\n",
              "  'title': 'End to End Table Transformer Yun Young Choi*[0000-0002-6017-258X], Taehoon Kim* [0009-0004-4676-8784]',\n",
              "  'authors': 'Namwook Kim0°°9-0006-2348-490017 Taehee Lee[°°°9-0006-7701-2335], and, Seongho Joe[°°°°-0003-1419-9930]'},\n",
              " 18: {'id': '0071.pdf',\n",
              "  'title': 'Deep Learning-Driven Innovative Model for Generating Functional Knowledge Units',\n",
              "  'authors': \"Pan Qiangangl, Hu Yahongl*, Xie Youbai2'3, Meng Xianghui2, and Zhang, Yilun2, 1 Zhejiang University of Technology, Hangzhou, Zhejiang, China\"},\n",
              " 19: {'id': '0084.pdf',\n",
              "  'title': 'Privacy-Aware Document Visual Question Answering',\n",
              "  'authors': 'Ruben Tito\", Khanh Nguyen\\', Marlon Tobaben*2, Raouf Kerkouche3,, Mohamed Ali Souibguil, Kangsoo June, Joonas Ja110, Vincent Poulain, D\\'Andecy5, Aurelie Josephs, Lei Kang1, Ernest Valvenyl, Antti Honkela2,'},\n",
              " 20: {'id': '0054.pdf',\n",
              "  'title': 'A region-based approach for layout analysis of music score images in scarce data scenarios',\n",
              "  'authors': 'Francisco J. Castellanos[0000-0001-9949-5522] Juan P., Martinez-Esteso[0009-0006-3246-3781] Alejandro, Galan-Cuenca[°°°°-0002-5799-6270] , and Antonio Javier'},\n",
              " 21: {'id': '0081.pdf',\n",
              "  'title': 'Multi-task Learning for License Plate Recognition in Unconstrained Scenarios',\n",
              "  'authors': 'Zhen-Lun Mot, Song-Lu Chenl, Qi Liu1, Feng Chen2, and Xu-Cheng Yinl, 1 University of Science and Technology Beijing, Beijing, China, {songluchen,xuchengyin}Oustb.edu.cn'},\n",
              " 22: {'id': '0016.pdf',\n",
              "  'title': 'A Real-Time Scene Uyghur Text Detection Network Based on Feature Complementation',\n",
              "  'authors': 'Mayire Ibrayim1,2[0000-0002-8766-0647], Mengmeng Chen1,2[0000-0003-4604-0051],, Askar Ham.dulla2,3[0000-0002-2321-308X], Jianjun Kang1,2 , and Chunhu, Zhang1,2[0009-0002-2564-0998]'},\n",
              " 23: {'id': '0004.pdf',\n",
              "  'title': 'SAGHOG: Self-Supervised Autoencoder for Generating HOG Features for Writer Retrieval',\n",
              "  'authors': 'Marco PeerO, Florian Klebere, and Robert Sablatnigwo, Computer Vision Lab, TU Wien, {mpeer, kleber,'},\n",
              " 24: {'id': '0075.pdf',\n",
              "  'title': 'ConClue: Conditional Clue Extraction for Multiple Choice Question Answering*',\n",
              "  'authors': \"Wangli Yang1, Jie Yangl*, Wanqing Li', and Yi Guo2, 1 School of Computing and Information Technology,, University of Wollongong, Australia\"},\n",
              " 25: {'id': '0077.pdf',\n",
              "  'title': 'Progressive Evolution from Single-Point to Polygon for Scene Text',\n",
              "  'authors': 'Linger Denglt, Mingxin Huang2t, Xudong Xie1, Yuliang Liul*, Lianwen Jin2,, and Xiang Bail, 1 Huazhong University of Science and Technology, WuHan, China'},\n",
              " 26: {'id': '0030.pdf',\n",
              "  'title': 'Visual Prompt Learning for Chinese Handwriting Recognition',\n",
              "  'authors': 'Gang yao1,2 [0000-0001-9311-0337], Ning Ding1,2 [0000-0001-5808-8795]7 Tianqi, Zhao1,2 [0009-0001-0433-1662], Kemeng Zhao1,2 [0009-0006-3539-5163], pei, Tang1,2[0000-0002— 1176-0086] 7 Yao Ta03 [0009-0007-2666-8315]7 and Liangrui'},\n",
              " 27: {'id': '0038.pdf',\n",
              "  'title': 'Self-supervised Pre-training of Text Recognizers Martin Kimi [0000-0001—6853-0508] and Michal Hradigi [0000-0002-6364-129X]',\n",
              "  'authors': 'Faculty of Information Technology, Brno University of Technology,, Brno, Czech Republic, {ikiss,hradis}@fit.vutbr.cz'},\n",
              " 28: {'id': '0037.pdf',\n",
              "  'title': 'Drawing the Line: Deep Segmentation for Extracting Art from Ancient Etruscan Mirrors*',\n",
              "  'authors': 'Rafael SterzingerC, Simon BrennerO, and Robert Sablatnige, Computer Vision Lab, TU Wien, Vienna, AUT, Ifirstname.lastnamel@tuwien.ac.at'},\n",
              " 29: {'id': '0013.pdf',\n",
              "  'title': 'Revisiting N-Gram Models: Their Impact in Modern Neural Networks for Handwritten Text',\n",
              "  'authors': 'Recognition, Solene Tarride[0000-0001-6174-9865] and Christopher, Kermorvant[0000-0002-7508-4080]'},\n",
              " 30: {'id': '0064.pdf',\n",
              "  'title': 'Learning to Kern: Set-wise Estimation of Optimal Letter Space',\n",
              "  'authors': 'Kei Nakatsuru and Seiichi Uchida[°°°°-0001-8592-7566], Kyushu University, Fukuoka, Japan, uchida@ait.kyushu-u.ac.jp'},\n",
              " 31: {'id': '0095.pdf',\n",
              "  'title': 'ClusterTabNet: Supervised clustering method for table detection and table structure',\n",
              "  'authors': 'recognition, Marek Polewczykl and Marco Spinacil, SAP Business AT {marek . polewczyk , marco . spinaci }@sap . com'},\n",
              " 32: {'id': '0018.pdf',\n",
              "  'title': 'A Multiclass Imbalanced Dataset Classification of Symbols from Piping and Instrumentation',\n",
              "  'authors': 'Diagrams*, Laura Jamiesonl , Carlos Francisco Moreno-Garciai[0000-0001 —7218-9023] , and, Eyad Elyanl [0000-0002-8342-9026]'},\n",
              " 33: {'id': '0058.pdf',\n",
              "  'title': 'Analysis of the Calibration of Handwriting Text Recognition Models',\n",
              "  'authors': 'Eric Ayllon[°°°9-0005-7468-1712], Francisco J. Castellanos[0000-0001-9949-5522],, and Jorge Calvo-Zaragoza[0000-0003-3183-2232], Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain'},\n",
              " 34: {'id': '0074.pdf',\n",
              "  'title': 'One-shot Transformer-based Framework for Visually-Rich Document Understanding',\n",
              "  'authors': \"Huynh Vu The' [0000-0002-8980-283x]®, Van Pham Hoail [0009-0006-8867-9899], 1, and Jeff yang1[0009-0007-6847-0075]\"},\n",
              " 35: {'id': '0032.pdf',\n",
              "  'title': 'AltChart: Enhancing VLM-based Chart Summarization Through Multi-Pretext Tasks',\n",
              "  'authors': \"Omar Moured1'2, Jiaming Zhang1,2,*, M. Saquib Sarfrazl, and Rainer, Stiefelhagen1'2, 1 CV:HCI lab, Karlsruhe Institute of Technology, Germany.\"},\n",
              " 36: {'id': '0046.pdf',\n",
              "  'title': 'The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French',\n",
              "  'authors': 'Censuses, Melodie Boilletl [0000-0002-0618-7852] , Solene Tarridei [0000-0001-6174-9865] ,, Yoann Schneiders, Bastien Abadiel, Lionel Kesztenbaum2, and Christopher'},\n",
              " 37: {'id': '0082.pdf',\n",
              "  'title': 'WikiDT: Visual-based Table Recognition and Question Answering Dataset',\n",
              "  'authors': 'Hui Shit, Yusheng Xie2, Luis Goncalves3, Sicun Gaol, and Jishen Zhao1, 1 University of California San Diego, 2 Amazon AGI'},\n",
              " 38: {'id': '0079.pdf',\n",
              "  'title': 'Font Impression Estimation in the Wild Kazuki Kitajima, Daichi Haraguchi[0000-0002-3109-9053]7 and',\n",
              "  'authors': 'Seiichi Uchida[°°°°-0001-8592-7566], Kyushu University, Fukuoka, Japan, uchida@ait.kyushu-u.ac.jp'},\n",
              " 39: {'id': '0059.pdf',\n",
              "  'title': 'GraphMLLM: A Graph-based Multi-level Layout Language-independent Model for',\n",
              "  'authors': \"Document Understanding, He-Sen Dai1'2, Xiao-Hui Lie('), Fei Yin2, Xudong Yana, Shuqi Mei3, and, Cheng-Lin Liu1'2\"},\n",
              " 40: {'id': '0019.pdf',\n",
              "  'title': 'Light-Weight Multi-Modality Feature Fusion Network for Visually-Rich Document',\n",
              "  'authors': 'Understanding, Jeff yangi [0009-0007-6847-0075] , Huynh Vu Thei [0000-0002-8980-283X] 124 and, Hai Luu Tu.& [0009-0008-7524-951X]'},\n",
              " 41: {'id': '0034.pdf',\n",
              "  'title': 'Mining and Analyzing Statistical Information from Untranscribed Form Images',\n",
              "  'authors': '1,2 (E) [0000-0002-5541 —8231], Jose Andresl,2(®) [0000-0002-5541-8231] , Alejandro H., Toselli1,2 [0000-0001-6955-9249] , and Enrique Vida11,2 [0000-0003-4579-5196]'},\n",
              " 42: {'id': '0031.pdf',\n",
              "  'title': 'Geometric-aware Control in Diffusion Model for Handwritten Chinese Font Generation',\n",
              "  'authors': 'yao1,2 [0000-0001-9311-033/, 1,2 [0009-0006-3539-5163], Gang'},\n",
              " 43: {'id': '0078.pdf',\n",
              "  'title': 'Test Time Augmentation as a Defense Against Adversarial Attacks on Online Handwriting*',\n",
              "  'authors': 'Yoh Yamashita and Brian Kenji Iwana[ww-0002-5146-6818], Graduate School of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan'},\n",
              " 44: {'id': '0022.pdf',\n",
              "  'title': 'CREPE: Coordinate-Aware End-to-End Document Parser',\n",
              "  'authors': 'Yamato Okamoto1,2* t [0009-0009-2153-3782], Youngmin Baek1,2*$ [0000-0001-7001-4641], Geewook Kiml [0009-0001-6713-3858] , Ryota Nakao1,2 [0009-0001 —6692 —6952]'},\n",
              " 45: {'id': '0080.pdf',\n",
              "  'title': 'Typographic Text Generation with Off-the-Shelf Diffusion Model',\n",
              "  'authors': 'KhayTze Peong, Seiichi Uchida[0000-0001-8592-7566], and Daichi, —0002-3109-9053], HaraguchiP°°'},\n",
              " 46: {'id': '0097.pdf',\n",
              "  'title': 'Reading Order Independent Metrics for Information Extraction in Handwritten',\n",
              "  'authors': 'Documents, David Villanova-Aparisii* [0000-0003-2301-6673] , Solene, Tarride2* [0000-0001 -6174-9865] , Carlos-D.'},\n",
              " 47: {'id': '0047.pdf',\n",
              "  'title': 'Weakly Supervised Training for Hologram Verification in Identity Documents',\n",
              "  'authors': \"Glen Pouliquen1'20, Guillaume Chironle, Joseph Chazalon2G,, Thierry Geraud20, and Ahmad Montaser Awalle, 1 IDnow AI & ML Center of Excellence, Cesson-Sevigne, France\"},\n",
              " 48: {'id': '0036.pdf',\n",
              "  'title': 'A Historical Handwritten Dataset for Ethiopic OCR with Baseline Models and Human-level',\n",
              "  'authors': 'Performance, Birhanu Hailu Belays [0009-0000-6709-7773] , Isabelle, Guyon1,2,3[0000-0002-9266-1783] , Tadele Mengiste4 [0009-0006-7739-2462] ,'},\n",
              " 49: {'id': '0050.pdf',\n",
              "  'title': 'GDP: Generic Document Pretraining to Improve Document Understanding',\n",
              "  'authors': 'Akkshita Trivedil [0009-0007-6344-8581] , Akarsh Upadhyayl [0009-0003-6474-4500], Rudrabha Mukhopadhyay2 [0009-0000-6628-7065] , and Santanu, Chaudhury1 [0000-0002-5488-7773]'}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use pandas to make a dataset from the dictionary containing text metadata\n",
        "import pandas as pd\n",
        "df=pd.DataFrame(text_metadata).T\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vY8V6cy10Ydt",
        "outputId": "377b2506-17e2-4093-90d6-37b00a801f8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                              title  \\\n",
              "0  0049.pdf  Synthesizing Realistic Data for Table Recognition   \n",
              "1  0044.pdf         1 DLAFormer: An End-to-End Transformer For   \n",
              "2  0086.pdf  Handwritten Document Recognition Using Pre-tra...   \n",
              "3  0020.pdf  Improving Automatic Text Recognition with Lang...   \n",
              "4  0089.pdf  Global-SEG: Text Semantic Segmentation Based o...   \n",
              "\n",
              "                                             authors  \n",
              "0  Qiyu Houl [0009-0009-4150-9907], Jun wangl [00...  \n",
              "1  Document Layout Analysis, Jiawei Wang1,2,*,t, ...  \n",
              "2  Daniel Parresl [0000-0002-2078-0329] , Dan Ani...  \n",
              "3  Library, Solene Tarride[0000-0001-6174-9865], -‘,  \n",
              "4  Wenjun Sunl [0009-0002-7857-8737] , Hanh Thi H...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02d0fc2b-4469-4203-a092-e58f40fc93da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0049.pdf</td>\n",
              "      <td>Synthesizing Realistic Data for Table Recognition</td>\n",
              "      <td>Qiyu Houl [0009-0009-4150-9907], Jun wangl [00...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0044.pdf</td>\n",
              "      <td>1 DLAFormer: An End-to-End Transformer For</td>\n",
              "      <td>Document Layout Analysis, Jiawei Wang1,2,*,t, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0086.pdf</td>\n",
              "      <td>Handwritten Document Recognition Using Pre-tra...</td>\n",
              "      <td>Daniel Parresl [0000-0002-2078-0329] , Dan Ani...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020.pdf</td>\n",
              "      <td>Improving Automatic Text Recognition with Lang...</td>\n",
              "      <td>Library, Solene Tarride[0000-0001-6174-9865], -‘,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0089.pdf</td>\n",
              "      <td>Global-SEG: Text Semantic Segmentation Based o...</td>\n",
              "      <td>Wenjun Sunl [0009-0002-7857-8737] , Hanh Thi H...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02d0fc2b-4469-4203-a092-e58f40fc93da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-02d0fc2b-4469-4203-a092-e58f40fc93da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-02d0fc2b-4469-4203-a092-e58f40fc93da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b5031ac6-b8e6-4f0c-8adc-3af169eb7283\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b5031ac6-b8e6-4f0c-8adc-3af169eb7283')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b5031ac6-b8e6-4f0c-8adc-3af169eb7283 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"0043.pdf\",\n          \"0059.pdf\",\n          \"0064.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Dynamic Relation Transformer for Contextual Text Block Detection\",\n          \"GraphMLLM: A Graph-based Multi-level Layout Language-independent Model for\",\n          \"Learning to Kern: Set-wise Estimation of Optimal Letter Space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Jiawei Wang1'3'*'t, Shunchi Zhang2,3'*'t, Kai Hul'3'*'t,, Chixiang Ma\\\", Zhuoyao Zhong\\\", Lei Sun\\\", and Qiang Huo3, 1 University of Science and Technology of China\",\n          \"Document Understanding, He-Sen Dai1'2, Xiao-Hui Lie('), Fei Yin2, Xudong Yana, Shuqi Mei3, and, Cheng-Lin Liu1'2\",\n          \"Kei Nakatsuru and Seiichi Uchida[\\u00b0\\u00b0\\u00b0\\u00b0-0001-8592-7566], Kyushu University, Fukuoka, Japan, uchida@ait.kyushu-u.ac.jp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2 : Pre-Processing the data\n",
        "Now we have imported our dataset we can start \"cleaning up\" our data so that it is of better use, when we start feeding it to our model. FIrstly, some key actrions we have to take :\n",
        "\n",
        "- Remove missing values (if they exist)\n",
        "- Remove duplicate data entries (if they exist)\n",
        "- Lower casing all data entries\n",
        "- Remove punctuation from all data entries We perform actions 3 and 4 so that for example the word \"Hello!\" is treated the same way and has the same numerical value as the word \"HELLO\" and the word \"hello.\"\n",
        "- Remove all stopwords since they dont provide valuable information in analysing the text\n",
        "- Remove all URLs from the text since they dont provide any info or have any value in our dataset\n"
      ],
      "metadata": {
        "id": "oB5Tjizj00_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get additional info about our data\n",
        "df.info()"
      ],
      "metadata": {
        "id": "3ObfLaTK07u_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7643c4f-0657-4019-ab18-2fc8edea0c6d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 50 entries, 0 to 49\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   id       50 non-null     object\n",
            " 1   title    50 non-null     object\n",
            " 2   authors  50 non-null     object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop duplicates and check the shape of our data again\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"the shape of our data when duplicates are removed is {df.shape}\")"
      ],
      "metadata": {
        "id": "nMoh59t7084M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb732cc-644e-451a-a4aa-0fa5943e3bb2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the shape of our data when duplicates are removed is (50, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now to check for missing values in each column\n",
        "print(f\"total missing values in our dataset are : {df.isnull().sum()}\")\n",
        "\n",
        "#remove all missing values and check the shape\n",
        "df.dropna(axis=0, inplace=True)\n",
        "print(f\"the shape of our data after removing all null/missing values is {df.shape}\")"
      ],
      "metadata": {
        "id": "BJ87q7D508y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23033ac-9418-48f2-e123-34a77674c8d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total missing values in our dataset are : id         0\n",
            "title      0\n",
            "authors    0\n",
            "dtype: int64\n",
            "the shape of our data after removing all null/missing values is (50, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lower casing all columns. We will be initializing 2 new columns that will correspond to the pre-processed values of our titles and authors.\n",
        "#we want to save the original values in order to use them in the end by inserting them in the outputs json file\n",
        "df['title_preprocessed']=df[\"title\"].str.lower()\n",
        "df['authors_preprocessed']=df['authors'].str.lower()\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "my8dfk7HrrTh",
        "outputId": "98e37930-bccb-4f17-c381-f3509d4ea408"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                              title  \\\n",
              "0  0049.pdf  Synthesizing Realistic Data for Table Recognition   \n",
              "1  0044.pdf         1 DLAFormer: An End-to-End Transformer For   \n",
              "2  0086.pdf  Handwritten Document Recognition Using Pre-tra...   \n",
              "3  0020.pdf  Improving Automatic Text Recognition with Lang...   \n",
              "4  0089.pdf  Global-SEG: Text Semantic Segmentation Based o...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  Qiyu Houl [0009-0009-4150-9907], Jun wangl [00...   \n",
              "1  Document Layout Analysis, Jiawei Wang1,2,*,t, ...   \n",
              "2  Daniel Parresl [0000-0002-2078-0329] , Dan Ani...   \n",
              "3  Library, Solene Tarride[0000-0001-6174-9865], -‘,   \n",
              "4  Wenjun Sunl [0009-0002-7857-8737] , Hanh Thi H...   \n",
              "\n",
              "                                  title_preprocessed  \\\n",
              "0  synthesizing realistic data for table recognition   \n",
              "1         1 dlaformer: an end-to-end transformer for   \n",
              "2  handwritten document recognition using pre-tra...   \n",
              "3  improving automatic text recognition with lang...   \n",
              "4  global-seg: text semantic segmentation based o...   \n",
              "\n",
              "                                authors_preprocessed  \n",
              "0  qiyu houl [0009-0009-4150-9907], jun wangl [00...  \n",
              "1  document layout analysis, jiawei wang1,2,*,t, ...  \n",
              "2  daniel parresl [0000-0002-2078-0329] , dan ani...  \n",
              "3  library, solene tarride[0000-0001-6174-9865], -‘,  \n",
              "4  wenjun sunl [0009-0002-7857-8737] , hanh thi h...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba6705e1-1712-4a79-bdd2-dcb229f97b11\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>title_preprocessed</th>\n",
              "      <th>authors_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0049.pdf</td>\n",
              "      <td>Synthesizing Realistic Data for Table Recognition</td>\n",
              "      <td>Qiyu Houl [0009-0009-4150-9907], Jun wangl [00...</td>\n",
              "      <td>synthesizing realistic data for table recognition</td>\n",
              "      <td>qiyu houl [0009-0009-4150-9907], jun wangl [00...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0044.pdf</td>\n",
              "      <td>1 DLAFormer: An End-to-End Transformer For</td>\n",
              "      <td>Document Layout Analysis, Jiawei Wang1,2,*,t, ...</td>\n",
              "      <td>1 dlaformer: an end-to-end transformer for</td>\n",
              "      <td>document layout analysis, jiawei wang1,2,*,t, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0086.pdf</td>\n",
              "      <td>Handwritten Document Recognition Using Pre-tra...</td>\n",
              "      <td>Daniel Parresl [0000-0002-2078-0329] , Dan Ani...</td>\n",
              "      <td>handwritten document recognition using pre-tra...</td>\n",
              "      <td>daniel parresl [0000-0002-2078-0329] , dan ani...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020.pdf</td>\n",
              "      <td>Improving Automatic Text Recognition with Lang...</td>\n",
              "      <td>Library, Solene Tarride[0000-0001-6174-9865], -‘,</td>\n",
              "      <td>improving automatic text recognition with lang...</td>\n",
              "      <td>library, solene tarride[0000-0001-6174-9865], -‘,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0089.pdf</td>\n",
              "      <td>Global-SEG: Text Semantic Segmentation Based o...</td>\n",
              "      <td>Wenjun Sunl [0009-0002-7857-8737] , Hanh Thi H...</td>\n",
              "      <td>global-seg: text semantic segmentation based o...</td>\n",
              "      <td>wenjun sunl [0009-0002-7857-8737] , hanh thi h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba6705e1-1712-4a79-bdd2-dcb229f97b11')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba6705e1-1712-4a79-bdd2-dcb229f97b11 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba6705e1-1712-4a79-bdd2-dcb229f97b11');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f3983ec3-cbce-4047-9f6d-eb3554b94851\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3983ec3-cbce-4047-9f6d-eb3554b94851')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f3983ec3-cbce-4047-9f6d-eb3554b94851 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"0043.pdf\",\n          \"0059.pdf\",\n          \"0064.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Dynamic Relation Transformer for Contextual Text Block Detection\",\n          \"GraphMLLM: A Graph-based Multi-level Layout Language-independent Model for\",\n          \"Learning to Kern: Set-wise Estimation of Optimal Letter Space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Jiawei Wang1'3'*'t, Shunchi Zhang2,3'*'t, Kai Hul'3'*'t,, Chixiang Ma\\\", Zhuoyao Zhong\\\", Lei Sun\\\", and Qiang Huo3, 1 University of Science and Technology of China\",\n          \"Document Understanding, He-Sen Dai1'2, Xiao-Hui Lie('), Fei Yin2, Xudong Yana, Shuqi Mei3, and, Cheng-Lin Liu1'2\",\n          \"Kei Nakatsuru and Seiichi Uchida[\\u00b0\\u00b0\\u00b0\\u00b0-0001-8592-7566], Kyushu University, Fukuoka, Japan, uchida@ait.kyushu-u.ac.jp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"dynamic relation transformer for contextual text block detection\",\n          \"graphmllm: a graph-based multi-level layout language-independent model for\",\n          \"learning to kern: set-wise estimation of optimal letter space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"jiawei wang1'3'*'t, shunchi zhang2,3'*'t, kai hul'3'*'t,, chixiang ma\\\", zhuoyao zhong\\\", lei sun\\\", and qiang huo3, 1 university of science and technology of china\",\n          \"document understanding, he-sen dai1'2, xiao-hui lie('), fei yin2, xudong yana, shuqi mei3, and, cheng-lin liu1'2\",\n          \"kei nakatsuru and seiichi uchida[\\u00b0\\u00b0\\u00b0\\u00b0-0001-8592-7566], kyushu university, fukuoka, japan, uchida@ait.kyushu-u.ac.jp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuation besides ','\n",
        "import string\n",
        "\n",
        "# Define punctuation to remove, excluding the comma\n",
        "PUNCT_TO_REMOVE = string.punctuation.replace(',', '')\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation except commas\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "df['title_preprocessed'] = df['title_preprocessed'].apply(lambda text: remove_punctuation(text))\n",
        "df['authors_preprocessed']=df['authors_preprocessed'].apply(lambda text: remove_punctuation(text))\n",
        "df['title'] = df['title'].apply(lambda text: remove_punctuation(text))\n",
        "df['authors']=df['authors'].apply(lambda text: remove_punctuation(text))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "_PdbBQ5Ur0Q4",
        "outputId": "e378bb94-03da-4cff-fb30-ab7b1b722bd0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                              title  \\\n",
              "0  0049.pdf  Synthesizing Realistic Data for Table Recognition   \n",
              "1  0044.pdf            1 DLAFormer An EndtoEnd Transformer For   \n",
              "2  0086.pdf  Handwritten Document Recognition Using Pretrai...   \n",
              "3  0020.pdf  Improving Automatic Text Recognition with Lang...   \n",
              "4  0089.pdf  GlobalSEG Text Semantic Segmentation Based on ...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  Qiyu Houl 0009000941509907, Jun wangl 00000002...   \n",
              "1  Document Layout Analysis, Jiawei Wang1,2,,t, K...   \n",
              "2  Daniel Parresl 0000000220780329 , Dan Aniteii ...   \n",
              "3        Library, Solene Tarride0000000161749865, ‘,   \n",
              "4  Wenjun Sunl 0009000278578737 , Hanh Thi Hong, ...   \n",
              "\n",
              "                                  title_preprocessed  \\\n",
              "0  synthesizing realistic data for table recognition   \n",
              "1            1 dlaformer an endtoend transformer for   \n",
              "2  handwritten document recognition using pretrai...   \n",
              "3  improving automatic text recognition with lang...   \n",
              "4  globalseg text semantic segmentation based on ...   \n",
              "\n",
              "                                authors_preprocessed  \n",
              "0  qiyu houl 0009000941509907, jun wangl 00000002...  \n",
              "1  document layout analysis, jiawei wang1,2,,t, k...  \n",
              "2  daniel parresl 0000000220780329 , dan aniteii ...  \n",
              "3        library, solene tarride0000000161749865, ‘,  \n",
              "4  wenjun sunl 0009000278578737 , hanh thi hong, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7181bcba-7e9f-4c9c-8a66-04722acf2e32\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>title_preprocessed</th>\n",
              "      <th>authors_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0049.pdf</td>\n",
              "      <td>Synthesizing Realistic Data for Table Recognition</td>\n",
              "      <td>Qiyu Houl 0009000941509907, Jun wangl 00000002...</td>\n",
              "      <td>synthesizing realistic data for table recognition</td>\n",
              "      <td>qiyu houl 0009000941509907, jun wangl 00000002...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0044.pdf</td>\n",
              "      <td>1 DLAFormer An EndtoEnd Transformer For</td>\n",
              "      <td>Document Layout Analysis, Jiawei Wang1,2,,t, K...</td>\n",
              "      <td>1 dlaformer an endtoend transformer for</td>\n",
              "      <td>document layout analysis, jiawei wang1,2,,t, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0086.pdf</td>\n",
              "      <td>Handwritten Document Recognition Using Pretrai...</td>\n",
              "      <td>Daniel Parresl 0000000220780329 , Dan Aniteii ...</td>\n",
              "      <td>handwritten document recognition using pretrai...</td>\n",
              "      <td>daniel parresl 0000000220780329 , dan aniteii ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020.pdf</td>\n",
              "      <td>Improving Automatic Text Recognition with Lang...</td>\n",
              "      <td>Library, Solene Tarride0000000161749865, ‘,</td>\n",
              "      <td>improving automatic text recognition with lang...</td>\n",
              "      <td>library, solene tarride0000000161749865, ‘,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0089.pdf</td>\n",
              "      <td>GlobalSEG Text Semantic Segmentation Based on ...</td>\n",
              "      <td>Wenjun Sunl 0009000278578737 , Hanh Thi Hong, ...</td>\n",
              "      <td>globalseg text semantic segmentation based on ...</td>\n",
              "      <td>wenjun sunl 0009000278578737 , hanh thi hong, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7181bcba-7e9f-4c9c-8a66-04722acf2e32')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7181bcba-7e9f-4c9c-8a66-04722acf2e32 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7181bcba-7e9f-4c9c-8a66-04722acf2e32');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0ae1013f-767e-45fc-96a4-58677bc8fc46\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0ae1013f-767e-45fc-96a4-58677bc8fc46')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0ae1013f-767e-45fc-96a4-58677bc8fc46 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"0043.pdf\",\n          \"0059.pdf\",\n          \"0064.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Dynamic Relation Transformer for Contextual Text Block Detection\",\n          \"GraphMLLM A Graphbased Multilevel Layout Languageindependent Model for\",\n          \"Learning to Kern Setwise Estimation of Optimal Letter Space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Jiawei Wang13t, Shunchi Zhang2,3t, Kai Hul3t,, Chixiang Ma, Zhuoyao Zhong, Lei Sun, and Qiang Huo3, 1 University of Science and Technology of China\",\n          \"Document Understanding, HeSen Dai12, XiaoHui Lie, Fei Yin2, Xudong Yana, Shuqi Mei3, and, ChengLin Liu12\",\n          \"Kei Nakatsuru and Seiichi Uchida\\u00b0\\u00b0\\u00b0\\u00b0000185927566, Kyushu University, Fukuoka, Japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"dynamic relation transformer for contextual text block detection\",\n          \"graphmllm a graphbased multilevel layout languageindependent model for\",\n          \"learning to kern setwise estimation of optimal letter space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"jiawei wang13t, shunchi zhang2,3t, kai hul3t,, chixiang ma, zhuoyao zhong, lei sun, and qiang huo3, 1 university of science and technology of china\",\n          \"document understanding, hesen dai12, xiaohui lie, fei yin2, xudong yana, shuqi mei3, and, chenglin liu12\",\n          \"kei nakatsuru and seiichi uchida\\u00b0\\u00b0\\u00b0\\u00b0000185927566, kyushu university, fukuoka, japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#we want to remove unrelated numbers from all of our columns, not just those that are to be pre-processed\n",
        "def remove_numbers(text):\n",
        "    \"\"\"\n",
        "    This function removes all numbers from the input string.\n",
        "\n",
        "    :param text: The string from which to remove numbers\n",
        "    :return: A new string with all numbers removed\n",
        "    \"\"\"\n",
        "    return re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "for column in df.columns:\n",
        "    if column == 'id':\n",
        "        continue\n",
        "    print(f\"Removing numbers from {column}\")\n",
        "    df[column] = df[column].apply(remove_numbers)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "h4CDw8-3sdSS",
        "outputId": "a66c5692-7ed2-4822-cb93-ebda6724300d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing numbers from title\n",
            "Removing numbers from authors\n",
            "Removing numbers from title_preprocessed\n",
            "Removing numbers from authors_preprocessed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                              title  \\\n",
              "0  0049.pdf  Synthesizing Realistic Data for Table Recognition   \n",
              "1  0044.pdf              DLAFormer An EndtoEnd Transformer For   \n",
              "2  0086.pdf  Handwritten Document Recognition Using Pretrai...   \n",
              "3  0020.pdf  Improving Automatic Text Recognition with Lang...   \n",
              "4  0089.pdf  GlobalSEG Text Semantic Segmentation Based on ...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...   \n",
              "1  Document Layout Analysis, Jiawei Wang , ,,t, K...   \n",
              "2  Daniel Parresl   , Dan Aniteii   —  , and, Rob...   \n",
              "3                       Library, Solene Tarride , ‘,   \n",
              "4  Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...   \n",
              "\n",
              "                                  title_preprocessed  \\\n",
              "0  synthesizing realistic data for table recognition   \n",
              "1              dlaformer an endtoend transformer for   \n",
              "2  handwritten document recognition using pretrai...   \n",
              "3  improving automatic text recognition with lang...   \n",
              "4  globalseg text semantic segmentation based on ...   \n",
              "\n",
              "                                authors_preprocessed  \n",
              "0  qiyu houl  , jun wangl  x pa meixuan, qiao , a...  \n",
              "1  document layout analysis, jiawei wang , ,,t, k...  \n",
              "2  daniel parresl   , dan aniteii   —  , and, rob...  \n",
              "3                       library, solene tarride , ‘,  \n",
              "4  wenjun sunl   , hanh thi hong, tran , ,    car...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bde14b1-679f-49e1-b0ec-999c60b6079c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>title_preprocessed</th>\n",
              "      <th>authors_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0049.pdf</td>\n",
              "      <td>Synthesizing Realistic Data for Table Recognition</td>\n",
              "      <td>Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...</td>\n",
              "      <td>synthesizing realistic data for table recognition</td>\n",
              "      <td>qiyu houl  , jun wangl  x pa meixuan, qiao , a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0044.pdf</td>\n",
              "      <td>DLAFormer An EndtoEnd Transformer For</td>\n",
              "      <td>Document Layout Analysis, Jiawei Wang , ,,t, K...</td>\n",
              "      <td>dlaformer an endtoend transformer for</td>\n",
              "      <td>document layout analysis, jiawei wang , ,,t, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0086.pdf</td>\n",
              "      <td>Handwritten Document Recognition Using Pretrai...</td>\n",
              "      <td>Daniel Parresl   , Dan Aniteii   —  , and, Rob...</td>\n",
              "      <td>handwritten document recognition using pretrai...</td>\n",
              "      <td>daniel parresl   , dan aniteii   —  , and, rob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020.pdf</td>\n",
              "      <td>Improving Automatic Text Recognition with Lang...</td>\n",
              "      <td>Library, Solene Tarride , ‘,</td>\n",
              "      <td>improving automatic text recognition with lang...</td>\n",
              "      <td>library, solene tarride , ‘,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0089.pdf</td>\n",
              "      <td>GlobalSEG Text Semantic Segmentation Based on ...</td>\n",
              "      <td>Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...</td>\n",
              "      <td>globalseg text semantic segmentation based on ...</td>\n",
              "      <td>wenjun sunl   , hanh thi hong, tran , ,    car...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bde14b1-679f-49e1-b0ec-999c60b6079c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2bde14b1-679f-49e1-b0ec-999c60b6079c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2bde14b1-679f-49e1-b0ec-999c60b6079c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4e88b66c-53c8-4ec0-8aaa-8b0b7328da0b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e88b66c-53c8-4ec0-8aaa-8b0b7328da0b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4e88b66c-53c8-4ec0-8aaa-8b0b7328da0b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"0043.pdf\",\n          \"0059.pdf\",\n          \"0064.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Dynamic Relation Transformer for Contextual Text Block Detection\",\n          \"GraphMLLM A Graphbased Multilevel Layout Languageindependent Model for\",\n          \"Learning to Kern Setwise Estimation of Optimal Letter Space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Jiawei Wang t, Shunchi Zhang , t, Kai Hul t,, Chixiang Ma, Zhuoyao Zhong, Lei Sun, and Qiang Huo ,   University of Science and Technology of China\",\n          \"Document Understanding, HeSen Dai , XiaoHui Lie, Fei Yin , Xudong Yana, Shuqi Mei , and, ChengLin Liu \",\n          \"Kei Nakatsuru and Seiichi Uchida\\u00b0\\u00b0\\u00b0\\u00b0 , Kyushu University, Fukuoka, Japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"dynamic relation transformer for contextual text block detection\",\n          \"graphmllm a graphbased multilevel layout languageindependent model for\",\n          \"learning to kern setwise estimation of optimal letter space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"jiawei wang t, shunchi zhang , t, kai hul t,, chixiang ma, zhuoyao zhong, lei sun, and qiang huo ,   university of science and technology of china\",\n          \"document understanding, hesen dai , xiaohui lie, fei yin , xudong yana, shuqi mei , and, chenglin liu \",\n          \"kei nakatsuru and seiichi uchida\\u00b0\\u00b0\\u00b0\\u00b0 , kyushu university, fukuoka, japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove all special characters from the dataset\n",
        "df['title'] = df['title'].str.replace('[^a-zA-Z0-9\\s]', ' ')\n",
        "df['authors'] = df['authors'].str.replace('[^a-zA-Z0-9\\s]', ' ')\n",
        "df['authors_preprocessed'] = df['authors_preprocessed'].str.replace('[^a-zA-Z0-9\\s]', ' ')\n",
        "df['title_preprocessed'] = df['title_preprocessed'].str.replace('[^a-zA-Z0-9\\s]', ' ')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "6REP05HXswl9",
        "outputId": "bdcf5e02-069d-46ee-89cd-806f461eda53"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                              title  \\\n",
              "0  0049.pdf  Synthesizing Realistic Data for Table Recognition   \n",
              "1  0044.pdf              DLAFormer An EndtoEnd Transformer For   \n",
              "2  0086.pdf  Handwritten Document Recognition Using Pretrai...   \n",
              "3  0020.pdf  Improving Automatic Text Recognition with Lang...   \n",
              "4  0089.pdf  GlobalSEG Text Semantic Segmentation Based on ...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...   \n",
              "1  Document Layout Analysis, Jiawei Wang , ,,t, K...   \n",
              "2  Daniel Parresl   , Dan Aniteii   —  , and, Rob...   \n",
              "3                       Library, Solene Tarride , ‘,   \n",
              "4  Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...   \n",
              "\n",
              "                                  title_preprocessed  \\\n",
              "0  synthesizing realistic data for table recognition   \n",
              "1              dlaformer an endtoend transformer for   \n",
              "2  handwritten document recognition using pretrai...   \n",
              "3  improving automatic text recognition with lang...   \n",
              "4  globalseg text semantic segmentation based on ...   \n",
              "\n",
              "                                authors_preprocessed  \n",
              "0  qiyu houl  , jun wangl  x pa meixuan, qiao , a...  \n",
              "1  document layout analysis, jiawei wang , ,,t, k...  \n",
              "2  daniel parresl   , dan aniteii   —  , and, rob...  \n",
              "3                       library, solene tarride , ‘,  \n",
              "4  wenjun sunl   , hanh thi hong, tran , ,    car...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-891db28a-99e6-42f1-975f-5b9b064412fd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>title_preprocessed</th>\n",
              "      <th>authors_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0049.pdf</td>\n",
              "      <td>Synthesizing Realistic Data for Table Recognition</td>\n",
              "      <td>Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...</td>\n",
              "      <td>synthesizing realistic data for table recognition</td>\n",
              "      <td>qiyu houl  , jun wangl  x pa meixuan, qiao , a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0044.pdf</td>\n",
              "      <td>DLAFormer An EndtoEnd Transformer For</td>\n",
              "      <td>Document Layout Analysis, Jiawei Wang , ,,t, K...</td>\n",
              "      <td>dlaformer an endtoend transformer for</td>\n",
              "      <td>document layout analysis, jiawei wang , ,,t, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0086.pdf</td>\n",
              "      <td>Handwritten Document Recognition Using Pretrai...</td>\n",
              "      <td>Daniel Parresl   , Dan Aniteii   —  , and, Rob...</td>\n",
              "      <td>handwritten document recognition using pretrai...</td>\n",
              "      <td>daniel parresl   , dan aniteii   —  , and, rob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020.pdf</td>\n",
              "      <td>Improving Automatic Text Recognition with Lang...</td>\n",
              "      <td>Library, Solene Tarride , ‘,</td>\n",
              "      <td>improving automatic text recognition with lang...</td>\n",
              "      <td>library, solene tarride , ‘,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0089.pdf</td>\n",
              "      <td>GlobalSEG Text Semantic Segmentation Based on ...</td>\n",
              "      <td>Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...</td>\n",
              "      <td>globalseg text semantic segmentation based on ...</td>\n",
              "      <td>wenjun sunl   , hanh thi hong, tran , ,    car...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-891db28a-99e6-42f1-975f-5b9b064412fd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-891db28a-99e6-42f1-975f-5b9b064412fd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-891db28a-99e6-42f1-975f-5b9b064412fd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0866fd4c-f022-46c6-878e-5928ba777ef6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0866fd4c-f022-46c6-878e-5928ba777ef6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0866fd4c-f022-46c6-878e-5928ba777ef6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"0043.pdf\",\n          \"0059.pdf\",\n          \"0064.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Dynamic Relation Transformer for Contextual Text Block Detection\",\n          \"GraphMLLM A Graphbased Multilevel Layout Languageindependent Model for\",\n          \"Learning to Kern Setwise Estimation of Optimal Letter Space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Jiawei Wang t, Shunchi Zhang , t, Kai Hul t,, Chixiang Ma, Zhuoyao Zhong, Lei Sun, and Qiang Huo ,   University of Science and Technology of China\",\n          \"Document Understanding, HeSen Dai , XiaoHui Lie, Fei Yin , Xudong Yana, Shuqi Mei , and, ChengLin Liu \",\n          \"Kei Nakatsuru and Seiichi Uchida\\u00b0\\u00b0\\u00b0\\u00b0 , Kyushu University, Fukuoka, Japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"dynamic relation transformer for contextual text block detection\",\n          \"graphmllm a graphbased multilevel layout languageindependent model for\",\n          \"learning to kern setwise estimation of optimal letter space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"jiawei wang t, shunchi zhang , t, kai hul t,, chixiang ma, zhuoyao zhong, lei sun, and qiang huo ,   university of science and technology of china\",\n          \"document understanding, hesen dai , xiaohui lie, fei yin , xudong yana, shuqi mei , and, chenglin liu \",\n          \"kei nakatsuru and seiichi uchida\\u00b0\\u00b0\\u00b0\\u00b0 , kyushu university, fukuoka, japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#view all stopwords that we need to remove\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "try:\n",
        "  nltk.download('stopwords')\n",
        "except:\n",
        "  print(\"error in downloading stopwords\")\n",
        "\n",
        "\", \".join(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "gK0qPleltRL9",
        "outputId": "ce1f9a44-1ff0-41e3-b3b9-55ac163a1161"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use a function to remove all stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "df['title_preprocessed'] = df[\"title_preprocessed\"].apply(lambda text: remove_stopwords(text))\n",
        "df['authors_preprocessed']=df['authors_preprocessed'].apply(lambda text: remove_stopwords(text))\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "x_uFKzSZtV2X",
        "outputId": "dd9b531d-2260-46b4-aa07-3cb4340152c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                              title  \\\n",
              "0  0049.pdf  Synthesizing Realistic Data for Table Recognition   \n",
              "1  0044.pdf              DLAFormer An EndtoEnd Transformer For   \n",
              "2  0086.pdf  Handwritten Document Recognition Using Pretrai...   \n",
              "3  0020.pdf  Improving Automatic Text Recognition with Lang...   \n",
              "4  0089.pdf  GlobalSEG Text Semantic Segmentation Based on ...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...   \n",
              "1  Document Layout Analysis, Jiawei Wang , ,,t, K...   \n",
              "2  Daniel Parresl   , Dan Aniteii   —  , and, Rob...   \n",
              "3                       Library, Solene Tarride , ‘,   \n",
              "4  Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...   \n",
              "\n",
              "                                  title_preprocessed  \\\n",
              "0      synthesizing realistic data table recognition   \n",
              "1                     dlaformer endtoend transformer   \n",
              "2  handwritten document recognition using pretrai...   \n",
              "3  improving automatic text recognition language ...   \n",
              "4  globalseg text semantic segmentation based glo...   \n",
              "\n",
              "                                authors_preprocessed  \n",
              "0  qiyu houl , jun wangl x pa meixuan, qiao , luj...  \n",
              "1  document layout analysis, jiawei wang , ,,t, k...  \n",
              "2  daniel parresl , dan aniteii — , and, roberto ...  \n",
              "3                       library, solene tarride , ‘,  \n",
              "4  wenjun sunl , hanh thi hong, tran , , carlosem...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c20c1d8-a040-48ab-b7f2-91423b4a7a94\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>title_preprocessed</th>\n",
              "      <th>authors_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0049.pdf</td>\n",
              "      <td>Synthesizing Realistic Data for Table Recognition</td>\n",
              "      <td>Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...</td>\n",
              "      <td>synthesizing realistic data table recognition</td>\n",
              "      <td>qiyu houl , jun wangl x pa meixuan, qiao , luj...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0044.pdf</td>\n",
              "      <td>DLAFormer An EndtoEnd Transformer For</td>\n",
              "      <td>Document Layout Analysis, Jiawei Wang , ,,t, K...</td>\n",
              "      <td>dlaformer endtoend transformer</td>\n",
              "      <td>document layout analysis, jiawei wang , ,,t, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0086.pdf</td>\n",
              "      <td>Handwritten Document Recognition Using Pretrai...</td>\n",
              "      <td>Daniel Parresl   , Dan Aniteii   —  , and, Rob...</td>\n",
              "      <td>handwritten document recognition using pretrai...</td>\n",
              "      <td>daniel parresl , dan aniteii — , and, roberto ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020.pdf</td>\n",
              "      <td>Improving Automatic Text Recognition with Lang...</td>\n",
              "      <td>Library, Solene Tarride , ‘,</td>\n",
              "      <td>improving automatic text recognition language ...</td>\n",
              "      <td>library, solene tarride , ‘,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0089.pdf</td>\n",
              "      <td>GlobalSEG Text Semantic Segmentation Based on ...</td>\n",
              "      <td>Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...</td>\n",
              "      <td>globalseg text semantic segmentation based glo...</td>\n",
              "      <td>wenjun sunl , hanh thi hong, tran , , carlosem...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c20c1d8-a040-48ab-b7f2-91423b4a7a94')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1c20c1d8-a040-48ab-b7f2-91423b4a7a94 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1c20c1d8-a040-48ab-b7f2-91423b4a7a94');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-273f1d9d-3c75-443f-969e-4299279062ac\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-273f1d9d-3c75-443f-969e-4299279062ac')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-273f1d9d-3c75-443f-969e-4299279062ac button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"0043.pdf\",\n          \"0059.pdf\",\n          \"0064.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Dynamic Relation Transformer for Contextual Text Block Detection\",\n          \"GraphMLLM A Graphbased Multilevel Layout Languageindependent Model for\",\n          \"Learning to Kern Setwise Estimation of Optimal Letter Space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Jiawei Wang t, Shunchi Zhang , t, Kai Hul t,, Chixiang Ma, Zhuoyao Zhong, Lei Sun, and Qiang Huo ,   University of Science and Technology of China\",\n          \"Document Understanding, HeSen Dai , XiaoHui Lie, Fei Yin , Xudong Yana, Shuqi Mei , and, ChengLin Liu \",\n          \"Kei Nakatsuru and Seiichi Uchida\\u00b0\\u00b0\\u00b0\\u00b0 , Kyushu University, Fukuoka, Japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"dynamic relation transformer contextual text block detection\",\n          \"graphmllm graphbased multilevel layout languageindependent model\",\n          \"learning kern setwise estimation optimal letter space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"jiawei wang t, shunchi zhang , t, kai hul t,, chixiang ma, zhuoyao zhong, lei sun, qiang huo , university science technology china\",\n          \"document understanding, hesen dai , xiaohui lie, fei yin , xudong yana, shuqi mei , and, chenglin liu\",\n          \"kei nakatsuru seiichi uchida\\u00b0\\u00b0\\u00b0\\u00b0 , kyushu university, fukuoka, japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove all urls\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "df[\"title\"] = df[\"title\"].apply(lambda text: remove_urls(text))\n",
        "df['authors']=df['authors'].apply(lambda text: remove_urls(text))\n",
        "df['title_preprocessed'] = df[\"title_preprocessed\"].apply(lambda text: remove_urls(text))\n",
        "df['authors_preprocessed']=df['authors_preprocessed'].apply(lambda text: remove_urls(text))\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "xxZtufdNt5KW",
        "outputId": "6c83fd03-d1f2-4724-f341-a38f98bf9ae7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                              title  \\\n",
              "0  0049.pdf  Synthesizing Realistic Data for Table Recognition   \n",
              "1  0044.pdf              DLAFormer An EndtoEnd Transformer For   \n",
              "2  0086.pdf  Handwritten Document Recognition Using Pretrai...   \n",
              "3  0020.pdf  Improving Automatic Text Recognition with Lang...   \n",
              "4  0089.pdf  GlobalSEG Text Semantic Segmentation Based on ...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...   \n",
              "1  Document Layout Analysis, Jiawei Wang , ,,t, K...   \n",
              "2  Daniel Parresl   , Dan Aniteii   —  , and, Rob...   \n",
              "3                       Library, Solene Tarride , ‘,   \n",
              "4  Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...   \n",
              "\n",
              "                                  title_preprocessed  \\\n",
              "0      synthesizing realistic data table recognition   \n",
              "1                     dlaformer endtoend transformer   \n",
              "2  handwritten document recognition using pretrai...   \n",
              "3  improving automatic text recognition language ...   \n",
              "4  globalseg text semantic segmentation based glo...   \n",
              "\n",
              "                                authors_preprocessed  \n",
              "0  qiyu houl , jun wangl x pa meixuan, qiao , luj...  \n",
              "1  document layout analysis, jiawei wang , ,,t, k...  \n",
              "2  daniel parresl , dan aniteii — , and, roberto ...  \n",
              "3                       library, solene tarride , ‘,  \n",
              "4  wenjun sunl , hanh thi hong, tran , , carlosem...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aee3bc98-5b53-4f92-83ae-c56d60e434d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>title_preprocessed</th>\n",
              "      <th>authors_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0049.pdf</td>\n",
              "      <td>Synthesizing Realistic Data for Table Recognition</td>\n",
              "      <td>Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...</td>\n",
              "      <td>synthesizing realistic data table recognition</td>\n",
              "      <td>qiyu houl , jun wangl x pa meixuan, qiao , luj...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0044.pdf</td>\n",
              "      <td>DLAFormer An EndtoEnd Transformer For</td>\n",
              "      <td>Document Layout Analysis, Jiawei Wang , ,,t, K...</td>\n",
              "      <td>dlaformer endtoend transformer</td>\n",
              "      <td>document layout analysis, jiawei wang , ,,t, k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0086.pdf</td>\n",
              "      <td>Handwritten Document Recognition Using Pretrai...</td>\n",
              "      <td>Daniel Parresl   , Dan Aniteii   —  , and, Rob...</td>\n",
              "      <td>handwritten document recognition using pretrai...</td>\n",
              "      <td>daniel parresl , dan aniteii — , and, roberto ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020.pdf</td>\n",
              "      <td>Improving Automatic Text Recognition with Lang...</td>\n",
              "      <td>Library, Solene Tarride , ‘,</td>\n",
              "      <td>improving automatic text recognition language ...</td>\n",
              "      <td>library, solene tarride , ‘,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0089.pdf</td>\n",
              "      <td>GlobalSEG Text Semantic Segmentation Based on ...</td>\n",
              "      <td>Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...</td>\n",
              "      <td>globalseg text semantic segmentation based glo...</td>\n",
              "      <td>wenjun sunl , hanh thi hong, tran , , carlosem...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aee3bc98-5b53-4f92-83ae-c56d60e434d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aee3bc98-5b53-4f92-83ae-c56d60e434d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aee3bc98-5b53-4f92-83ae-c56d60e434d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-92de4b43-2c02-47df-8899-d4c420da00f2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-92de4b43-2c02-47df-8899-d4c420da00f2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-92de4b43-2c02-47df-8899-d4c420da00f2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"0043.pdf\",\n          \"0059.pdf\",\n          \"0064.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Dynamic Relation Transformer for Contextual Text Block Detection\",\n          \"GraphMLLM A Graphbased Multilevel Layout Languageindependent Model for\",\n          \"Learning to Kern Setwise Estimation of Optimal Letter Space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Jiawei Wang t, Shunchi Zhang , t, Kai Hul t,, Chixiang Ma, Zhuoyao Zhong, Lei Sun, and Qiang Huo ,   University of Science and Technology of China\",\n          \"Document Understanding, HeSen Dai , XiaoHui Lie, Fei Yin , Xudong Yana, Shuqi Mei , and, ChengLin Liu \",\n          \"Kei Nakatsuru and Seiichi Uchida\\u00b0\\u00b0\\u00b0\\u00b0 , Kyushu University, Fukuoka, Japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"dynamic relation transformer contextual text block detection\",\n          \"graphmllm graphbased multilevel layout languageindependent model\",\n          \"learning kern setwise estimation optimal letter space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"jiawei wang t, shunchi zhang , t, kai hul t,, chixiang ma, zhuoyao zhong, lei sun, qiang huo , university science technology china\",\n          \"document understanding, hesen dai , xiaohui lie, fei yin , xudong yana, shuqi mei , and, chenglin liu\",\n          \"kei nakatsuru seiichi uchida\\u00b0\\u00b0\\u00b0\\u00b0 , kyushu university, fukuoka, japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwMNkXASyqu-",
        "outputId": "6893c8b8-3162-4bed-a6eb-377ba60758db"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set labels that papers are to be classified into\n",
        "classes = ['Tables',\n",
        "'Classification',\n",
        "'Key Information Extraction',\n",
        "'Optical Character Recognition',\n",
        "'Datasets',\n",
        "'Document Layout Understanding',\n",
        "'Others',\n",
        "'Deep Learning Models',\n",
        "'Multimodal Document Analysis',\n",
        "'Handwriting Recognition']"
      ],
      "metadata": {
        "id": "bAUVqs6SQMt4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koQ7ARqXisBe",
        "outputId": "1db33c10-e15c-486a-96ce-667b347934a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Zero shot classification Implementation\n",
        "Since we are dealing with an unlabeled dataset, we will be using a zero-shot classification approach, meaning that we will be attempting to use an already trained model on our own data to make predictions, in order to avoid training it from scratch. More specifically we will be using a pre-trained model from HuggingFace used for multi-class classification and apply it on our own features and labels. Firstly we test it out on a random record, and then we will proceed by applying the process on the whole dataset  "
      ],
      "metadata": {
        "id": "EEvYdlxp46Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQWK5NJ1ytk8",
        "outputId": "ee967966-99b4-4e50-dd0a-3149ffbde8a0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = df['title_preprocessed'][0] + \" \" + df['authors_preprocessed'][0]\n",
        "print(sequence)\n",
        "\n",
        "zero_shot_classifier(sequence, classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-BOanUt3kTO",
        "outputId": "2a4d85f7-ceb8-49ad-a7d5-74dad8f7280e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "synthesizing realistic data table recognition qiyu houl , jun wangl x pa meixuan, qiao , lujun tianl, iwudao tech\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'synthesizing realistic data table recognition qiyu houl , jun wangl x pa meixuan, qiao , lujun tianl, iwudao tech',\n",
              " 'labels': ['Tables',\n",
              "  'Datasets',\n",
              "  'Others',\n",
              "  'Key Information Extraction',\n",
              "  'Multimodal Document Analysis',\n",
              "  'Deep Learning Models',\n",
              "  'Document Layout Understanding',\n",
              "  'Classification',\n",
              "  'Optical Character Recognition',\n",
              "  'Handwriting Recognition'],\n",
              " 'scores': [0.575657308101654,\n",
              "  0.14436371624469757,\n",
              "  0.08407388627529144,\n",
              "  0.04487946256995201,\n",
              "  0.040758635848760605,\n",
              "  0.03771326318383217,\n",
              "  0.03026938997209072,\n",
              "  0.015615738928318024,\n",
              "  0.01465910580009222,\n",
              "  0.012009557336568832]}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = None\n",
        "for i in range(len(df)):\n",
        "  sequence = df['title_preprocessed'][i] + \" \" + df['authors_preprocessed'][i]\n",
        "  prediction = zero_shot_classifier(sequence, classes)\n",
        "  print(prediction)\n",
        "  #find the position of the max score\n",
        "  max_score_index = np.argmax(prediction['scores'])\n",
        "  #add it as the most likely label column to the dataset\n",
        "  df.loc[i, \"label\"] = prediction['labels'][max_score_index]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiEg07li7PDx",
        "outputId": "bbdfd370-0478-4b4c-e0c9-3f8a927b38af"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sequence': 'synthesizing realistic data table recognition qiyu houl , jun wangl x pa meixuan, qiao , lujun tianl, iwudao tech', 'labels': ['Tables', 'Datasets', 'Others', 'Key Information Extraction', 'Multimodal Document Analysis', 'Deep Learning Models', 'Document Layout Understanding', 'Classification', 'Optical Character Recognition', 'Handwriting Recognition'], 'scores': [0.575657308101654, 0.14436371624469757, 0.08407388627529144, 0.04487946256995201, 0.040758635848760605, 0.03771326318383217, 0.03026938997209072, 0.015615738928318024, 0.01465910580009222, 0.012009557336568832]}\n",
            "{'sequence': 'dlaformer endtoend transformer document layout analysis, jiawei wang , ,,t, kai hui , ,, , qiang huo , department eeis, university science technology china, hefei, china', 'labels': ['Deep Learning Models', 'Others', 'Multimodal Document Analysis', 'Datasets', 'Key Information Extraction', 'Classification', 'Document Layout Understanding', 'Tables', 'Optical Character Recognition', 'Handwriting Recognition'], 'scores': [0.32983866333961487, 0.178084135055542, 0.0993332788348198, 0.08711857348680496, 0.08039014786481857, 0.06653694808483124, 0.06329872459173203, 0.03851189464330673, 0.031451351940631866, 0.025436220690608025]}\n",
            "{'sequence': 'handwritten document recognition using pretrained vision transformers daniel parresl , dan aniteii — , and, roberto paredes , , prhlt research center, universitat politecnica valencia, valencia, spain', 'labels': ['Handwriting Recognition', 'Others', 'Multimodal Document Analysis', 'Optical Character Recognition', 'Key Information Extraction', 'Document Layout Understanding', 'Classification', 'Datasets', 'Deep Learning Models', 'Tables'], 'scores': [0.8771543502807617, 0.047531310468912125, 0.015431303530931473, 0.015369437634944916, 0.014388728886842728, 0.009661809541285038, 0.007304013706743717, 0.00574851781129837, 0.0056222788989543915, 0.0017882136162370443]}\n",
            "{'sequence': 'improving automatic text recognition language models pylaia opensource library, solene tarride , ‘,', 'labels': ['Others', 'Datasets', 'Key Information Extraction', 'Multimodal Document Analysis', 'Tables', 'Deep Learning Models', 'Classification', 'Document Layout Understanding', 'Handwriting Recognition', 'Optical Character Recognition'], 'scores': [0.3527064919471741, 0.13012026250362396, 0.1099511906504631, 0.09860293567180634, 0.08805306255817413, 0.06979028880596161, 0.0612415187060833, 0.05160218104720116, 0.02083413116633892, 0.01709802821278572]}\n",
            "{'sequence': 'globalseg text semantic segmentation based global semantic pair relations wenjun sunl , hanh thi hong, tran , , carlosemiliano, gonzalezgallardol , mithael', 'labels': ['Others', 'Classification', 'Datasets', 'Multimodal Document Analysis', 'Key Information Extraction', 'Document Layout Understanding', 'Deep Learning Models', 'Handwriting Recognition', 'Optical Character Recognition', 'Tables'], 'scores': [0.31792759895324707, 0.20791174471378326, 0.09273082762956619, 0.09034401923418045, 0.07894261181354523, 0.07097575813531876, 0.0675789937376976, 0.029566893354058266, 0.02253848686814308, 0.02148302085697651]}\n",
            "{'sequence': 'impressionclip contrastive shapeimpression embedding fonts yugo kubota, daichi haraguchi , and, seiichi uchida°°°° , kyushu university, fukuoka, japan yugo kubotahuman ait kyushuu ac j p', 'labels': ['Others', 'Key Information Extraction', 'Multimodal Document Analysis', 'Datasets', 'Classification', 'Handwriting Recognition', 'Document Layout Understanding', 'Optical Character Recognition', 'Tables', 'Deep Learning Models'], 'scores': [0.5102348923683167, 0.08920899033546448, 0.07412324845790863, 0.06439431756734848, 0.05721781775355339, 0.0550578236579895, 0.05405968427658081, 0.05211467295885086, 0.029820678755640984, 0.01376783475279808]}\n",
            "{'sequence': 'towards endtoend semisupervised table detection semantic aligned matching transformer tahira shehzadi , , x shalini sarode , ,, didier stricker , muhammad zeshan afza , , , department computer science, technical university kaiserslautern, , germany', 'labels': ['Tables', 'Others', 'Datasets', 'Multimodal Document Analysis', 'Key Information Extraction', 'Deep Learning Models', 'Document Layout Understanding', 'Classification', 'Optical Character Recognition', 'Handwriting Recognition'], 'scores': [0.5421335101127625, 0.11801902949810028, 0.06390668451786041, 0.0625414028763771, 0.05390722677111626, 0.052241504192352295, 0.04412968456745148, 0.032403912395238876, 0.01685510016977787, 0.013861963525414467]}\n",
            "{'sequence': 'univie unified label space approach visual information extraction formlike documents, kai hul , jiawei wang , ,, weihong line,,, zhuoyao zhong , lei sung, qiang huo', 'labels': ['Document Layout Understanding', 'Others', 'Multimodal Document Analysis', 'Key Information Extraction', 'Classification', 'Handwriting Recognition', 'Optical Character Recognition', 'Datasets', 'Deep Learning Models', 'Tables'], 'scores': [0.16094867885112762, 0.1571522057056427, 0.13157771527767181, 0.12013424187898636, 0.11852938681840897, 0.1123952716588974, 0.10733503103256226, 0.0457349456846714, 0.03137151524424553, 0.014821011573076248]}\n",
            "{'sequence': 'multicell decoder mutual learning table structure character recognition takaya kawakatsui , preferred networks, inc, otemachi, chiyoda, tokyo, japan, katniiacjpgmailcom', 'labels': ['Tables', 'Others', 'Datasets', 'Multimodal Document Analysis', 'Classification', 'Key Information Extraction', 'Document Layout Understanding', 'Deep Learning Models', 'Optical Character Recognition', 'Handwriting Recognition'], 'scores': [0.2676922380924225, 0.2506925165653229, 0.11120270192623138, 0.094741590321064, 0.08867208659648895, 0.06579210609197617, 0.04872889071702957, 0.04056943580508232, 0.016597827896475792, 0.01531055849045515]}\n",
            "{'sequence': 'sourcefree domain adaptation optical music recognition adrian rose , eliseo fuentesmartinezi , maria alfarocontreras , david, rizo , jorge calvozaragozal, pattern recognition artificial intelligence group, university alicante, spain', 'labels': ['Others', 'Optical Character Recognition', 'Key Information Extraction', 'Classification', 'Datasets', 'Multimodal Document Analysis', 'Deep Learning Models', 'Document Layout Understanding', 'Tables', 'Handwriting Recognition'], 'scores': [0.3436508774757385, 0.34176960587501526, 0.07426873594522476, 0.05321105942130089, 0.05190078914165497, 0.04547266662120819, 0.040472567081451416, 0.024179747328162193, 0.014525307342410088, 0.01054868008941412]}\n",
            "{'sequence': 'lmtextspotter towards better scene text spotting language modeling transformer xin xia , guodong ding , siyuan lie, college computer science technology, zhejiang university, hithink royalflush information network co ltd, china', 'labels': ['Others', 'Datasets', 'Multimodal Document Analysis', 'Key Information Extraction', 'Classification', 'Document Layout Understanding', 'Deep Learning Models', 'Optical Character Recognition', 'Handwriting Recognition', 'Tables'], 'scores': [0.4548487067222595, 0.1027003601193428, 0.09206150472164154, 0.0893329530954361, 0.07162676751613617, 0.0636633113026619, 0.03817307949066162, 0.03099440224468708, 0.02856355533003807, 0.028035417199134827]}\n",
            "{'sequence': 'binarizing documents leveraging space frequency fabio quattrini , vittorio pippip l , silvia cascianelli , rita cucchiara x, university modena reggio emilia, modena, italy', 'labels': ['Multimodal Document Analysis', 'Key Information Extraction', 'Others', 'Document Layout Understanding', 'Optical Character Recognition', 'Classification', 'Datasets', 'Handwriting Recognition', 'Deep Learning Models', 'Tables'], 'scores': [0.27607619762420654, 0.1899888664484024, 0.16405801475048065, 0.11283141374588013, 0.06970126181840897, 0.06311341375112534, 0.04567664861679077, 0.04202313348650932, 0.020369408652186394, 0.016161605715751648]}\n",
            "{'sequence': ' hybrid approach document layout, analysis document images, tahira shehzadi , , x didier stricker , ,', 'labels': ['Multimodal Document Analysis', 'Document Layout Understanding', 'Others', 'Key Information Extraction', 'Optical Character Recognition', 'Classification', 'Datasets', 'Handwriting Recognition', 'Tables', 'Deep Learning Models'], 'scores': [0.5030775666236877, 0.16614507138729095, 0.12784349918365479, 0.06385493278503418, 0.03481074050068855, 0.026185160502791405, 0.02539035491645336, 0.021411016583442688, 0.01599380560219288, 0.015287835150957108]}\n",
            "{'sequence': 'dynamic relation transformer contextual text block detection jiawei wang t, shunchi zhang , t, kai hul t,, chixiang ma, zhuoyao zhong, lei sun, qiang huo , university science technology china', 'labels': ['Others', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Key Information Extraction', 'Classification', 'Datasets', 'Deep Learning Models', 'Handwriting Recognition', 'Optical Character Recognition', 'Tables'], 'scores': [0.2600921094417572, 0.16769225895404816, 0.1284991204738617, 0.12525196373462677, 0.08255777508020401, 0.07491236180067062, 0.06254208087921143, 0.058532532304525375, 0.020991221070289612, 0.018928635865449905]}\n",
            "{'sequence': 'multimodal adaptive inference document image classification anytime early exiting omar hamedl, souhail bakkali , matthew, blaschko — x, sien moens', 'labels': ['Classification', 'Multimodal Document Analysis', 'Others', 'Datasets', 'Key Information Extraction', 'Optical Character Recognition', 'Deep Learning Models', 'Document Layout Understanding', 'Tables', 'Handwriting Recognition'], 'scores': [0.3745180368423462, 0.3518628776073456, 0.1711885631084442, 0.022014642134308815, 0.020445281639695168, 0.019738443195819855, 0.017379986122250557, 0.012943316251039505, 0.0049589150585234165, 0.0049499403685331345]}\n",
            "{'sequence': 'moose multiorientation sharing experts openset scene text recognition chang liu, simon corbille, elisa h barney smith, machine learning group, lulea university technology, sweden, changliultuse,', 'labels': ['Others', 'Multimodal Document Analysis', 'Key Information Extraction', 'Datasets', 'Classification', 'Document Layout Understanding', 'Deep Learning Models', 'Tables', 'Handwriting Recognition', 'Optical Character Recognition'], 'scores': [0.6968802809715271, 0.09803933650255203, 0.06243021413683891, 0.04538070783019066, 0.02636181004345417, 0.023811420425772667, 0.018449123948812485, 0.01156618818640709, 0.008651050738990307, 0.008429757319390774]}\n",
            "{'sequence': 'less enhancing abundance refining redundancy textpriorguided scene text image superresolution, wei yang , , yihong luo g, mayire, ,r askar hamdulla ,', 'labels': ['Others', 'Key Information Extraction', 'Document Layout Understanding', 'Multimodal Document Analysis', 'Classification', 'Optical Character Recognition', 'Handwriting Recognition', 'Datasets', 'Tables', 'Deep Learning Models'], 'scores': [0.37632718682289124, 0.12569603323936462, 0.0879523903131485, 0.08741569519042969, 0.06730960309505463, 0.062441568821668625, 0.060648005455732346, 0.06017988175153732, 0.04102575033903122, 0.031003892421722412]}\n",
            "{'sequence': 'end end table transformer yun young choi x, taehoon kim namwook kim °° taehee lee°°° , and, seongho joe°°°°', 'labels': ['Others', 'Tables', 'Key Information Extraction', 'Datasets', 'Classification', 'Multimodal Document Analysis', 'Handwriting Recognition', 'Document Layout Understanding', 'Optical Character Recognition', 'Deep Learning Models'], 'scores': [0.6211339831352234, 0.12968403100967407, 0.04735106974840164, 0.04351125285029411, 0.03527258709073067, 0.03282083943486214, 0.03167223557829857, 0.026091625913977623, 0.02355070784687996, 0.00891164131462574]}\n",
            "{'sequence': 'deep learningdriven innovative model generating functional knowledge units pan qiangangl, hu yahongl, xie youbai , meng xianghui , zhang, yilun , zhejiang university technology, hangzhou, zhejiang, china', 'labels': ['Deep Learning Models', 'Others', 'Key Information Extraction', 'Document Layout Understanding', 'Datasets', 'Classification', 'Multimodal Document Analysis', 'Tables', 'Handwriting Recognition', 'Optical Character Recognition'], 'scores': [0.8600744605064392, 0.04369109123945236, 0.016496403142809868, 0.015180852264165878, 0.012792473658919334, 0.012125611305236816, 0.012018674984574318, 0.010903939604759216, 0.010252791456878185, 0.006463755387812853]}\n",
            "{'sequence': 'privacyaware document visual question answering ruben tito, khanh nguyen, marlon tobaben , raouf kerkouche ,, mohamed ali souibguil, kangsoo june, joonas ja , vincent poulain, dandecy , aurelie josephs, lei kang , ernest valvenyl, antti honkela ,', 'labels': ['Others', 'Key Information Extraction', 'Multimodal Document Analysis', 'Datasets', 'Classification', 'Document Layout Understanding', 'Optical Character Recognition', 'Handwriting Recognition', 'Tables', 'Deep Learning Models'], 'scores': [0.7073615789413452, 0.08168547600507736, 0.062167294323444366, 0.04029863327741623, 0.024331610649824142, 0.022403903305530548, 0.020671142265200615, 0.020315542817115784, 0.012449325993657112, 0.008315496146678925]}\n",
            "{'sequence': 'regionbased approach layout analysis music score images scarce data scenarios francisco j castellanos juan p, martinezesteso alejandro, galancuenca°°°° , antonio javier', 'labels': ['Others', 'Classification', 'Datasets', 'Key Information Extraction', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Handwriting Recognition', 'Optical Character Recognition', 'Tables', 'Deep Learning Models'], 'scores': [0.21018807590007782, 0.19327451288700104, 0.14414331316947937, 0.12381190806627274, 0.11248540133237839, 0.09506634622812271, 0.03780139982700348, 0.0372791662812233, 0.026798224076628685, 0.019151709973812103]}\n",
            "{'sequence': 'multitask learning license plate recognition unconstrained scenarios zhenlun mot, songlu chenl, qi liu , feng chen , xucheng yinl, university science technology beijing, beijing, china, songluchen,xuchengyinoustbeducn', 'labels': ['Others', 'Datasets', 'Key Information Extraction', 'Multimodal Document Analysis', 'Classification', 'Deep Learning Models', 'Tables', 'Document Layout Understanding', 'Optical Character Recognition', 'Handwriting Recognition'], 'scores': [0.38538023829460144, 0.12631896138191223, 0.12401621043682098, 0.08073501288890839, 0.08021147549152374, 0.05949445441365242, 0.0555865615606308, 0.039508674293756485, 0.03604687750339508, 0.012701524421572685]}\n",
            "{'sequence': 'realtime scene uyghur text detection network based feature complementation mayire ibrayim , , mengmeng chen , ,, askar hamdulla , x, jianjun kang , , chunhu, zhang ,', 'labels': ['Others', 'Multimodal Document Analysis', 'Datasets', 'Classification', 'Key Information Extraction', 'Handwriting Recognition', 'Deep Learning Models', 'Document Layout Understanding', 'Tables', 'Optical Character Recognition'], 'scores': [0.4000587463378906, 0.1360354870557785, 0.1289992481470108, 0.08922085165977478, 0.06926313787698746, 0.047244004905223846, 0.0459740050137043, 0.03876996040344238, 0.02947871945798397, 0.014955897815525532]}\n",
            "{'sequence': 'saghog selfsupervised autoencoder generating hog features writer retrieval marco peero, florian klebere, robert sablatnigwo, computer vision lab, tu wien, mpeer, kleber,', 'labels': ['Others', 'Key Information Extraction', 'Datasets', 'Multimodal Document Analysis', 'Classification', 'Deep Learning Models', 'Document Layout Understanding', 'Tables', 'Handwriting Recognition', 'Optical Character Recognition'], 'scores': [0.4481375813484192, 0.10043522715568542, 0.09804897010326385, 0.06916197389364243, 0.06402482837438583, 0.054629869759082794, 0.04633048549294472, 0.04497314617037773, 0.04137219488620758, 0.032885659486055374]}\n",
            "{'sequence': 'conclue conditional clue extraction multiple choice question answering wangli yang , jie yangl, wanqing li, yi guo , school computing information technology,, university wollongong, australia', 'labels': ['Key Information Extraction', 'Others', 'Datasets', 'Document Layout Understanding', 'Multimodal Document Analysis', 'Classification', 'Handwriting Recognition', 'Optical Character Recognition', 'Deep Learning Models', 'Tables'], 'scores': [0.21020589768886566, 0.19797082245349884, 0.1170169785618782, 0.10916396230459213, 0.0877012237906456, 0.06230616942048073, 0.05857314169406891, 0.05620132386684418, 0.05490153282880783, 0.045958954840898514]}\n",
            "{'sequence': 'progressive evolution singlepoint polygon scene text linger denglt, mingxin huang t, xudong xie , yuliang liul, lianwen jin ,, xiang bail, huazhong university science technology, wuhan, china', 'labels': ['Others', 'Key Information Extraction', 'Document Layout Understanding', 'Classification', 'Datasets', 'Multimodal Document Analysis', 'Optical Character Recognition', 'Handwriting Recognition', 'Tables', 'Deep Learning Models'], 'scores': [0.2372121810913086, 0.20112426578998566, 0.11355224251747131, 0.09859336167573929, 0.08729522675275803, 0.07355261594057083, 0.06742854416370392, 0.06474379450082779, 0.03162814676761627, 0.024869656190276146]}\n",
            "{'sequence': 'visual prompt learning chinese handwriting recognition gang yao , , ning ding , tianqi, zhao , , kemeng zhao , , pei, tang , — yao ta liangrui', 'labels': ['Handwriting Recognition', 'Others', 'Key Information Extraction', 'Datasets', 'Classification', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Deep Learning Models', 'Optical Character Recognition', 'Tables'], 'scores': [0.9262233376502991, 0.02698413096368313, 0.011595900170505047, 0.010399304330348969, 0.007180868647992611, 0.00627138139680028, 0.0036208222154527903, 0.003214783500880003, 0.0025196389760822058, 0.0019897993188351393]}\n",
            "{'sequence': 'selfsupervised pretraining text recognizers martin kimi — michal hradigi x faculty information technology, brno university technology,, brno, czech republic, ikiss,hradisfitvutbrcz', 'labels': ['Others', 'Datasets', 'Key Information Extraction', 'Deep Learning Models', 'Classification', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Tables', 'Handwriting Recognition', 'Optical Character Recognition'], 'scores': [0.5496776700019836, 0.08882489055395126, 0.08031682670116425, 0.07444152981042862, 0.0719398632645607, 0.05458500236272812, 0.03750664368271828, 0.015730250626802444, 0.01531715877354145, 0.01166011393070221]}\n",
            "{'sequence': 'drawing line deep segmentation extracting art ancient etruscan mirrors rafael sterzingerc, simon brennero, robert sablatnige, computer vision lab, tu wien, vienna, aut, ifirstnamelastnameltuwienacat', 'labels': ['Others', 'Key Information Extraction', 'Classification', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Datasets', 'Deep Learning Models', 'Optical Character Recognition', 'Tables', 'Handwriting Recognition'], 'scores': [0.3757282495498657, 0.129312202334404, 0.09448122978210449, 0.0907263532280922, 0.07278720289468765, 0.06114787235856056, 0.05214008688926697, 0.050550345331430435, 0.03720692917704582, 0.03591956943273544]}\n",
            "{'sequence': 'revisiting ngram models impact modern neural networks handwritten text recognition, solene tarride christopher, kermorvant', 'labels': ['Handwriting Recognition', 'Others', 'Key Information Extraction', 'Deep Learning Models', 'Multimodal Document Analysis', 'Datasets', 'Classification', 'Document Layout Understanding', 'Tables', 'Optical Character Recognition'], 'scores': [0.830411970615387, 0.05094826593995094, 0.023868974298238754, 0.019635966047644615, 0.018386656418442726, 0.018008021637797356, 0.012177144177258015, 0.011724653653800488, 0.011104654520750046, 0.0037336351815611124]}\n",
            "{'sequence': 'learning kern setwise estimation optimal letter space kei nakatsuru seiichi uchida°°°° , kyushu university, fukuoka, japan, uchidaaitkyushuuacjp', 'labels': ['Others', 'Key Information Extraction', 'Document Layout Understanding', 'Datasets', 'Handwriting Recognition', 'Classification', 'Multimodal Document Analysis', 'Deep Learning Models', 'Optical Character Recognition', 'Tables'], 'scores': [0.24040773510932922, 0.1722353994846344, 0.17023655772209167, 0.10783945769071579, 0.0831185057759285, 0.0752074271440506, 0.058155693113803864, 0.0379241481423378, 0.030127959325909615, 0.024747172370553017]}\n",
            "{'sequence': 'clustertabnet supervised clustering method table detection table structure recognition, marek polewczykl marco spinacil, sap business marek polewczyk , marco spinaci sap com', 'labels': ['Tables', 'Others', 'Datasets', 'Classification', 'Deep Learning Models', 'Key Information Extraction', 'Document Layout Understanding', 'Multimodal Document Analysis', 'Optical Character Recognition', 'Handwriting Recognition'], 'scores': [0.45008230209350586, 0.12395001947879791, 0.08774442225694656, 0.07949196547269821, 0.07817281037569046, 0.05851636081933975, 0.05671704560518265, 0.03720535710453987, 0.018186446279287338, 0.0099332919344306]}\n",
            "{'sequence': 'multiclass imbalanced dataset classification symbols piping instrumentation diagrams, laura jamiesonl , carlos francisco morenogarciai — , and, eyad elyanl', 'labels': ['Classification', 'Datasets', 'Others', 'Multimodal Document Analysis', 'Key Information Extraction', 'Tables', 'Document Layout Understanding', 'Deep Learning Models', 'Handwriting Recognition', 'Optical Character Recognition'], 'scores': [0.4169314205646515, 0.29794058203697205, 0.12824054062366486, 0.05632389709353447, 0.028305744752287865, 0.019491493701934814, 0.01933743804693222, 0.011482447385787964, 0.011215360835194588, 0.010731048882007599]}\n",
            "{'sequence': 'analysis calibration handwriting text recognition models eric ayllon°°° , francisco j castellanos ,, jorge calvozaragoza , pattern recognition artificial intelligence group, university alicante, spain', 'labels': ['Handwriting Recognition', 'Others', 'Classification', 'Datasets', 'Key Information Extraction', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Deep Learning Models', 'Tables', 'Optical Character Recognition'], 'scores': [0.8020695447921753, 0.06168429180979729, 0.035448599606752396, 0.03424600139260292, 0.02178529091179371, 0.018001729622483253, 0.010268143378198147, 0.009772292338311672, 0.0033622784540057182, 0.0033618060406297445]}\n",
            "{'sequence': 'oneshot transformerbased framework visuallyrich document understanding huynh vu x®, van pham hoail , , jeff yang', 'labels': ['Others', 'Key Information Extraction', 'Multimodal Document Analysis', 'Datasets', 'Optical Character Recognition', 'Classification', 'Handwriting Recognition', 'Document Layout Understanding', 'Tables', 'Deep Learning Models'], 'scores': [0.38537636399269104, 0.1261642575263977, 0.12492482364177704, 0.07746957987546921, 0.06991609185934067, 0.06118195876479149, 0.05463622137904167, 0.04698001593351364, 0.02849949151277542, 0.024851180613040924]}\n",
            "{'sequence': 'altchart enhancing vlmbased chart summarization multipretext tasks omar moured , jiaming zhang , ,, saquib sarfrazl, rainer, stiefelhagen , cvhci lab, karlsruhe institute technology, germany', 'labels': ['Others', 'Multimodal Document Analysis', 'Key Information Extraction', 'Document Layout Understanding', 'Datasets', 'Classification', 'Tables', 'Handwriting Recognition', 'Deep Learning Models', 'Optical Character Recognition'], 'scores': [0.5019992589950562, 0.13816455006599426, 0.09117551147937775, 0.07175584882497787, 0.06096401810646057, 0.03366506099700928, 0.030190568417310715, 0.025330347940325737, 0.023970304057002068, 0.022784456610679626]}\n",
            "{'sequence': 'socface project largescale collection, processing, analysis century french censuses, melodie boilletl , solene tarridei ,, yoann schneiders, bastien abadiel, lionel kesztenbaum , christopher', 'labels': ['Multimodal Document Analysis', 'Datasets', 'Classification', 'Others', 'Key Information Extraction', 'Document Layout Understanding', 'Optical Character Recognition', 'Tables', 'Handwriting Recognition', 'Deep Learning Models'], 'scores': [0.3361791670322418, 0.1651720553636551, 0.13382689654827118, 0.12506939470767975, 0.11481189727783203, 0.03414982929825783, 0.029580673202872276, 0.0239868201315403, 0.022411512210965157, 0.014811753295361996]}\n",
            "{'sequence': 'wikidt visualbased table recognition question answering dataset hui shit, yusheng xie , luis goncalves , sicun gaol, jishen zhao , university california san diego, amazon agi', 'labels': ['Tables', 'Datasets', 'Others', 'Key Information Extraction', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Classification', 'Optical Character Recognition', 'Deep Learning Models', 'Handwriting Recognition'], 'scores': [0.3878328800201416, 0.37960389256477356, 0.08848006278276443, 0.04302121698856354, 0.031228337436914444, 0.020574484020471573, 0.01883980631828308, 0.014118892140686512, 0.01195474062114954, 0.00434566754847765]}\n",
            "{'sequence': 'font impression estimation wild kazuki kitajima, daichi haraguchi seiichi uchida°°°° , kyushu university, fukuoka, japan, uchidaaitkyushuuacjp', 'labels': ['Others', 'Key Information Extraction', 'Classification', 'Datasets', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Handwriting Recognition', 'Tables', 'Optical Character Recognition', 'Deep Learning Models'], 'scores': [0.22641117870807648, 0.14163313806056976, 0.13335873186588287, 0.1115809753537178, 0.10949587821960449, 0.07601302117109299, 0.05645475909113884, 0.0558774396777153, 0.05228215083479881, 0.03689270466566086]}\n",
            "{'sequence': 'graphmllm graphbased multilevel layout languageindependent model document understanding, hesen dai , xiaohui lie, fei yin , xudong yana, shuqi mei , and, chenglin liu', 'labels': ['Document Layout Understanding', 'Others', 'Multimodal Document Analysis', 'Key Information Extraction', 'Classification', 'Datasets', 'Optical Character Recognition', 'Handwriting Recognition', 'Deep Learning Models', 'Tables'], 'scores': [0.6288977265357971, 0.15117399394512177, 0.08079800009727478, 0.04232233017683029, 0.025776714086532593, 0.02225041389465332, 0.015840701758861542, 0.015486130490899086, 0.009224413894116879, 0.008229600265622139]}\n",
            "{'sequence': 'lightweight multimodality feature fusion network visuallyrich document understanding, jeff yangi , huynh vu thei x and, hai luu tu x', 'labels': ['Others', 'Multimodal Document Analysis', 'Key Information Extraction', 'Datasets', 'Document Layout Understanding', 'Deep Learning Models', 'Optical Character Recognition', 'Handwriting Recognition', 'Classification', 'Tables'], 'scores': [0.4488641619682312, 0.18098142743110657, 0.06924529373645782, 0.05946263298392296, 0.05175049602985382, 0.0515795461833477, 0.044227104634046555, 0.03743726387619972, 0.035510748624801636, 0.020941374823451042]}\n",
            "{'sequence': 'mining analyzing statistical information untranscribed form images , e — , jose andresl, ® , alejandro h, toselli , , enrique vida ,', 'labels': ['Others', 'Multimodal Document Analysis', 'Key Information Extraction', 'Datasets', 'Document Layout Understanding', 'Classification', 'Optical Character Recognition', 'Deep Learning Models', 'Handwriting Recognition', 'Tables'], 'scores': [0.4172021448612213, 0.14616361260414124, 0.1384841948747635, 0.09026644378900528, 0.05081352964043617, 0.04668256267905235, 0.04478730633854866, 0.02521653100848198, 0.023484287783503532, 0.016899345442652702]}\n",
            "{'sequence': 'geometricaware control diffusion model handwritten chinese font generation yao , , , , gang', 'labels': ['Others', 'Handwriting Recognition', 'Classification', 'Multimodal Document Analysis', 'Key Information Extraction', 'Document Layout Understanding', 'Datasets', 'Deep Learning Models', 'Optical Character Recognition', 'Tables'], 'scores': [0.4021088778972626, 0.19971303641796112, 0.07018792629241943, 0.06723032146692276, 0.06704577803611755, 0.060811955481767654, 0.0523916557431221, 0.029555264860391617, 0.026887472718954086, 0.024067772552371025]}\n",
            "{'sequence': 'test time augmentation defense adversarial attacks online handwriting yoh yamashita brian kenji iwanaww , graduate school information science electrical engineering, kyushu university, fukuoka, japan', 'labels': ['Handwriting Recognition', 'Others', 'Key Information Extraction', 'Datasets', 'Multimodal Document Analysis', 'Classification', 'Document Layout Understanding', 'Tables', 'Deep Learning Models', 'Optical Character Recognition'], 'scores': [0.33913806080818176, 0.15888634324073792, 0.12839844822883606, 0.07693292200565338, 0.0707658976316452, 0.07023134082555771, 0.06425482779741287, 0.0359889417886734, 0.033049240708351135, 0.022354044020175934]}\n",
            "{'sequence': 'crepe coordinateaware endtoend document parser yamato okamoto , , youngmin baek , , geewook kiml , ryota nakao , — —', 'labels': ['Others', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Key Information Extraction', 'Classification', 'Optical Character Recognition', 'Datasets', 'Handwriting Recognition', 'Tables', 'Deep Learning Models'], 'scores': [0.25851982831954956, 0.18365967273712158, 0.14278045296669006, 0.1404712200164795, 0.08020801842212677, 0.059044044464826584, 0.05139097198843956, 0.04711213707923889, 0.01948481798171997, 0.01732887513935566]}\n",
            "{'sequence': 'typographic text generation offtheshelf diffusion model khaytze peong, seiichi uchida , daichi, — , haraguchip°°', 'labels': ['Others', 'Key Information Extraction', 'Document Layout Understanding', 'Multimodal Document Analysis', 'Classification', 'Handwriting Recognition', 'Datasets', 'Optical Character Recognition', 'Deep Learning Models', 'Tables'], 'scores': [0.32265451550483704, 0.12731075286865234, 0.1052846759557724, 0.10267911851406097, 0.08852016180753708, 0.08331937342882156, 0.07479701191186905, 0.03839905187487602, 0.032108571380376816, 0.024926768615841866]}\n",
            "{'sequence': 'reading order independent metrics information extraction handwritten documents, david villanovaaparisii , solene, tarride , carlosd', 'labels': ['Handwriting Recognition', 'Others', 'Document Layout Understanding', 'Multimodal Document Analysis', 'Classification', 'Key Information Extraction', 'Datasets', 'Optical Character Recognition', 'Tables', 'Deep Learning Models'], 'scores': [0.3460403084754944, 0.31694960594177246, 0.09038620442152023, 0.07815618813037872, 0.0521455742418766, 0.05072067305445671, 0.028035355731844902, 0.01604066975414753, 0.014106539078056812, 0.007418930530548096]}\n",
            "{'sequence': 'weakly supervised training hologram verification identity documents glen pouliquen , guillaume chironle, joseph chazalon g,, thierry geraud , ahmad montaser awalle, idnow ai ml center excellence, cessonsevigne, france', 'labels': ['Others', 'Key Information Extraction', 'Optical Character Recognition', 'Multimodal Document Analysis', 'Tables', 'Datasets', 'Classification', 'Handwriting Recognition', 'Document Layout Understanding', 'Deep Learning Models'], 'scores': [0.5609448552131653, 0.09970618784427643, 0.0921158418059349, 0.04937334358692169, 0.04865165054798126, 0.043313466012477875, 0.0387176051735878, 0.027304530143737793, 0.027086012065410614, 0.01278650015592575]}\n",
            "{'sequence': 'historical handwritten dataset ethiopic ocr baseline models humanlevel performance, birhanu hailu belays , isabelle, guyon , , , tadele mengiste ,', 'labels': ['Datasets', 'Others', 'Handwriting Recognition', 'Key Information Extraction', 'Multimodal Document Analysis', 'Document Layout Understanding', 'Classification', 'Optical Character Recognition', 'Tables', 'Deep Learning Models'], 'scores': [0.44828715920448303, 0.12161839008331299, 0.09743659198284149, 0.08059005439281464, 0.060620181262493134, 0.05343862250447273, 0.05090869590640068, 0.039487455040216446, 0.031606484204530716, 0.016006365418434143]}\n",
            "{'sequence': 'gdp generic document pretraining improve document understanding akkshita trivedil , akarsh upadhyayl , rudrabha mukhopadhyay , santanu, chaudhury', 'labels': ['Others', 'Deep Learning Models', 'Key Information Extraction', 'Multimodal Document Analysis', 'Datasets', 'Handwriting Recognition', 'Document Layout Understanding', 'Classification', 'Optical Character Recognition', 'Tables'], 'scores': [0.42587462067604065, 0.29916152358055115, 0.05543002858757973, 0.053492024540901184, 0.04176539555191994, 0.030599813908338547, 0.03000267967581749, 0.025077467784285545, 0.01969178207218647, 0.018904617056250572]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dq8UJBk_-iEE",
        "outputId": "c02307c8-204f-48fb-8cbf-f761a3c4a2c2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                              title  \\\n",
              "0   0049.pdf  Synthesizing Realistic Data for Table Recognition   \n",
              "1   0044.pdf              DLAFormer An EndtoEnd Transformer For   \n",
              "2   0086.pdf  Handwritten Document Recognition Using Pretrai...   \n",
              "3   0020.pdf  Improving Automatic Text Recognition with Lang...   \n",
              "4   0089.pdf  GlobalSEG Text Semantic Segmentation Based on ...   \n",
              "5   0083.pdf  ImpressionCLIP Contrastive ShapeImpression Emb...   \n",
              "6   0027.pdf  Towards EndtoEnd SemiSupervised Table Detectio...   \n",
              "7   0045.pdf  UniVIE A Unified Label Space Approach to Visua...   \n",
              "8   0060.pdf  MultiCell Decoder and Mutual Learning for Tabl...   \n",
              "9   0051.pdf  SourceFree Domain Adaptation for Optical Music...   \n",
              "10  0042.pdf  LMTextSpotter Towards Better Scene Text Spotti...   \n",
              "11  0007.pdf  Binarizing Documents by Leveraging both Space ...   \n",
              "12  0025.pdf                                                      \n",
              "13  0043.pdf  Dynamic Relation Transformer for Contextual Te...   \n",
              "14  0094.pdf  Multimodal Adaptive Inference for Document Ima...   \n",
              "15  0069.pdf  MOoSE MultiOrientation Sharing Experts for Ope...   \n",
              "16  0092.pdf  More and Less Enhancing Abundance and Refining...   \n",
              "17  0055.pdf  End to End Table Transformer Yun Young Choi X,...   \n",
              "18  0071.pdf  Deep LearningDriven Innovative Model for Gener...   \n",
              "19  0084.pdf    PrivacyAware Document Visual Question Answering   \n",
              "20  0054.pdf  A regionbased approach for layout analysis of ...   \n",
              "21  0081.pdf  Multitask Learning for License Plate Recogniti...   \n",
              "22  0016.pdf  A RealTime Scene Uyghur Text Detection Network...   \n",
              "23  0004.pdf  SAGHOG SelfSupervised Autoencoder for Generati...   \n",
              "24  0075.pdf  ConClue Conditional Clue Extraction for Multip...   \n",
              "25  0077.pdf  Progressive Evolution from SinglePoint to Poly...   \n",
              "26  0030.pdf  Visual Prompt Learning for Chinese Handwriting...   \n",
              "27  0038.pdf  Selfsupervised Pretraining of Text Recognizers...   \n",
              "28  0037.pdf  Drawing the Line Deep Segmentation for Extract...   \n",
              "29  0013.pdf  Revisiting NGram Models Their Impact in Modern...   \n",
              "30  0064.pdf  Learning to Kern Setwise Estimation of Optimal...   \n",
              "31  0095.pdf  ClusterTabNet Supervised clustering method for...   \n",
              "32  0018.pdf  A Multiclass Imbalanced Dataset Classification...   \n",
              "33  0058.pdf  Analysis of the Calibration of Handwriting Tex...   \n",
              "34  0074.pdf  Oneshot Transformerbased Framework for Visuall...   \n",
              "35  0032.pdf  AltChart Enhancing VLMbased Chart Summarizatio...   \n",
              "36  0046.pdf  The Socface Project LargeScale Collection, Pro...   \n",
              "37  0082.pdf  WikiDT Visualbased Table Recognition and Quest...   \n",
              "38  0079.pdf  Font Impression Estimation in the Wild Kazuki ...   \n",
              "39  0059.pdf  GraphMLLM A Graphbased Multilevel Layout Langu...   \n",
              "40  0019.pdf  LightWeight MultiModality Feature Fusion Netwo...   \n",
              "41  0034.pdf  Mining and Analyzing Statistical Information f...   \n",
              "42  0031.pdf  Geometricaware Control in Diffusion Model for ...   \n",
              "43  0078.pdf  Test Time Augmentation as a Defense Against Ad...   \n",
              "44  0022.pdf     CREPE CoordinateAware EndtoEnd Document Parser   \n",
              "45  0080.pdf  Typographic Text Generation with OfftheShelf D...   \n",
              "46  0097.pdf  Reading Order Independent Metrics for Informat...   \n",
              "47  0047.pdf  Weakly Supervised Training for Hologram Verifi...   \n",
              "48  0036.pdf  A Historical Handwritten Dataset for Ethiopic ...   \n",
              "49  0050.pdf  GDP Generic Document Pretraining to Improve Do...   \n",
              "\n",
              "                                              authors  \\\n",
              "0   Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...   \n",
              "1   Document Layout Analysis, Jiawei Wang , ,,t, K...   \n",
              "2   Daniel Parresl   , Dan Aniteii   —  , and, Rob...   \n",
              "3                        Library, Solene Tarride , ‘,   \n",
              "4   Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...   \n",
              "5   Yugo Kubota, Daichi Haraguchi , and, Seiichi U...   \n",
              "6   Tahira Shehzadi , , X Shalini Sarode ,   ,, Di...   \n",
              "7   Documents, Kai Hul t , Jiawei Wang , ,, Weihon...   \n",
              "8   Takaya Kawakatsui , Preferred Networks, Inc,  ...   \n",
              "9   Adrian Rose , Eliseo FuentesMartinezi , Maria ...   \n",
              "10  Xin Xia , Guodong Ding , and Siyuan Lie,   Col...   \n",
              "11  Fabio Quattrini , Vittorio Pippip l , Silvia C...   \n",
              "12  A Hybrid Approach for Document Layout, Analysi...   \n",
              "13  Jiawei Wang t, Shunchi Zhang , t, Kai Hul t,, ...   \n",
              "14  Omar Hamedl, Souhail Bakkali    , Matthew, Bla...   \n",
              "15  Chang Liu, Simon Corbille, and Elisa H Barney ...   \n",
              "16  Scene Text Image SuperResolution, Wei Yang , ,...   \n",
              "17  Namwook Kim °°  Taehee Lee°°° , and, Seongho J...   \n",
              "18  Pan Qiangangl, Hu Yahongl, Xie Youbai , Meng X...   \n",
              "19  Ruben Tito, Khanh Nguyen, Marlon Tobaben , Rao...   \n",
              "20  Francisco J Castellanos  Juan P, MartinezEstes...   \n",
              "21  ZhenLun Mot, SongLu Chenl, Qi Liu , Feng Chen ...   \n",
              "22  Mayire Ibrayim , , Mengmeng Chen , ,, Askar Ha...   \n",
              "23  Marco PeerO, Florian Klebere, and Robert Sabla...   \n",
              "24  Wangli Yang , Jie Yangl, Wanqing Li, and Yi Gu...   \n",
              "25  Linger Denglt, Mingxin Huang t, Xudong Xie , Y...   \n",
              "26  Gang yao ,   , Ning Ding ,    Tianqi, Zhao ,  ...   \n",
              "27  Faculty of Information Technology, Brno Univer...   \n",
              "28  Rafael SterzingerC, Simon BrennerO, and Robert...   \n",
              "29  Recognition, Solene Tarride  and Christopher, ...   \n",
              "30  Kei Nakatsuru and Seiichi Uchida°°°° , Kyushu ...   \n",
              "31  recognition, Marek Polewczykl and Marco Spinac...   \n",
              "32  Diagrams, Laura Jamiesonl , Carlos Francisco M...   \n",
              "33  Eric Ayllon°°° , Francisco J Castellanos ,, an...   \n",
              "34  Huynh Vu The  x®, Van Pham Hoail  ,  , and Jef...   \n",
              "35  Omar Moured , Jiaming Zhang , ,, M Saquib Sarf...   \n",
              "36  Censuses, Melodie Boilletl   , Solene Tarridei...   \n",
              "37  Hui Shit, Yusheng Xie , Luis Goncalves , Sicun...   \n",
              "38  Seiichi Uchida°°°° , Kyushu University, Fukuok...   \n",
              "39  Document Understanding, HeSen Dai , XiaoHui Li...   \n",
              "40  Understanding, Jeff yangi   , Huynh Vu Thei  X...   \n",
              "41   ,  E   — , Jose Andresl, ®   , Alejandro H, T...   \n",
              "42                              yao ,   ,  ,   , Gang   \n",
              "43  Yoh Yamashita and Brian Kenji Iwanaww , Gradua...   \n",
              "44  Yamato Okamoto ,  t  , Youngmin Baek ,   , Gee...   \n",
              "45  KhayTze Peong, Seiichi Uchida , and Daichi, — ...   \n",
              "46  Documents, David VillanovaAparisii   , Solene,...   \n",
              "47  Glen Pouliquen , Guillaume Chironle, Joseph Ch...   \n",
              "48  Performance, Birhanu Hailu Belays   , Isabelle...   \n",
              "49  Akkshita Trivedil   , Akarsh Upadhyayl  , Rudr...   \n",
              "\n",
              "                                   title_preprocessed  \\\n",
              "0       synthesizing realistic data table recognition   \n",
              "1                      dlaformer endtoend transformer   \n",
              "2   handwritten document recognition using pretrai...   \n",
              "3   improving automatic text recognition language ...   \n",
              "4   globalseg text semantic segmentation based glo...   \n",
              "5   impressionclip contrastive shapeimpression emb...   \n",
              "6   towards endtoend semisupervised table detectio...   \n",
              "7   univie unified label space approach visual inf...   \n",
              "8   multicell decoder mutual learning table struct...   \n",
              "9   sourcefree domain adaptation optical music rec...   \n",
              "10  lmtextspotter towards better scene text spotti...   \n",
              "11    binarizing documents leveraging space frequency   \n",
              "12                                                      \n",
              "13  dynamic relation transformer contextual text b...   \n",
              "14  multimodal adaptive inference document image c...   \n",
              "15  moose multiorientation sharing experts openset...   \n",
              "16  less enhancing abundance refining redundancy t...   \n",
              "17  end end table transformer yun young choi x, ta...   \n",
              "18  deep learningdriven innovative model generatin...   \n",
              "19    privacyaware document visual question answering   \n",
              "20  regionbased approach layout analysis music sco...   \n",
              "21  multitask learning license plate recognition u...   \n",
              "22  realtime scene uyghur text detection network b...   \n",
              "23  saghog selfsupervised autoencoder generating h...   \n",
              "24  conclue conditional clue extraction multiple c...   \n",
              "25  progressive evolution singlepoint polygon scen...   \n",
              "26  visual prompt learning chinese handwriting rec...   \n",
              "27  selfsupervised pretraining text recognizers ma...   \n",
              "28  drawing line deep segmentation extracting art ...   \n",
              "29  revisiting ngram models impact modern neural n...   \n",
              "30  learning kern setwise estimation optimal lette...   \n",
              "31  clustertabnet supervised clustering method tab...   \n",
              "32  multiclass imbalanced dataset classification s...   \n",
              "33  analysis calibration handwriting text recognit...   \n",
              "34  oneshot transformerbased framework visuallyric...   \n",
              "35  altchart enhancing vlmbased chart summarizatio...   \n",
              "36  socface project largescale collection, process...   \n",
              "37  wikidt visualbased table recognition question ...   \n",
              "38  font impression estimation wild kazuki kitajim...   \n",
              "39  graphmllm graphbased multilevel layout languag...   \n",
              "40  lightweight multimodality feature fusion netwo...   \n",
              "41  mining analyzing statistical information untra...   \n",
              "42  geometricaware control diffusion model handwri...   \n",
              "43  test time augmentation defense adversarial att...   \n",
              "44     crepe coordinateaware endtoend document parser   \n",
              "45  typographic text generation offtheshelf diffus...   \n",
              "46  reading order independent metrics information ...   \n",
              "47  weakly supervised training hologram verificati...   \n",
              "48  historical handwritten dataset ethiopic ocr ba...   \n",
              "49  gdp generic document pretraining improve docum...   \n",
              "\n",
              "                                 authors_preprocessed  \\\n",
              "0   qiyu houl , jun wangl x pa meixuan, qiao , luj...   \n",
              "1   document layout analysis, jiawei wang , ,,t, k...   \n",
              "2   daniel parresl , dan aniteii — , and, roberto ...   \n",
              "3                        library, solene tarride , ‘,   \n",
              "4   wenjun sunl , hanh thi hong, tran , , carlosem...   \n",
              "5   yugo kubota, daichi haraguchi , and, seiichi u...   \n",
              "6   tahira shehzadi , , x shalini sarode , ,, didi...   \n",
              "7   documents, kai hul , jiawei wang , ,, weihong ...   \n",
              "8   takaya kawakatsui , preferred networks, inc, o...   \n",
              "9   adrian rose , eliseo fuentesmartinezi , maria ...   \n",
              "10  xin xia , guodong ding , siyuan lie, college c...   \n",
              "11  fabio quattrini , vittorio pippip l , silvia c...   \n",
              "12  hybrid approach document layout, analysis docu...   \n",
              "13  jiawei wang t, shunchi zhang , t, kai hul t,, ...   \n",
              "14  omar hamedl, souhail bakkali , matthew, blasch...   \n",
              "15  chang liu, simon corbille, elisa h barney smit...   \n",
              "16  scene text image superresolution, wei yang , ,...   \n",
              "17  namwook kim °° taehee lee°°° , and, seongho jo...   \n",
              "18  pan qiangangl, hu yahongl, xie youbai , meng x...   \n",
              "19  ruben tito, khanh nguyen, marlon tobaben , rao...   \n",
              "20  francisco j castellanos juan p, martinezesteso...   \n",
              "21  zhenlun mot, songlu chenl, qi liu , feng chen ...   \n",
              "22  mayire ibrayim , , mengmeng chen , ,, askar ha...   \n",
              "23  marco peero, florian klebere, robert sablatnig...   \n",
              "24  wangli yang , jie yangl, wanqing li, yi guo , ...   \n",
              "25  linger denglt, mingxin huang t, xudong xie , y...   \n",
              "26  gang yao , , ning ding , tianqi, zhao , , keme...   \n",
              "27  faculty information technology, brno universit...   \n",
              "28  rafael sterzingerc, simon brennero, robert sab...   \n",
              "29  recognition, solene tarride christopher, kermo...   \n",
              "30  kei nakatsuru seiichi uchida°°°° , kyushu univ...   \n",
              "31  recognition, marek polewczykl marco spinacil, ...   \n",
              "32  diagrams, laura jamiesonl , carlos francisco m...   \n",
              "33  eric ayllon°°° , francisco j castellanos ,, jo...   \n",
              "34          huynh vu x®, van pham hoail , , jeff yang   \n",
              "35  omar moured , jiaming zhang , ,, saquib sarfra...   \n",
              "36  censuses, melodie boilletl , solene tarridei ,...   \n",
              "37  hui shit, yusheng xie , luis goncalves , sicun...   \n",
              "38  seiichi uchida°°°° , kyushu university, fukuok...   \n",
              "39  document understanding, hesen dai , xiaohui li...   \n",
              "40  understanding, jeff yangi , huynh vu thei x an...   \n",
              "41  , e — , jose andresl, ® , alejandro h, toselli...   \n",
              "42                                   yao , , , , gang   \n",
              "43  yoh yamashita brian kenji iwanaww , graduate s...   \n",
              "44  yamato okamoto , , youngmin baek , , geewook k...   \n",
              "45  khaytze peong, seiichi uchida , daichi, — , ha...   \n",
              "46  documents, david villanovaaparisii , solene, t...   \n",
              "47  glen pouliquen , guillaume chironle, joseph ch...   \n",
              "48  performance, birhanu hailu belays , isabelle, ...   \n",
              "49  akkshita trivedil , akarsh upadhyayl , rudrabh...   \n",
              "\n",
              "                            label  \n",
              "0                          Tables  \n",
              "1            Deep Learning Models  \n",
              "2         Handwriting Recognition  \n",
              "3                          Others  \n",
              "4                          Others  \n",
              "5                          Others  \n",
              "6                          Tables  \n",
              "7   Document Layout Understanding  \n",
              "8                          Tables  \n",
              "9                          Others  \n",
              "10                         Others  \n",
              "11   Multimodal Document Analysis  \n",
              "12   Multimodal Document Analysis  \n",
              "13                         Others  \n",
              "14                 Classification  \n",
              "15                         Others  \n",
              "16                         Others  \n",
              "17                         Others  \n",
              "18           Deep Learning Models  \n",
              "19                         Others  \n",
              "20                         Others  \n",
              "21                         Others  \n",
              "22                         Others  \n",
              "23                         Others  \n",
              "24     Key Information Extraction  \n",
              "25                         Others  \n",
              "26        Handwriting Recognition  \n",
              "27                         Others  \n",
              "28                         Others  \n",
              "29        Handwriting Recognition  \n",
              "30                         Others  \n",
              "31                         Tables  \n",
              "32                 Classification  \n",
              "33        Handwriting Recognition  \n",
              "34                         Others  \n",
              "35                         Others  \n",
              "36   Multimodal Document Analysis  \n",
              "37                         Tables  \n",
              "38                         Others  \n",
              "39  Document Layout Understanding  \n",
              "40                         Others  \n",
              "41                         Others  \n",
              "42                         Others  \n",
              "43        Handwriting Recognition  \n",
              "44                         Others  \n",
              "45                         Others  \n",
              "46        Handwriting Recognition  \n",
              "47                         Others  \n",
              "48                       Datasets  \n",
              "49                         Others  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77ed246d-0dd9-46ad-88f6-a0d589ce053c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>title_preprocessed</th>\n",
              "      <th>authors_preprocessed</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0049.pdf</td>\n",
              "      <td>Synthesizing Realistic Data for Table Recognition</td>\n",
              "      <td>Qiyu Houl  , Jun wangl  X PA Meixuan, Qiao , a...</td>\n",
              "      <td>synthesizing realistic data table recognition</td>\n",
              "      <td>qiyu houl , jun wangl x pa meixuan, qiao , luj...</td>\n",
              "      <td>Tables</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0044.pdf</td>\n",
              "      <td>DLAFormer An EndtoEnd Transformer For</td>\n",
              "      <td>Document Layout Analysis, Jiawei Wang , ,,t, K...</td>\n",
              "      <td>dlaformer endtoend transformer</td>\n",
              "      <td>document layout analysis, jiawei wang , ,,t, k...</td>\n",
              "      <td>Deep Learning Models</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0086.pdf</td>\n",
              "      <td>Handwritten Document Recognition Using Pretrai...</td>\n",
              "      <td>Daniel Parresl   , Dan Aniteii   —  , and, Rob...</td>\n",
              "      <td>handwritten document recognition using pretrai...</td>\n",
              "      <td>daniel parresl , dan aniteii — , and, roberto ...</td>\n",
              "      <td>Handwriting Recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0020.pdf</td>\n",
              "      <td>Improving Automatic Text Recognition with Lang...</td>\n",
              "      <td>Library, Solene Tarride , ‘,</td>\n",
              "      <td>improving automatic text recognition language ...</td>\n",
              "      <td>library, solene tarride , ‘,</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0089.pdf</td>\n",
              "      <td>GlobalSEG Text Semantic Segmentation Based on ...</td>\n",
              "      <td>Wenjun Sunl   , Hanh Thi Hong, Tran , ,    Car...</td>\n",
              "      <td>globalseg text semantic segmentation based glo...</td>\n",
              "      <td>wenjun sunl , hanh thi hong, tran , , carlosem...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0083.pdf</td>\n",
              "      <td>ImpressionCLIP Contrastive ShapeImpression Emb...</td>\n",
              "      <td>Yugo Kubota, Daichi Haraguchi , and, Seiichi U...</td>\n",
              "      <td>impressionclip contrastive shapeimpression emb...</td>\n",
              "      <td>yugo kubota, daichi haraguchi , and, seiichi u...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0027.pdf</td>\n",
              "      <td>Towards EndtoEnd SemiSupervised Table Detectio...</td>\n",
              "      <td>Tahira Shehzadi , , X Shalini Sarode ,   ,, Di...</td>\n",
              "      <td>towards endtoend semisupervised table detectio...</td>\n",
              "      <td>tahira shehzadi , , x shalini sarode , ,, didi...</td>\n",
              "      <td>Tables</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0045.pdf</td>\n",
              "      <td>UniVIE A Unified Label Space Approach to Visua...</td>\n",
              "      <td>Documents, Kai Hul t , Jiawei Wang , ,, Weihon...</td>\n",
              "      <td>univie unified label space approach visual inf...</td>\n",
              "      <td>documents, kai hul , jiawei wang , ,, weihong ...</td>\n",
              "      <td>Document Layout Understanding</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0060.pdf</td>\n",
              "      <td>MultiCell Decoder and Mutual Learning for Tabl...</td>\n",
              "      <td>Takaya Kawakatsui , Preferred Networks, Inc,  ...</td>\n",
              "      <td>multicell decoder mutual learning table struct...</td>\n",
              "      <td>takaya kawakatsui , preferred networks, inc, o...</td>\n",
              "      <td>Tables</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0051.pdf</td>\n",
              "      <td>SourceFree Domain Adaptation for Optical Music...</td>\n",
              "      <td>Adrian Rose , Eliseo FuentesMartinezi , Maria ...</td>\n",
              "      <td>sourcefree domain adaptation optical music rec...</td>\n",
              "      <td>adrian rose , eliseo fuentesmartinezi , maria ...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0042.pdf</td>\n",
              "      <td>LMTextSpotter Towards Better Scene Text Spotti...</td>\n",
              "      <td>Xin Xia , Guodong Ding , and Siyuan Lie,   Col...</td>\n",
              "      <td>lmtextspotter towards better scene text spotti...</td>\n",
              "      <td>xin xia , guodong ding , siyuan lie, college c...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0007.pdf</td>\n",
              "      <td>Binarizing Documents by Leveraging both Space ...</td>\n",
              "      <td>Fabio Quattrini , Vittorio Pippip l , Silvia C...</td>\n",
              "      <td>binarizing documents leveraging space frequency</td>\n",
              "      <td>fabio quattrini , vittorio pippip l , silvia c...</td>\n",
              "      <td>Multimodal Document Analysis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0025.pdf</td>\n",
              "      <td></td>\n",
              "      <td>A Hybrid Approach for Document Layout, Analysi...</td>\n",
              "      <td></td>\n",
              "      <td>hybrid approach document layout, analysis docu...</td>\n",
              "      <td>Multimodal Document Analysis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0043.pdf</td>\n",
              "      <td>Dynamic Relation Transformer for Contextual Te...</td>\n",
              "      <td>Jiawei Wang t, Shunchi Zhang , t, Kai Hul t,, ...</td>\n",
              "      <td>dynamic relation transformer contextual text b...</td>\n",
              "      <td>jiawei wang t, shunchi zhang , t, kai hul t,, ...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0094.pdf</td>\n",
              "      <td>Multimodal Adaptive Inference for Document Ima...</td>\n",
              "      <td>Omar Hamedl, Souhail Bakkali    , Matthew, Bla...</td>\n",
              "      <td>multimodal adaptive inference document image c...</td>\n",
              "      <td>omar hamedl, souhail bakkali , matthew, blasch...</td>\n",
              "      <td>Classification</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0069.pdf</td>\n",
              "      <td>MOoSE MultiOrientation Sharing Experts for Ope...</td>\n",
              "      <td>Chang Liu, Simon Corbille, and Elisa H Barney ...</td>\n",
              "      <td>moose multiorientation sharing experts openset...</td>\n",
              "      <td>chang liu, simon corbille, elisa h barney smit...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0092.pdf</td>\n",
              "      <td>More and Less Enhancing Abundance and Refining...</td>\n",
              "      <td>Scene Text Image SuperResolution, Wei Yang , ,...</td>\n",
              "      <td>less enhancing abundance refining redundancy t...</td>\n",
              "      <td>scene text image superresolution, wei yang , ,...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0055.pdf</td>\n",
              "      <td>End to End Table Transformer Yun Young Choi X,...</td>\n",
              "      <td>Namwook Kim °°  Taehee Lee°°° , and, Seongho J...</td>\n",
              "      <td>end end table transformer yun young choi x, ta...</td>\n",
              "      <td>namwook kim °° taehee lee°°° , and, seongho jo...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0071.pdf</td>\n",
              "      <td>Deep LearningDriven Innovative Model for Gener...</td>\n",
              "      <td>Pan Qiangangl, Hu Yahongl, Xie Youbai , Meng X...</td>\n",
              "      <td>deep learningdriven innovative model generatin...</td>\n",
              "      <td>pan qiangangl, hu yahongl, xie youbai , meng x...</td>\n",
              "      <td>Deep Learning Models</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0084.pdf</td>\n",
              "      <td>PrivacyAware Document Visual Question Answering</td>\n",
              "      <td>Ruben Tito, Khanh Nguyen, Marlon Tobaben , Rao...</td>\n",
              "      <td>privacyaware document visual question answering</td>\n",
              "      <td>ruben tito, khanh nguyen, marlon tobaben , rao...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0054.pdf</td>\n",
              "      <td>A regionbased approach for layout analysis of ...</td>\n",
              "      <td>Francisco J Castellanos  Juan P, MartinezEstes...</td>\n",
              "      <td>regionbased approach layout analysis music sco...</td>\n",
              "      <td>francisco j castellanos juan p, martinezesteso...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0081.pdf</td>\n",
              "      <td>Multitask Learning for License Plate Recogniti...</td>\n",
              "      <td>ZhenLun Mot, SongLu Chenl, Qi Liu , Feng Chen ...</td>\n",
              "      <td>multitask learning license plate recognition u...</td>\n",
              "      <td>zhenlun mot, songlu chenl, qi liu , feng chen ...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0016.pdf</td>\n",
              "      <td>A RealTime Scene Uyghur Text Detection Network...</td>\n",
              "      <td>Mayire Ibrayim , , Mengmeng Chen , ,, Askar Ha...</td>\n",
              "      <td>realtime scene uyghur text detection network b...</td>\n",
              "      <td>mayire ibrayim , , mengmeng chen , ,, askar ha...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0004.pdf</td>\n",
              "      <td>SAGHOG SelfSupervised Autoencoder for Generati...</td>\n",
              "      <td>Marco PeerO, Florian Klebere, and Robert Sabla...</td>\n",
              "      <td>saghog selfsupervised autoencoder generating h...</td>\n",
              "      <td>marco peero, florian klebere, robert sablatnig...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0075.pdf</td>\n",
              "      <td>ConClue Conditional Clue Extraction for Multip...</td>\n",
              "      <td>Wangli Yang , Jie Yangl, Wanqing Li, and Yi Gu...</td>\n",
              "      <td>conclue conditional clue extraction multiple c...</td>\n",
              "      <td>wangli yang , jie yangl, wanqing li, yi guo , ...</td>\n",
              "      <td>Key Information Extraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0077.pdf</td>\n",
              "      <td>Progressive Evolution from SinglePoint to Poly...</td>\n",
              "      <td>Linger Denglt, Mingxin Huang t, Xudong Xie , Y...</td>\n",
              "      <td>progressive evolution singlepoint polygon scen...</td>\n",
              "      <td>linger denglt, mingxin huang t, xudong xie , y...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0030.pdf</td>\n",
              "      <td>Visual Prompt Learning for Chinese Handwriting...</td>\n",
              "      <td>Gang yao ,   , Ning Ding ,    Tianqi, Zhao ,  ...</td>\n",
              "      <td>visual prompt learning chinese handwriting rec...</td>\n",
              "      <td>gang yao , , ning ding , tianqi, zhao , , keme...</td>\n",
              "      <td>Handwriting Recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0038.pdf</td>\n",
              "      <td>Selfsupervised Pretraining of Text Recognizers...</td>\n",
              "      <td>Faculty of Information Technology, Brno Univer...</td>\n",
              "      <td>selfsupervised pretraining text recognizers ma...</td>\n",
              "      <td>faculty information technology, brno universit...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0037.pdf</td>\n",
              "      <td>Drawing the Line Deep Segmentation for Extract...</td>\n",
              "      <td>Rafael SterzingerC, Simon BrennerO, and Robert...</td>\n",
              "      <td>drawing line deep segmentation extracting art ...</td>\n",
              "      <td>rafael sterzingerc, simon brennero, robert sab...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0013.pdf</td>\n",
              "      <td>Revisiting NGram Models Their Impact in Modern...</td>\n",
              "      <td>Recognition, Solene Tarride  and Christopher, ...</td>\n",
              "      <td>revisiting ngram models impact modern neural n...</td>\n",
              "      <td>recognition, solene tarride christopher, kermo...</td>\n",
              "      <td>Handwriting Recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0064.pdf</td>\n",
              "      <td>Learning to Kern Setwise Estimation of Optimal...</td>\n",
              "      <td>Kei Nakatsuru and Seiichi Uchida°°°° , Kyushu ...</td>\n",
              "      <td>learning kern setwise estimation optimal lette...</td>\n",
              "      <td>kei nakatsuru seiichi uchida°°°° , kyushu univ...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0095.pdf</td>\n",
              "      <td>ClusterTabNet Supervised clustering method for...</td>\n",
              "      <td>recognition, Marek Polewczykl and Marco Spinac...</td>\n",
              "      <td>clustertabnet supervised clustering method tab...</td>\n",
              "      <td>recognition, marek polewczykl marco spinacil, ...</td>\n",
              "      <td>Tables</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0018.pdf</td>\n",
              "      <td>A Multiclass Imbalanced Dataset Classification...</td>\n",
              "      <td>Diagrams, Laura Jamiesonl , Carlos Francisco M...</td>\n",
              "      <td>multiclass imbalanced dataset classification s...</td>\n",
              "      <td>diagrams, laura jamiesonl , carlos francisco m...</td>\n",
              "      <td>Classification</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0058.pdf</td>\n",
              "      <td>Analysis of the Calibration of Handwriting Tex...</td>\n",
              "      <td>Eric Ayllon°°° , Francisco J Castellanos ,, an...</td>\n",
              "      <td>analysis calibration handwriting text recognit...</td>\n",
              "      <td>eric ayllon°°° , francisco j castellanos ,, jo...</td>\n",
              "      <td>Handwriting Recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0074.pdf</td>\n",
              "      <td>Oneshot Transformerbased Framework for Visuall...</td>\n",
              "      <td>Huynh Vu The  x®, Van Pham Hoail  ,  , and Jef...</td>\n",
              "      <td>oneshot transformerbased framework visuallyric...</td>\n",
              "      <td>huynh vu x®, van pham hoail , , jeff yang</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0032.pdf</td>\n",
              "      <td>AltChart Enhancing VLMbased Chart Summarizatio...</td>\n",
              "      <td>Omar Moured , Jiaming Zhang , ,, M Saquib Sarf...</td>\n",
              "      <td>altchart enhancing vlmbased chart summarizatio...</td>\n",
              "      <td>omar moured , jiaming zhang , ,, saquib sarfra...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0046.pdf</td>\n",
              "      <td>The Socface Project LargeScale Collection, Pro...</td>\n",
              "      <td>Censuses, Melodie Boilletl   , Solene Tarridei...</td>\n",
              "      <td>socface project largescale collection, process...</td>\n",
              "      <td>censuses, melodie boilletl , solene tarridei ,...</td>\n",
              "      <td>Multimodal Document Analysis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0082.pdf</td>\n",
              "      <td>WikiDT Visualbased Table Recognition and Quest...</td>\n",
              "      <td>Hui Shit, Yusheng Xie , Luis Goncalves , Sicun...</td>\n",
              "      <td>wikidt visualbased table recognition question ...</td>\n",
              "      <td>hui shit, yusheng xie , luis goncalves , sicun...</td>\n",
              "      <td>Tables</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0079.pdf</td>\n",
              "      <td>Font Impression Estimation in the Wild Kazuki ...</td>\n",
              "      <td>Seiichi Uchida°°°° , Kyushu University, Fukuok...</td>\n",
              "      <td>font impression estimation wild kazuki kitajim...</td>\n",
              "      <td>seiichi uchida°°°° , kyushu university, fukuok...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0059.pdf</td>\n",
              "      <td>GraphMLLM A Graphbased Multilevel Layout Langu...</td>\n",
              "      <td>Document Understanding, HeSen Dai , XiaoHui Li...</td>\n",
              "      <td>graphmllm graphbased multilevel layout languag...</td>\n",
              "      <td>document understanding, hesen dai , xiaohui li...</td>\n",
              "      <td>Document Layout Understanding</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0019.pdf</td>\n",
              "      <td>LightWeight MultiModality Feature Fusion Netwo...</td>\n",
              "      <td>Understanding, Jeff yangi   , Huynh Vu Thei  X...</td>\n",
              "      <td>lightweight multimodality feature fusion netwo...</td>\n",
              "      <td>understanding, jeff yangi , huynh vu thei x an...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0034.pdf</td>\n",
              "      <td>Mining and Analyzing Statistical Information f...</td>\n",
              "      <td>,  E   — , Jose Andresl, ®   , Alejandro H, T...</td>\n",
              "      <td>mining analyzing statistical information untra...</td>\n",
              "      <td>, e — , jose andresl, ® , alejandro h, toselli...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0031.pdf</td>\n",
              "      <td>Geometricaware Control in Diffusion Model for ...</td>\n",
              "      <td>yao ,   ,  ,   , Gang</td>\n",
              "      <td>geometricaware control diffusion model handwri...</td>\n",
              "      <td>yao , , , , gang</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0078.pdf</td>\n",
              "      <td>Test Time Augmentation as a Defense Against Ad...</td>\n",
              "      <td>Yoh Yamashita and Brian Kenji Iwanaww , Gradua...</td>\n",
              "      <td>test time augmentation defense adversarial att...</td>\n",
              "      <td>yoh yamashita brian kenji iwanaww , graduate s...</td>\n",
              "      <td>Handwriting Recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0022.pdf</td>\n",
              "      <td>CREPE CoordinateAware EndtoEnd Document Parser</td>\n",
              "      <td>Yamato Okamoto ,  t  , Youngmin Baek ,   , Gee...</td>\n",
              "      <td>crepe coordinateaware endtoend document parser</td>\n",
              "      <td>yamato okamoto , , youngmin baek , , geewook k...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0080.pdf</td>\n",
              "      <td>Typographic Text Generation with OfftheShelf D...</td>\n",
              "      <td>KhayTze Peong, Seiichi Uchida , and Daichi, — ...</td>\n",
              "      <td>typographic text generation offtheshelf diffus...</td>\n",
              "      <td>khaytze peong, seiichi uchida , daichi, — , ha...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0097.pdf</td>\n",
              "      <td>Reading Order Independent Metrics for Informat...</td>\n",
              "      <td>Documents, David VillanovaAparisii   , Solene,...</td>\n",
              "      <td>reading order independent metrics information ...</td>\n",
              "      <td>documents, david villanovaaparisii , solene, t...</td>\n",
              "      <td>Handwriting Recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0047.pdf</td>\n",
              "      <td>Weakly Supervised Training for Hologram Verifi...</td>\n",
              "      <td>Glen Pouliquen , Guillaume Chironle, Joseph Ch...</td>\n",
              "      <td>weakly supervised training hologram verificati...</td>\n",
              "      <td>glen pouliquen , guillaume chironle, joseph ch...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0036.pdf</td>\n",
              "      <td>A Historical Handwritten Dataset for Ethiopic ...</td>\n",
              "      <td>Performance, Birhanu Hailu Belays   , Isabelle...</td>\n",
              "      <td>historical handwritten dataset ethiopic ocr ba...</td>\n",
              "      <td>performance, birhanu hailu belays , isabelle, ...</td>\n",
              "      <td>Datasets</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0050.pdf</td>\n",
              "      <td>GDP Generic Document Pretraining to Improve Do...</td>\n",
              "      <td>Akkshita Trivedil   , Akarsh Upadhyayl  , Rudr...</td>\n",
              "      <td>gdp generic document pretraining improve docum...</td>\n",
              "      <td>akkshita trivedil , akarsh upadhyayl , rudrabh...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77ed246d-0dd9-46ad-88f6-a0d589ce053c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-77ed246d-0dd9-46ad-88f6-a0d589ce053c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-77ed246d-0dd9-46ad-88f6-a0d589ce053c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0433ddac-1896-4360-9303-a669a46f01fb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0433ddac-1896-4360-9303-a669a46f01fb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0433ddac-1896-4360-9303-a669a46f01fb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"0043.pdf\",\n          \"0059.pdf\",\n          \"0064.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Dynamic Relation Transformer for Contextual Text Block Detection\",\n          \"GraphMLLM A Graphbased Multilevel Layout Languageindependent Model for\",\n          \"Learning to Kern Setwise Estimation of Optimal Letter Space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Jiawei Wang t, Shunchi Zhang , t, Kai Hul t,, Chixiang Ma, Zhuoyao Zhong, Lei Sun, and Qiang Huo ,   University of Science and Technology of China\",\n          \"Document Understanding, HeSen Dai , XiaoHui Lie, Fei Yin , Xudong Yana, Shuqi Mei , and, ChengLin Liu \",\n          \"Kei Nakatsuru and Seiichi Uchida\\u00b0\\u00b0\\u00b0\\u00b0 , Kyushu University, Fukuoka, Japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"dynamic relation transformer contextual text block detection\",\n          \"graphmllm graphbased multilevel layout languageindependent model\",\n          \"learning kern setwise estimation optimal letter space\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"jiawei wang t, shunchi zhang , t, kai hul t,, chixiang ma, zhuoyao zhong, lei sun, qiang huo , university science technology china\",\n          \"document understanding, hesen dai , xiaohui lie, fei yin , xudong yana, shuqi mei , and, chenglin liu\",\n          \"kei nakatsuru seiichi uchida\\u00b0\\u00b0\\u00b0\\u00b0 , kyushu university, fukuoka, japan, uchidaaitkyushuuacjp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"Key Information Extraction\",\n          \"Deep Learning Models\",\n          \"Multimodal Document Analysis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the results in a pie diagramm\n",
        "df['label'].value_counts().plot(kind='pie')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "k0xNyx_Yve0L",
        "outputId": "3608881c-dad7-4dc1-e03e-798d1b65ef8c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAp8AAAGFCAYAAAC7TZhoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACUBUlEQVR4nOzdd3hTZRvH8e9J9150U2gLBQqUWVBANkpRkKEsUUGGCxRUBBURUHHCK6KCCjJEFBQRkCmroOxZVkFWaemgpQu6R/L+UYlUVkeak7b357p6QZOT5/ySlnDnOc9QdDqdDiGEEEIIIYxAo3YAIYQQQghRfUjxKYQQQgghjEaKTyGEEEIIYTRSfAohhBBCCKOR4lMIIYQQQhiNFJ9CCCGEEMJopPgUQgghhBBGI8WnEEIIIYQwGik+hRBCCCGE0UjxKYQQQgghjEaKTyGEEEIIYTRSfAohhBBCCKOR4lMIIYQQQhiNFJ9CCCGEEMJopPgUQgghhBBGI8WnEEIIIYQwGik+hRBCCCGE0UjxKYQQQgghjEaKTyGEEEIIYTRSfAohhBBCCKOR4lMIIYQQQhiNFJ9CCCGEEMJopPgUQgghhBBGI8WnEEIIIYQwGik+hRBCCCGE0UjxKYQQQgghjEaKTyGEEEIIYTRSfApRSS1atAhnZ2e1YwghhBClIsWnECqLiYlh+PDh+Pj4YGlpSe3atRk7dizJycn6Y/z9/Zk1a5Z6IYUQQggDkeJTCBVduHCB0NBQzp49y08//cS5c+f4+uuv2bp1K23atCElJcXomfLz841+TiGEENWHFJ9CqGj06NFYWlryxx9/0LFjR2rVqkWPHj3YsmULsbGxTJo0iU6dOnHp0iVeeeUVFEVBUZRibWzatIng4GDs7e0JCwsjPj6+2P3z588nODgYa2trGjRowJw5c/T3RUVFoSgKy5cvp2PHjlhbW7N06VIuXbpEr169cHFxwc7OjkaNGrF+/XqjvCZCCCGqNnO1AwhRXaWkpLBp0yamT5+OjY1Nsfu8vLwYMmQIy5cv5+zZszRr1oxnn32WUaNGFTsuKyuLGTNmsGTJEjQaDU8++STjx49n6dKlACxdupR33nmHL7/8kubNm3PkyBFGjRqFnZ0dQ4cO1bfzxhtvMHPmTJo3b461tTWjRo0iLy+PnTt3Ymdnx6lTp7C3t6/4F0UIIUSVJ8WnECo5e/YsOp2O4ODg294fHBxMamoqhYWFmJmZ4eDggJeXV7Fj8vPz+frrr6lTpw4AY8aM4d1339XfP2XKFGbOnEm/fv0ACAgI4NSpU3zzzTfFis9x48bpjwGIjo7mscceIyQkBIDAwEDDPGkhhBDVnhSfQqhMp9OV+bG2trb6whPA29ubxMREADIzMzl//jwjRowo1mNaUFCAk5NTsXZCQ0OLff/yyy/zwgsv8Mcff9CtWzcee+wxmjRpUuacQgghxA0y5lMIldStWxdFUYiMjLzt/ZGRkbi4uODu7n7HNiwsLIp9ryiKvpjNyMgAYN68eRw9elT/deLECfbu3VvscXZ2dsW+HzlyJBcuXOCpp57i+PHjhIaG8sUXX5T6OQohhBD/JcWnECpxc3PjwQcfZM6cOWRnZxe7LyEhgaVLlzJw4EAURcHS0pLCwsJSte/p6YmPjw8XLlygbt26xb4CAgLu+Xg/Pz+ef/55Vq5cyWuvvca8efNKdX4hhBDidqT4FEJFX375Jbm5uXTv3p2dO3cSExPDxo0befDBB/H19WX69OlA0TqfO3fuJDY2lqtXr5a4/WnTpvHhhx8ye/Zs/v77b44fP87ChQv53//+d9fHjRs3jk2bNnHx4kUOHz7M9u3b7zg2VQghhCgNKT6FUFFQUBAHDx4kMDCQAQMGUKdOHZ599lk6d+7Mnj17cHV1BeDdd98lKiqKOnXq3PUy/H+NHDmS+fPns3DhQkJCQujYsSOLFi26Z89nYWEho0ePJjg4mLCwMOrVq1dsiSYhhBCirBRdeWY7CCGEEEIIUQrS8ymEEEIIIYxGik8hhBBCCGE0UnwKIYQQQgijkeJTCCGEEEIYjRSfQgghhBDCaKT4FEIIIYQQRiN7uwshjO56Tj6pmfkkZ+aSmpVHSmY+aVl5ZOUVkl+oJa9QS36BjgKttuj7f/6uAFbmZlhZaLAy12Blboa1xb9/2lub42ZnhZu9JTXsrXCzs8TcTD5jCyGEKZHiUwhhUPmFWmJTs4lOyfr3KzmLmNQskq7nkpaVT16h1ihZFAWcbCxws7PEzd4KHydrarvZ4V/DltpudgS42eFiZ2mULEIIIYrIIvNCiDLJK9ByLjGDyPhrRMZf48yV61y8mkl8eg6F2srztuJkY4G/W1ExWs/Tnka+ToT4OlHD3krtaEIIUSVJ8SmEuCetVsep+Gscu5zO8dg0jsem83dChtF6MNXg7WRN438K0ca+jjT2dcLDwVrtWEIIUelJ8SmEuEWhVsfJuHT2XUhh74VkDkSlcC2nQO1YqvN1tuG+QFfuD3Dj/kA3arnZqh1JCCEqHSk+hRAAnIhNZ9e5q+y9kMzBqFSu50qxeS++zja0q+tGu7o1eKBuDdzkUr0QQtyTFJ9CVFP5hVr2nE9m86krbIm8Qnx6jtqRKjVFgSa+TjzUyIvujbyo62GvdiQhhDBJUnwKUY2kZ+cTfiaRzaeusONMkvRuVqA67nZ0/6cQbernrHYcIYQwGVJ8ClHF5eQXsvnUFVYevsxf566SXyj/5I3N28ma7o286NPcl2ZSiAohqjkpPoWoog5EpbDy8GXWHovnukwWMhn1PO15vGVN+javibuDjBEVQlQ/UnwKUYXEpGTx6+HL/HYklkvJWWrHEXdhrlHoWM+d/qE16RrsiYXsxCSEqCak+BSiCtj5dxKLdkex/Uwi8i+68nG1s6Rfc1+GtvXHz1WWbxJCVG1SfApRSWXnFfLr4css2h3FucQMteMIAzDTKDzU0JMRDwQQ6u+qdhwhhKgQUnwKUcnEpmXz/e4olh2IIT07X+04ooI0renE8AcCeCTEG3O5JC+EqEKk+BSikjiXeJ3ZW8+x7nh8pdo7XZSPj5M1Q9v68+T9tbGzMlc7jhBClJsUn0KYuHOJ1/l86znWHYtDas7qy9XOkmc7BDK0jT82lmZqxxFCiDKT4lMIEyVFp7idGvZWPN8xkCfvr421hRShQojKR4pPIUzMucQMPt96VopOcVceDlaM7lyXQa39sDKXIlQIUXlI8SmEiUjNzOOzLX/z475oCqTqFCXk42TNaw/Vp18LXxRFUTuOEELckxSfQqgsv1DL93suMXvrWZm9LsqsmZ8zUx9tJNt3CiFMnhSfQqhoa+QVpq+P5EJSptpRRBWgKNCveU0m9qiPh4O12nGEEOK2pPgUQgVnr1zn3bWn+PPsVbWjiCrI3sqc0Z3rMuKBACzNZY1QIYRpkeJTCCPKK9Dy5bazzN1xnvxC+acnKpa/my3v9WlM+yB3taMIIYSeFJ9CGMmhS6m88esxzspWmMLIBoTW5O2eDXG0tlA7ihBCSPEpREXLyivgk41n+H5PlCydJFTj5WjN9L6N6RrsqXYUIUQ1J8WnEBVox99JvLXyOLFp2WpHEQKA3s18mNqrES52lmpHEUJUU1J8ClEBMnMLmPb7SX4+eFntKELcooa9Fe/1bkSPEG+1owghqiEpPoUwsKMxaYxbdoSo5Cy1owhxVwNCazLt0cayV7wQwqik+BTCQLRaHXN3nOezzX/LDkWi0gjysOfLJ1pQ38tB7ShCiGpCik8hDCDpei6vLD/KX+dk3U5R+VhbaJjcsyFD7qutdhQhRDUgxacQ5fTX2auMW36Uqxm5akcRolweaeLNh/1CZEkmIUSFkuJTiHL4avs5Zv5xRpZQElWGn6sNXz3RgiY1ndWOIoSooqT4FKIMsvMKeX1FBGuPxasdRQiDszLX8GG/EPq1qKl2FCFEFSTFpxClFJeWzajvD3Iy7praUYSoUCMfCODNh4Mx0yhqRxFCVCFSfApRCvsvpvDi0kNczchTO4oQRtE+qAZfDWkh40CFEAYjxacQJbR03yWmrjlJfqH8kxHVS10PexYMbUUtN1u1owghqgApPoW4B51Ox/vrIvnur4tqRxFCNa52lnz9ZEtaB7iqHUUIUclJ8SnEXeQXann9lwhWHY1TO4oQqrM01/DF4OZ0b+SldhQhRCUmxacQd5CVV8ALPxxmx99JakcRwmSYaRQ+7BfCgFA/taMIISopKT6FuI3UzDyeWXSAozFpakcRwuQoCrzVI5hRHQLVjiKEqISk+BTiP2LTsnn6u32cT8pUO4oQJu2FTnWYGNZA7RhCiEpGik8hbnIuMYOnvttHfHqO2lGEqBQGt67F9D6N0chaoEKIEpLiU4h/nE/KYNC3e0m6Lnu0C1EaPZt48/mg5rIYvRCiRDRqBxDCFERdzeSJeVJ4ClEWa4/F89rPR9FqpS9DCHFvUnyKai86OYvB8/Zy5ZoUnkKU1aqjcbyx8hhyMU0IcS9SfIpqLSalqPCUMZ5ClN/PBy8zefUJtWMIIUycFJ+i2opNy+aJ+XuJTctWO4oQVcYPe6N59/dTascQQpgwKT5FtZR0PZcn5u0lJkUKTyEMbcGui3y88bTaMYQQJkqKT1HtZOYWMHzRAS4lZ6kdRYgqa274eeaEn1M7hhDCBEnxKaqVgkIto388zPHYdLWjCFHlfbrpDGsi4tSOIYQwMVJ8imrl7VUnCD8je7ULYQw6HYz/JYIDUSlqRxFCmBApPkW18fmWsyw7EKN2DCGqlbwCLc9+f5ALSRlqRxFCmAgpPkW18MvBGD7b8rfaMYSollKz8nlm0QFSMvPUjiKEMAFSfIoqb9e5q7y58rjaMYSo1i4lZzFy8QFy8gvVjiKEUJkUn6JKu5yaxUs/HaFAtv0TQnWHo9MY/0uE2jGEECqT4lNUWTn5hbzww2G51CeECVl7LJ75f15QO4YQQkVSfIoq6+1VJ2RJJSFM0EcbTrP/osyAF6K6kuJTVElL9l5ixaHLascQQtxGgVbHmB8Pk3g9R+0oQggVSPEpqpxDl1J5T/aWFsKkJV7PZcyPRygo1KodRQhhZFJ8iiol6XouLy49RJ78hyaEydt/MUX2gBeiGpLiU1QZOp2O136J4Mq1XLWjCCFKaN6fF9lwPF7tGEIII5LiU1QZi3dHsfNv2TpTiMrmjZXHSUiX8Z9CVBdSfIoq4eyV63wkl++EqJTSs/N5fUUEOp2sxytEdSDFp6j08gq0jFt+lJx8GecpRGX159mrfL/nktoxhBBGIMWnqPRmbj7DybhrascQQpTThxsiOZ+UoXYMIUQFk+JTVGp7LyQzb6fsliJEVZCTr+XV5Udl+SUhqjgpPkWllZFbwGs/RyDbtgtRdURcTufL7efUjiGEqEBSfIpKa8amM8SmZasdQwhhYF9uO8cJ2RpXiCpLik9RKR2/nM73e6LUjiGEqAAFWh2TfjuOVi5rCFElSfEpKp1CrY43fzsml9uFqMIiLqezdH+02jGEEBVAik9R6SzaHcWJWJndLkRV9+nG0yRdlx3LhKhqpPgUlUp8ejb/++OM2jGEEEZwLaeAD9ZHqh1DCGFgUnyKSmXK6pNk5hWqHUMIYSS/HYll9/mrascQQhiQFJ+i0tgaeYU/Tl1RO4YQwsgmrzpBXoGs/SlEVSHFp6gUCgq1TJfLb0JUS+eTMvnur4tqxxBCGIgUn6JS+GHvJS4kZaodQwihkjnh50jLylM7hhDCAKT4FCYvPTufz7eeVTuGEEJF13MK+HKb7HwkRFUgxacweXPCz5Gala92DCGEyr7fe4nLqVlqxxBClJMUn8Kkxadns2hXlNoxhBAmIK9Ay//++FvtGEKIcpLiU5i0zzb/Ta7MchVC/GPV0Vgi42WTCSEqMyk+hck6l5jBr4dj1Y4hhDAhWh18vPG02jGEEOUgxacwWXO2n6NQNnAXQvxH+Jkk9l5IVjuGEKKMpPgUJikmJYs1EXFqxxBCmKjZsgKGEJWWFJ/CJM3dcZ4C6fUUQtzB7vPJHI5OVTuGEKIMpPgUJufKtRxWHLqsdgwhhIn7Stb9FKJSkuJTmJxvd16QfZyFEPe09XSizHwXohKS4lOYlJTMPH7cF612DCFEJfHtzgtqRxBClJIUn8KkLPjrItn5hWrHEEJUEr9HxBGblq12DCFEKUjxKUxGTn4hS/ddUjuGEKISKdDq+O7Pi2rHEEKUghSfwmSsPRYve7gLIUrtl4MxZOUVqB1DCFFCUnwKk7Fkr/R6CiFK73puAWuOyrrAQlQWUnwKk3D8cjoRMWlqxxBCVFJLZaKiEJWGFJ/CJCzZG6V2BCFEJXY8Np3jl9PVjiGEKAEpPoXq0rPzZStNIUS5yYRFISoHKT6F6n45GENOviwqL4QonzURcVzPkUmLQpg6KT6F6n7aL2O1hBDll5VXyKojsWrHEELcgxSfQlXHL6dzPilT7RhCiCrix/0xakcQQtyDFJ9CVauPSi+FEMJwIuOvcfbKdbVjCCHuQopPoRqtVsfaY/FqxxBCVDG/y/uKECZNik+hmr0Xk0m4lqN2DCFEFbP2mKyeIYQpk+JTqOZ3WV5JCFEBLiRlcjJO1vwUwlRJ8SlUkVegZcOJBLVjCCGqqN8j5NK7EKZKik+hih1/J5GWJevxCSEqxrrjcmVFCFMlxadQxYYT0ishhKg4MSnZHIlOVTuGEOI2pPgURqfT6dhxJkntGEKIKm7jSRnaI4QpkuJTGF3E5XSSM/PUjiGEqOLkQ64QpkmKT2F0204nqh1BCFENnE64TkK6LOcmhKmR4lMYXfgZKT6FEMax4295vxHC1EjxKYwq6Xoux2Nl/T0hhHHs+FsuvQthaqT4FEYVfiYRnU7tFEKI6uKvs1cp1MqbjhCmRIpPYVThMgFACGFE13IKZMklIUyMFJ/CqPZeSFY7ghCimpFL70KYFik+hdFcSMqQJZaEEEa357x86BXClEjxKYzmYJRc+hJCGN/x2HTyC7VqxxBC/EOKT2E0By+lqB1BCFEN5RZoORl3Te0YQoh/SPEpjObgJen5FEKo47C8/whhMqT4FEaRkpnHhaRMtWMIIaqpQzLjXQiTIcWnMIqDUXLJXQihniPS8ymEyZDiUxjFIXnjF0KoKC49R/Z5F8JESPEpjEIG+wsh1HZYLr0LYRKk+BRGcTrhutoRhBDV3Mm4dLUjCCGQ4lMYQUpmHlczctWOIYSo5v6+kqF2BCEEUnwKIzidIJfchRDqO5coxacQpkCKT1HhzsgldyGECYhOySInv1DtGEJUe1J8igonxacQwhQUanWy3rAQJkCKT1HhzlyR4lMIYRrOJsr7kRBqk+JTVLizMshfCGEi/pYPw0KoTopPUaGSrueSkVugdgwhhADkw7AQpkCKT1Gh4tKy1Y4ghBB60SlZakcQotqT4lNUKCk+hRCmJF622BRCdVJ8igoVK8WnEMKEpGfnk50nyy0JoSYpPk1Ely5dSEtLu+X2a9eu0aVLF+MHMhApPoUQpiYuXd6XhFCTFJ8mIjw8nLy8vFtuz8nJ4c8//1QhkWHIZXchhKlJkEvvQqjKXO0A1d2xY8f0fz916hQJCQn67wsLC9m4cSO+vr5qRDOIuDR5kxdCmBb5UCyEuqT4VFmzZs1QFAVFUW57ed3GxoYvvvhChWSGIW/yQghTIz2fQqhLik+VXbx4EZ1OR2BgIPv378fd3V1/n6WlJR4eHpiZmamYsOwKtTpSsm4dSiCEEGqKvybFpxBqkuJTZbVr1wZAq9WqnMTw0rPz0enUTiGEEMUlZ+SqHUGIak2KTxNy9uxZtm/fTmJi4i3F6DvvvKNSqrJLk15PIYQJkl3XhFCXFJ8mYt68ebzwwgvUqFEDLy8vFEXR36coSuUsPrPz1Y4ghBC3yMiR4lMINUnxaSLef/99pk+fzsSJE9WOYjDXpPgUQpig69LzKYSqZJ1PE5Gamkr//v3VjmFQcmlLCGGKpOdTCHVJ8Wki+vfvzx9//KF2DIOSN3ghhCmSD8ZCqEsuu5uIunXrMnnyZPbu3UtISAgWFhbF7n/55ZdVSlZ28gYvhDBFWXmFFGp1mGmUex8shDA4RaeTxXBMQUBAwB3vUxSFCxcuGDGNYXy1/RyfbjqjdgwhhLhFxDsP4WRrce8DhRAGJz2fJuLixYtqRzA4+VwjhDBVOQWFOCHFpxBqkDGfosJopfYUQpgorXw4FkI10vNpIoYPH37X+xcsWGCkJIYjb+5CCFNVKJ+OhVCNFJ8mIjU1tdj3+fn5nDhxgrS0NLp06aJSqvKR93YhhKmSz8ZCqEeKTxPx22+/3XKbVqvlhRdeoE6dOiokKj8Z8ykMoZFDJhO9D5FEHdwdXci3TSXP8goWltcwN0tBIRmQDQ1E6ThbtQBs1Y4hRLUkxacJ02g0vPrqq3Tq1IkJEyaoHafU5LK7KCsLjY6Xal5gsPl2asTvYJtVW7RRyaQndsLH0pkauQ256pBNgs114nISydEm4eJSiKNTPnZ22VhaXkOjSUWrTSI//wo6nSz7JYoz12jVjiBEtSXFp4k7f/48BQWV8z9OuewuSquFUwYTPfYTmroOs8R4/e3f2ZoR7JjOIwcj+LtOKw7GzOOBkAE0jfKkab47+dZarprlEZ+bzpnMKySmJAE1AVAUHa6uGtzcdDg65mFrm42F5XU0muSi4jQvER2FKj1joRZFkfm2QqhFik8T8eqrrxb7XqfTER8fz7p16xg6dKhKqcrHXBZwFiVgpdHyit85+mu24ZrwF0pM8R6pff6hHL92gRQ7Rx7fdwLnOp3R+vRm2+HF2Nu40L7hIKzjzfGOtsYba1rgSZ6tlkSPXOIt0rh8PYHk5BSSkwEs//ly4kZxqtGAm5uCq6sOB8dcbG2Kek4VTQqFhYnk5ycB0ktW1UjxKYR6ZJF5E9G5c+di32s0Gtzd3enSpQvDhw/H3LzyfU6YE36OTzbKIvPi9tq4pPO6+z6aJq/HLDPxjsc92/xB9qQV/R79ssCVgnyF/W3fxdI6lfT4ZeRkXKe2XxNa+z2CJv72RWK2g5akGtnEm6URfS2e9GvpJc5pbl5UnLq4aHF0zMfGJhMLy2soSjKFhVfIz08G5G20snnggX1YWdZQO4YQ1ZIUn6LCLN4dxZQ1J9WOIUyInZmW12qdoZ9uK04Je1DuUbSd9GnMIKtr+u+/D2+I9Z5jXHtoOAfzWmLvmkVW8s9kpaUA0Di4E43s28HVuw9VyXQqJLFGNnFKCpdT47memVHm52RuDu7uGlxcCnFwyMXaJgsLi/R/itPEf4pTYWo6dojA3Ny+zI8fNmwYixcvBsDc3BxXV1eaNGnC4MGDGTZsGBpNyXpWFy1axLhx40hLSytzlrIYNmwYaWlprFq1yqjnFQLksrvJSUpK4syZol6e+vXr4+7urnKisrO1NFM7gjARHd1Sec11L42vrkcTX/Ji7DsvP0j99wNMgo81/oDjHwvwfaIlsXG22DkPwtz8V65dvcKJyHBOKDto07wftQsboLt2+yLULt2MgHR7ArAHanHdtZBE1yzidMnEpMSTlZ1V4owFBRAfryU+XgGs//lyBYq2zLW0hBo1inpO7R1ysLEuKk5RkikouEJBQepdWhcVQ8HMzK7crYSFhbFw4UIKCwu5cuUKGzduZOzYsaxYsYI1a9ZUyitWQhiDDHoxEZmZmQwfPhxvb286dOhAhw4d8PHxYcSIEWRllfw/QlNiZyVvvNWZg3kB7wee5Fitz1icOZomMUvQZJe88Ixyr8PWtMhit0W65er/HvjXbCysNGSmW6Ox7o+zV9EYTnQ69hz+lV9PzSTZMwnF+t4fghxSzKhzzoH25/0ZnHY//R060qFmKHV9ArCysipx5tvJy4O4OB0nTyrs22tDeLgbmzcHsvmPVmzf1pMD+5/mwoWnSU0ZQn7+45ib9cDaqg1WVkGYmTmW69zi9szMbFGU8o9Jt7KywsvLC19fX1q0aMFbb73F6tWr2bBhA4sWLQLgf//7HyEhIdjZ2eHn58eLL75IRkZRT3t4eDjPPPMM6enpKIqCoihMnToVgCVLlhAaGoqDgwNeXl488cQTJCb+OzwlNTWVIUOG4O7ujo2NDUFBQSxcuFB/f0xMDAMGDMDZ2RlXV1d69+5NVFQUAFOnTmXx4sWsXr1af97w8HDy8vIYM2YM3t7eWFtbU7t2bT788MNyv05C/JdUBybi1VdfZceOHfz++++0a9cOgL/++ouXX36Z1157jblz56qcsPSk57N6eqhGCuNcdtEgaQOauLQyt7PQrwHa1OPFbtvvkESPf/5uEX2G4BYXOJbrT06mJRY2fXGruY7kyxcAyM/PYcveBTg41KBj0yewS7CFwnuPMlJ0Ck5J5jglOVEPJ7SaANLdC0hwzCAu/yqXk+PJzzfcuqI5OTpiL0PsZQ1g889XDaAuADa2CjVqgLNzIfb2OVhbZWBung5cJb8ggcLCsg8ZqK7Mzcp+uf1eunTpQtOmTVm5ciUjR45Eo9Ewe/ZsAgICuHDhAi+++CITJkxgzpw5tG3bllmzZvHOO+/or3jZ2xdly8/P57333qN+/fokJiby6quvMmzYMNavXw/A5MmTOXXqFBs2bKBGjRqcO3eO7Oxs/WO7d+9OmzZt+PPPPzE3N+f9998nLCyMY8eOMX78eCIjI7l27Zq+YHV1dWX27NmsWbOGn3/+mVq1ahETE0NMTEyFvVai+pIxnyaiRo0arFixgk6dOhW7ffv27QwYMICkpCR1gpXDgagU+n+9R+0YwghcLAp4w+8Uj+Rvwj7pSLnbu+LkQ48aNuRrby3yfpljiy69aByoTmPG8cfncDWx6PK6uWUhtjZ/kBgVecvjvDzr0LbuY1jEK+WaH6Q105HqkU+C/XUu5yURdzWBwkL1lmqysysqTp2cC7C3y8HaOhMz81R0uqsU5CdQqK2cV04qkq1tHdrc/0e52rjbmMlBgwZx7NgxTp06dct9K1as4Pnnn+fq1atAycd8Hjx4kFatWnH9+nXs7e159NFHqVGjxm23Xv7hhx94//33iYyM1Pfw5uXl4ezszKpVq3jooYdum//ll1/m5MmTbNmyxSA9w0LcifR8moisrCw8PT1vud3Dw6PyXna3lF+vqq6XRxJjnP6iXuJGlLjrBmv3+8AW5Kcdu+19+YG+mB8pKj4VbSH1Ty0hxeMJtFodBXlmZGi741XXkoRzEcUel3DlPCuvfEJQnfto4dENrpRt/VxNoYJbvCVuuNEINwot6pPskUeC7TUu5ySRkHwFrdZ4SzNlZurIzAQumQF2/3x5APUBcHBQcHPT4exciJ1dNlbWGZiZpaHTFS3Ar9VmGy2rqbCwcKrQ9nU6nb5427JlCx9++CGnT5/m2rVrFBQUkJOTQ1ZWFra2d95h6dChQ0ydOpWIiAhSU1P1v1PR0dE0bNiQF154gccee4zDhw/z0EMP0adPH9q2bQtAREQE586dw8HBoVibOTk5nD9//o7nHDZsGA8++CD169cnLCyMnj178tBDD5X35RDiFlIdmIg2bdowZcoUvv/+e6ytrQHIzs5m2rRptGnTRuV0ZeNoI79eVZGHVT5v1DxBWO5GbK8eh2v3fkxppNu6sCLj3B3vT/JzwPumzlWrE39Rb2AYp6+4AKAt0JCe0gXfBlbEnt5/y+PPnt/H2fP7aBHSg3oWLdGllu8Sulm+gkesFR640wR38q2CSfbMJd7qGpeziha8V/MC0/XrOq5fBzAD7P/58gIaAODkVLQAv5NTPrZ2OVhZXcfMLBWdNon8gitotbl3bLuysrBwqdD2IyMjCQgIICoqip49e/LCCy8wffp0XF1d+euvvxgxYgR5eXl3LD4zMzPp3r073bt3Z+nSpbi7uxMdHU337t3Jy8sDoEePHly6dIn169ezefNmunbtyujRo5kxYwYZGRm0bNmSpUuX3tL23SaxtmjRgosXL7Jhwwa2bNnCgAED6NatGytWrDDMCyPEP6Q6MBGzZs0iLCyMmjVr0rRpU6Do06uVlRV//FG+y0Nq8XCwRlFABnZUDY97XeEF+z8JTNyEEptZYef5MagNWddO3PH+s+4FeP/nNq81HxEX9j+u/VNI6nQKyVfaUbORNZdP7rxtO4ePb+Co5g8eaDkQn+wAdJmG2UnMIlfBK9oaL6xpjgd51jqSPHNIsEzjckYiSalXDXIeQ0lP15KeDkX/HdwoTm+8wjqcnf/ZHcopH3u7nKKtS81S0Wqvkp+fgE5nuPGvxmJh7lxhbW/bto3jx4/zyiuvcOjQIbRaLTNnztQvvfTzzz8XO97S0vKWYRunT58mOTmZjz76CD8/P6Dosvt/ubu7M3ToUIYOHUr79u15/fXXmTFjBi1atGD58uV4eHjg6Hj7SWu3Oy+Ao6MjAwcOZODAgTz++OOEhYWRkpKCq6trmV4PIW5Hik8TERISwtmzZ1m6dCmnT58GYPDgwQwZMgQbGxuV05WNpbkGF1tLUjLz1I4iysjbOo9JNSPolrUR65RISKvY82Vb2vJTzt0nOBxyTKHDf27TZGfQMHEDey263XSrwtW4UGqFWBF9fPNt29JqC9l54EesbRzp1PwJnK+6oMs37CVzyxwF30s2+GJDS7zJsdOS5J5DvEUqMdeukJpuykstKaSl6Sgajmjxz5cD4Ft0rwIuLgpubhRtXWpXtDuURpOKVptIfn4iOp3pbQ9sYeFskHZyc3NJSEgottTShx9+SM+ePXn66ac5ceIE+fn5fPHFF/Tq1Ytdu3bx9ddfF2vD39+fjIwMtm7dStOmTbG1taVWrVpYWlryxRdf8Pzzz3PixAnee++9Yo975513aNmyJY0aNSI3N5e1a9cSHBwMwJAhQ/j000/p3bs37777LjVr1uTSpUusXLmSCRMmULNmTfz9/dm0aRNnzpzBzc0NJycnvvjiC7y9vWnevDkajYZffvkFLy8vnJ0N83oJcYMUnybiww8/xNPTk1GjRhW7fcGCBSQlJTFx4kSVkpWPh4OVFJ+V0BDvOEbZ7aR2wmaUy8YbE/hr/fakZtw6WehmB6ziUKyt0eXkFLvddtdvBDzRgYtxlsVuT7wcQq0mVkQfW3vHNnOyr7Fx99e4uvrSvuFArOMtKmxHTetMDX6ZtvhhS2t8yXbUklgjizhNKjHpCVy7buBxDBVIp4OUFB0pKfBvcerIja1LFUWHm5sGV1cdjo552NhmYWl5/Z/i9Ar5eUnoMP5kLUtLN4O0s3HjRry9vTE3N8fFxYWmTZsye/Zshg4dikajoWnTpvzvf//j448/5s0336RDhw58+OGHPP300/o22rZty/PPP8/AgQNJTk5mypQpTJ06lUWLFvHWW28xe/ZsWrRowYwZM3j00Udveg6WvPnmm0RFRWFjY0P79u1ZtmwZALa2tuzcuZOJEyfSr18/rl+/jq+vL127dtX3hI4aNYrw8HBCQ0PJyMhg+/btODg48Mknn3D27FnMzMxo1aoV69evL/GC+UKUlMx2NxH+/v78+OOP+gHjN+zbt49BgwZx8eJFlZKVz9AF+9nxd+WbqV8d1bLJYZLvUTplbsQq9W+jnz9fY8HDDZqQkH3v35flq/xRIm8dF1ro5s2++6eSc5tL6B5+UUQf/61E40Bq1WzMfbV63XG7zoqU4fLPgvekEJMaR2YlnXBYEhpN0dalrq5aHBzzsLXNxtLiGoom5Z+tS69SEZ8CGjWahZdnL4O3K4QoGen5NBEJCQl4e/93JFvRmJ74+HgVEhmGl6O12hHEPQz3jWG4zU5847egXFZvcsm6+h1IyD5bomNTajnjdpsOUrPkeBoVHuIQTW+5LzHGH7+QAcSeXIH2HksjRV8+QfTlEzRq0IkQhwfQXTXeuEb7VDPsUx0IxAGozTW3Qq64ZBCvTSE6JY6c//T4VmZaLSQl6UhKUgCrf76cgVpA0dalNWrc2Lo0DxubLCws01GUlH+2Lr1KWdbNsra69b1WCGE8UnyaCD8/P3bt2kVAQECx23ft2oWPj49KqcrP07F8u8OIihFkl82bPodpf20DFskX1I6DDoWF5iW/vB/lqXCnC6dOG7/FZ/Ac4uJvLUqSYnzxCR5M/JnlFJZgofiTp8M5qezg/mZ98dcG33G7zorkmGyGY7ITQTihU/xJdy/gimMmsYXJXE6O089+rooKCiAhQUtCws3FqQvgD4CFxb9blzo45GJtk4mFxTUU/dalKbdt19q68r6nClEVSPFpIkaNGsW4cePIz8+nS5cuAGzdupUJEybw2muvqZyu7Dyk59NkKIqO52te4mnLHXjFb0OJMZ1ZytuCHuBCxqUSH3/UJZ2Wd7m/zp6vSKw3hoK8Wy/ZXo31wDNoCEkXlpFfkl5EnY69R1ZyyMKaDi2fwD3dC12OOovKKzoF50QLnBOdqY8zWk0gaZ4FJNhfJzb/KrFX4ykoML0JPhUlPx/i43XExyuA9T9fbkDRh3grq6LJUDd6Tq1tMrCwuI6V1a1rKgshjEeKTxPx+uuvk5yczIsvvqjvybC2tmbixIm8+eabKqcrO1/nyjlTvypp5JDJG16HaJO+HvOkaLXj3NZ3tmalWi90t3UsI8zM4A6Xzy2iThLcIprjKTVve39KvCtutYeQenk5uZkl254yPz+HrTe262zyBHZXSrZdZ0XSaBVc4y1wxZWGuFJoHkSKRz4JdteIzS3afcmYC96bmtxcHXFxEBen4UZx6uISRLu2svWvEGqSCUcmJiMjg8jISGxsbAgKCsLKqnJfto5OzqLDp9vVjlHtmClaXvKL4gnzbbjH70DRqbf9473s8w9lpJJY6sf9sswH3cU7F9M6M3OO9fuK5KQ79wQ6umWQkfgzWdfSSn1+T486tKv3GBZx5duusyIVWOpI9sglweY6l7MTSUi+ouqC96YgMDCw2GxzIYTxSfEpKpRWqyP4nY3kFlTf3hdjauaYwRueBwhNW4/59Vi145TIs80fZE/amVI/7ru9IThsv/s+8rlNO7HbrT+6u/z62Tllk3ttBRkpZVuVoW5AK1p4PohyxXQL/BvyrbVc9cgj3iqdy5lFuy9VN6GhofTs2VPtGEJUa3LZXVQojUYhoIYdpxMMt++3KM5Ko2Ws33kGarbhmvAnSkzlKfRP+jQuU+EJEONlQcN7HGMVEU69Qd05k3D7XV4AMtNtsHEYgJP5b6QnxpU6x7mLBzh38QDNG4dR3zK03Nt1ViSLHA3e0dZ4Y00LPMmz1ZLkkUucRRqXryeQnHb7CTpViezUI4T6pPgUFa6Oh70UnxWgtfM1Jnjsp3nyOsyuXFE7Tpl85+UHqSfL9NgTrpn3LD4BvNd+RGzXT8lIv3NRmH3dCivbx3D1/Z2U2Kgy5TlyYiMRms0G366zIllmafCNKtp9qRXeZDtoSaqRTbxZGtHX4km/lq52RIOT4lMI9cm2BaLC1XW3VztClWFjVsjb/mc44v8Vy3NeIDR6AWaZlbPwjHKvw9a0u+9mdDd/2ZVsWIEmI51GKbffXvNmuVkW5Bf2xr12vTJnurFd56pzn5PmnYZiUbneYm2ua6h10Y77zvnSPzGUJ6w60dXnPoJrBuFgVzX+HXt6Vo+Z7lOnTsXT0xNFUVi1apXacUqssuVV09SpU2nWrJnaMcqkcr0zikqprkfV+E9LTR1c01gdtIGTjq8wMmEaLgm7UEx1lksJLfRrgPZugzHvIcEsA8XLo0TH2v35C/4+9+6JzM81Iyu7B551QsqcCyAnJ4NNu7/hj6TF5PjmV9p3Wtt0MwIu2NPuXC0GJt/HILtOdPZtRX3fOtja2Kodr9SsrKzKtU/5sGHD6NOnT7HbVqxYgbW1NTNnzixfuDsIDw9HURTS0tJK/JjIyEimTZvGN998Q3x8PD169KiQbOVxp8LJGHkXLVqEoii3fFlbl3xpQGMXfrcrysePH8/WrVuNlsGQ5LK7qHB1pOezTOzMC5nod5o+2i04XtkHVWiXxStOPvyefrrc7WQHeGGdULKZ8rW3zCQ+9C1ys+4+Magw34xr6V3xqW9F3JmD5cqXkhrH6r/+h59vI+6r3Quz2yx8X5nYJ5thn+xIHRzRKbW5XqOQBKdM4rTJxCTHkZur3g5ZJeHh4YGiKAZrb/78+YwePZqvv/6aZ555xmDtltf58+cB6N27d7meb35+PhYWFoaKVSJeXl5GOY+joyNnzhQfb27I340bKvI1tLe3x96+cv7/Wkk/j4vKJNDdDgszw/+jrqoerJHCuqC1HLd/mafjpxcVnlXM94HNydeWf2JOvG/JeyrMEqNpxLESHasr1JCS1J6aDduWNVoxMbEnWbH7I45b7oEaVeMzv6JTcEwyp945JzpdCGTI9XY85tSBdn4tCPCqZfSipSQMecn9k08+4aWXXmLZsmXFCs/Vq1fTokULrK2tCQwMZNq0afqF/4cPH37LTPv8/Hw8PDz47rvvSnTeRYsW4ezszKZNmwgODsbe3p6wsDD9NsxTp06lV6+ifes1Go2+oNJqtbz77rvUrFkTKysrmjVrxsaNG/XtRkVFoSgKy5cvp2PHjlhbW7N06VJ9b+8HH3yAp6cnzs7OvPvuuxQUFPD666/j6upKzZo1WbhwYbGcEydOpF69etja2hIYGMjkyZPJ/2dXsUWLFjFt2jQiIiL0vY6LFi0Cbu3hO378OF26dMHGxgY3NzeeffZZMjL+XZv3Rr4ZM2bg7e2Nm5sbo0eP1p/rThRFwcvLq9jXjd+PpKQkvLy8+OCDD/TH7969G0tLS7Zu3XrP/HPnzuXRRx/Fzs6O6dOnU1hYyIgRIwgICMDGxob69evz+eef35JpwYIFNGrUCCsrK7y9vRkzZgwA/v7+APTt2xdFUfTf/7f3taQ/45UrV9K5c2dsbW1p2rQpe/bsuetrVRGqxrugMGnWFmY08HLkeGzVm7xgKC4WBUz0i6RnwR/YJx6Ckq17Ximl27qwIuO8QdqKrJFLwL0P03NePwfvwXOJjy/B5X6dQlL8ffg1tibmxLYyZ7zZqTM7OaX8yX3N+hCga4gu3fQnJZWURqvgcsUClysuBOOC1qwuqR75JNhf53Je0YL3hXfYFMBYvL0Ns6f7xIkTmTNnDmvXrqVr16762//880+efvppZs+eTfv27Tl//jzPPvssAFOmTGHkyJF06NCB+Ph4fZa1a9eSlZXFwIEDS3z+rKwsZsyYwZIlS9BoNDz55JOMHz+epUuXMn78ePz9/XnmmWf0BSnA559/zsyZM/nmm29o3rw5CxYs4NFHH+XkyZMEBQXpj3vjjTeYOXMmzZs3x9ramvDwcLZt20bNmjXZuXMnu3btYsSIEezevZsOHTqwb98+li9fznPPPceDDz5IzZpFGzs4ODiwaNEifHx8OH78OKNGjcLBwYEJEyYwcOBATpw4wcaNG9myZQsATk5OtzzPzMxMunfvTps2bThw4ACJiYmMHDmSMWPG6Is9gO3bt+Pt7c327ds5d+4cAwcOpFmzZowaNarEr+nN3N3dWbBgAX369OGhhx6ifv36PPXUU4wZM4auXbuSnZ191/xTp07lo48+YtasWZibm6PVaqlZsya//PILbm5u7N69m2effRZvb28GDBgAwNy5c3n11Vf56KOP6NGjB+np6ezatQuAAwcO4OHhwcKFCwkLC8PM7PabJJT0Zzxp0iRmzJhBUFAQkyZNYvDgwZw7dw5zc+OVhLLOpzCKyatOsGRvybdPrC4edr/KWOdd1EvcgJJbii1+KrG5TR9mzrUTBmmrQX4N3p2RUKrH5NVpwp7AFyjML/l4U4+ap4g+vvHeB5aCubklHUOH4J7urdp2ncZUaKEj2SOPBNtrXM5JIiH5itF3X3r22Wfx8Sn7vu7Dhg3jp59+Ii8vj61bt+q3Qr6hW7dudO3atdiudD/88AMTJkwgLq5oGa9GjRoxdOhQJkyYAMCjjz6Km5vbLT2HN4SHh9O5c2dSU1NxdnZm0aJFPPPMM5w7d446deoAMGfOHN59910SEor+LaxatYq+ffsW21DA19eX0aNH89Zbb+lva926Na1ateKrr74iKiqKgIAAZs2axdixY4s95/DwcC5cuIBGU3SxtEGDBnh4eLBz504ACgsLcXJyYv78+QwaNOi2z2PGjBksW7aMgweLhrJMnTqVVatWcfTo0WLHKYrCb7/9Rp8+fZg3bx4TJ04kJiYGOzs7ANavX0+vXr2Ii4vD09NTn+/8+fP6omzAgAFoNBqWLVt22yw3XsMbbd7Qvn17NmzYoP9+9OjRbNmyhdDQUI4fP86BAwf0G7/cLf+4ceP47LPPbnvuG8aMGUNCQgIrVqwAin4+zzzzDO+///5tj7/5dbnhvxlK+jOeP38+I0aMAODUqVM0atSIyMhIGjRocNfMhiQ9n8Iomvk5S/H5D3fLfN70O0FY7h/YXo2AarQKVbalLT/lxBisvdMWV1GcndCllbxX3fL8MYKbxXIiueS9YImXG1KriTXRx1eDgT6vFxTksXXvQhzs3ejQ9AnsE+2goOr2BZjlK3jEWuGBO01wJ98qmGTPXOKtrnE5q2jB+4rsCzEzMzPIZfcmTZpw9epVpkyZQuvWrYuNuYuIiGDXrl1Mnz5df1thYSE5OTlkZWVha2vLyJEj+fbbb5kwYQJXrlxhw4YNbNtWup51W1tbfeEJRT26iYl3Hvt87do14uLiaNeuXbHb27VrR0RERLHbQkNDb3l8o0aN9IUnFA1faNy4sf57MzMz3NzcimVYvnw5s2fP5vz582RkZFBQUICj453X272dyMhImjZtWqxIbNeuHVqtljNnzuh/no0aNSrWG+jt7c3x48fv2raDgwOHDx8udpuNTfHtoGfMmEHjxo355ZdfOHToUIl3HLzda/jVV1+xYMECoqOjyc7OJi8vT3/JPDExkbi4uGK96KVVmp9xkyZN9H+/0QOfmJho1OJTxnwKo2hWy1ntCKrr53mFLUG/st/6RfrFflpUeFYzv9ZvT2qeYYdf5NXxLfVjavz+KS41SjcmMTEmkFohj6NoDPu2eT0jmXW7viA8/RfyfbRQTYZHW+QqeEVb0/ysB71iQ3hK15kenm1p7tcQd5caBj+fp6fnHS9Xloavry/h4eHExsYSFhbG9ev/fnrMyMhg2rRpHD16VP91/Phxzp49q59J/fTTT3PhwgX27NnDDz/8QEBAAO3bty9Vhv+Op1UUxWCF+397A+90vtvddqMne8+ePQwZMoSHH36YtWvXcuTIESZNmkReXp5BMpYk37161TUaDXXr1i325etb/L3k/PnzxMXFodVqiYqKKnGe/76Gy5YtY/z48YwYMYI//viDo0eP8swzz+hfj/8WvRXt5tfr5jHBxiQ9n8IoAmvY4WhtzrWcqjPGrSS8rfN4q+ZxumVvxCb5JFTjYa/5GgsWF141eLtJvnb4HCrdYzR5uQRf+Jk9Tn1L1ZGZGOOHb8NBxJ/+mcICw/4uX0k8z8rET6kTEEpLz4cqxXadhmSZo+B7qWjB+5Z4k2OnJck9h3iLVGKuXSE1PbVc7f+3sCiP2rVrs2PHDjp37kxYWBgbN27EwcGBFi1acObMGerWrXvHx7q5udGnTx8WLlzInj17jDJL3tHRER8fH3bt2kXHjh31t+/atYvWrVsb/Hy7d++mdu3aTJo0SX/bpUvFr3xZWlrecwxwcHAwixYtIjMzU1/Q7dq1C41GQ/369Q2e+2Z5eXk8+eSTDBw4kPr16zNy5EiOHz+Oh4dHifPfsGvXLtq2bcuLL76ov+3GigRQ1Avr7+/P1q1b6dy5823bsLCwuOv5jP0zLi8pPoVRKIpCUz9n/jxr+OLDFD3hHc+zdjupfWUzyuUqtEZSOayr34GE7LMGb/eseyFlGcVnfXgLQYMf5O/40i1VcjXWC696Q7hybhkFeYZfWuj8xYOcv3iQ5iH/bNeZYrrbdVYk60wNfpm2+GFLa3zJdtSSWCOLOE0qMekJXLteujHStWvXNmg+Pz8//XjM7t27s3HjRt555x169uxJrVq1ePzxx9FoNERERHDixIliY/lGjhxJz549KSwsZOjQoQbNdSevv/46U6ZMoU6dOjRr1oyFCxdy9OhRli5davBzBQUFER0dzbJly2jVqhXr1q3jt99+K3aMv78/Fy9e5OjRo9SsWRMHB4dbLmsPGTKEKVOmMHToUKZOnUpSUhIvvfQSTz31VLmHUOh0Ov0Y2Zt5eHig0WiYNGkS6enpzJ49G3t7e9avX8/w4cNZu3ZtifPf/Hp8//33bNq0iYCAAJYsWcKBAwcICPh3uuTUqVN5/vnn8fDwoEePHly/fp1du3bx0ksv6c+3detW2rVrh5WVFS4uLrecx5g/4/KSy+7CaJr7OasdoULVssnh67r7OOM9jQ9SX8P/8mqUfCk8AXQoLDTPrpC2Dzoll/mxPr9/iJ1j6T+DJ8e74R44BMsKXGj9yPGN/HzkQ+LcolHspZ/A5pqG2hfsaXPOjwFJrRhk04kuvq1p4FsXO9t7/xwMXXwC1KxZk/DwcK5evaqflb127Vr++OMPWrVqxf33389nn312y7m7deuGt7c33bt3L9cEqNJ4+eWXefXVV3nttdcICQlh48aNrFmzptgsaEN59NFHeeWVVxgzZgzNmjVj9+7dTJ48udgxjz32GGFhYXTu3Bl3d3d++umnW9qxtbVl06ZNpKSk0KpVKx5//HG6du3Kl19+We6M165dw9vb+5avxMREwsPDmTVrFkuWLMHR0RGNRsOSJUv4888/mTt3bonz3/Dcc8/Rr18/Bg4cyH333UdycnKxXlCAoUOHMmvWLObMmUOjRo3o2bMnZ8/++2F95syZbN68GT8/P5o3b37b8xjzZ1xeMttdGM2Ov5MYumC/2jEMbpjPZUbY7qBmwlaUghy145ikrUHtGVdQMRPOzFBYNssMXXbZXvuMTk+wn3b3PvA2HGtc53rCcrJL2QtXWtbW9nRsMQSXZFd0ecYdm1VZXHMr5IpLBvHaFKJT4sjJ+ff3wc3NTd+DZAoyMjLw9fVl4cKF9OvXT+04QhidfJwWRnNfgCuW5hryCir/f551bLN5y/cI7a+vxzLlAqSonci0fWdrBhVUnxWiozDQD83Jsl3Stw//kVpPtCE6rvQXgq5ddcDB/Qk05r+QmVr2Hth7ubFdp7OzNx0bD8I63hIq/z8jg3JMNsMx2YkgnNAp/qS7F3DFMZPYwmTca5rGfu5arZarV68yc+ZMnJ2defTRR9WOJIQqpOdTGNWQ+XvZda7i/pOuSIqi4znfaJ622oF3wjaUwoqZuVnV7PMPZaRSsi0wy2rOkWbU2Fj2rTALvP3Z2+wN8rLLNsnH1iGXguxfuZZUujVHy6qmb0Pur/1opd+u01hcB9fHtqmH2jH06yzWrFmTRYsWlWtpHSEqM+n5FEbVPsi90hWfDeyzeMv7EG3S12NxVdYqLa3vXFwgrWKLz4seUJ7Feczjo2jU8hRHKNsM2qzrVljbPY6L9xpS46PLkaRkLseeYkXsKRrW70CIU3tIql6rSJSKAlZ1nNVOARRNGpH+HiFkwpEwsvZBhl+/ryKYKVpernWBfYEL2KB9ng4xc7G4JoVnaZ3yacSetDMVfp4I5/Jf03de9wWeXmVfBzIn05JC+uDmV+feBxvIqTM7WX7gA6KczqA4SV/C7Vh42WFmb6l2DCHETaT4FEbV0NuRGvYl2yVCDU0cM/gpKJwzNSbyauLbeMZtQdFKr1JZzfeqZZTz7La9DOXcl1jR6Qg6PB8z87Kv8p6XY05ubk88AhuWK0up6HTsO7qKFSc+JcnzCopN+RdSr0qsgpzVjiCE+A8pPoVRKYrCA3Xd1I5RjJVGy/ja5zgY+C2r81+gTcy3mF+PVTtWpRflXoetaZFGOVeGkge1yr+IuOXfB2ngcqVcbRTkm5Fx7SG8g5qVO0+pzluQx7a9i1gbPZcMn0woRxFdlVgH3boeohBCXVJ8CqNrH+SudgQAQp2u80vQFk65jGfMlXeoEReOoqteu8pUpIV+DdDqjDcl+5q/YT7UeKz+GGe30m29+V/aQg1pyZ3xDb7fIJlKIyMjlXW7vmR7+vJqtV3n7WhszbEKdFY7hhDiP6T4FEbXqb47Zhp1/ke0MStkkv8ZDvvP4Zfc52kVswCzTOPMUK5Orjj58Hv6aaOeM8bLMGMelbwcgqNXlrto0+kUria0wa9RB4PkKq3ExIus3PUpB3Rb0JVjLGtlZh3shmJWjatvIUyUFJ/C6Nzsrbg/0NWo53zANZ3f6m3ipNMrjEqYhmvCXyjIrNOK8n1gc/K1xt0W8rhrhsHasjmwkbpe5d+dSkEhKS6UWiEPGiBV2Vy4eIif93zAGbujKK7l69GtbGwam9YQHyFEESk+hSoeDvGu8HPYmRcyNSCSiNqzWZL1Is2jF6PJqh57y6sp3daFFRnnjX7e3bZxBm2v5vqPsXUwTG9q4uUQajXtaZC2yuroiU38fORDYt0uVYvtOhUrMxnvKYSJkuJTqCKskVeFXXrv4pbK70HrOO4wlmHx7+F0Za/0chrRj0FtyCow/p72CWYZKD5eBmtPk36VRlm7DNZeYnQ9ajV5DEVR721Xqy3kr4PL+O3sLNK801Asq+5/AdYNXFHMq+7zE6Iyk3+ZQhWGvvTuZFHAB4HHOV7rfyzIHE1IzFI02bLnpbFlW9ryU06MaufPCjDsNooOW7/Hz8dwH1wSY2pTs/EANGbqjsHMzclk0+5v2HhlIdm+eaDSGOyKZNO4cqwpLER1JMWnUM0jIT7lbqOH+1U2Bq3mqO1LPBH3IQ6JZd9iUZTfr/Xbk5qXrtr543wMv4ZswI5ZWFobrlhMuuyDT4MnMLNQf+HztLQE1vz1GX/lrKbQu+oUoIqFBuv6csldCFMlxadQTVjjsl16d7PMZ0bgUU76fcLc6y/TIGY5Sq56BY8okq+xYHGhumNqI91yDd6meew5Glr9bdA2r8a541l3CBbWNgZtt6xiYyNZsfsjjlnsBvfKPx7Uur4LGsvqOcNfiMpAik+hGlc7S9rWKfls1L6eiWwOWslB69E8HvcJdklHKy6cKLV19TuQkJ2kaoa99hWzbJbL75/j7mnYYiYlwQXXWkOwsrM3aLvlEfn3nyzfP52LTmdQnCrvzHibJqaxlrAQ4vak+BSq6tv87rvSeFnl8XmdQ0T6fsBn6eMIilmBkme4JXWEYehQWGierXYM/rZIRnFxNni7iraQ+scWojHwmpHpSY44eg3B1sm0LhHvP7qKFSc+IckjodJt16mxNcemoSyxJIQpk+JTqOrhEG8crG+9zDfIO57tQT+zx3I0vWNnYpN8QoV0oqS2BT3AhYzLascAIC+w/GOJb8cych8N3Azfs3s9xQ5rl8HYu5lWb11BQR7b9i1mbfRcrlei7Tptm3nILHchTJz8CxWqsrYwo1fTomKhpnUuc+vu54zPND5KfY2AmFUo+ZkqJxQlscDWdHrHEmtW3GVsz9Uf4VQBC7VnpVtjbjsQJ8/y709vaBkZqazf9SXb0paR52v623Xahhp2xQMhhOFJ8SlU90xrT3bW/Yk/zV+gx+VZWKWcUTuSKIX9/q04du2C2jH0zroXVFjbSm42DWPXVEgBlpNhic6sH66+AYZv3ACSkqL47a9POaA13e06LXztsfSp2DG0iqKwatWqCj0HQHh4OIqikJaWpr9t1apV1K1bFzMzM8aNG8eiRYtwdnau8CydOnVi3LhxFX4eUX1I8SlUF+TrQa3CGJSCHLWjiDKYXwFjLMvjgGPFzri32beWOt4V87ual21BfkEv3P3rV0j7hnAhqmi7ztO2R0xuu0671uXfZCAhIYGXXnqJwMBArKys8PPzo1evXmzdutUACUuubdu2xMfH4+TkpL/tueee4/HHHycmJob33nuPgQMH8vffhluJ4XYFL8DKlSt57733DHYeIaT4FKah1Qi1E4gyOOXTiD1pptVTfdgqHsWmYpcwqrnxE2wqaIvK/DxzsjLD8KrbpELaN5SIk3+w/PAHJrNdp2Jlhm1zj3K1ERUVRcuWLdm2bRuffvopx48fZ+PGjXTu3JnRo0cbKGnJWFpa4uXlhaIUdbNnZGSQmJhI9+7d8fHxwcHBARsbGzw8yvecS8LV1RUHB4cKP4+oPqT4FKYhpD9YO937OGFS5nvVUjvCLQrRURhYs0LPYZZyhUa5+yqs/cICM9JTu+BTv1WFncMQdDqtfrvOVK9UVbfrtG3hUe61PV988UUURWH//v089thj1KtXj0aNGvHqq6+yd+/e2z5m4sSJ1KtXD1tbWwIDA5k8eTL5+fn6+yMiIujcuTMODg44OjrSsmVLDh4s2gzj0qVL9OrVCxcXF+zs7GjUqBHr168HivdChoeH64u/Ll26oCgK4eHht73s/vvvv9OqVSusra2pUaMGffv21d+3ZMkSQkNDcXBwwMvLiyeeeILExESgqPDu3LkzAC4uLiiKwrBhw4BbL7unpqby9NNP4+Ligq2tLT169ODs2bP6+2/k2rRpE8HBwdjb2xMWFkZ8fHwZfiqiKpLiU5gGCxto9qTaKUQpRLnXYWtapNoxbiulVsV/kHHcvADfiplYD4BOqyEl8QFqNnyg4k5iILk5mfyx51tVt+u0v8+7XI9PSUlh48aNjB49Gjs7u1vuv9PYSgcHBxYtWsSpU6f4/PPPmTdvHp999pn+/iFDhlCzZk0OHDjAoUOHeOONN7CwKBquMHr0aHJzc9m5cyfHjx/n448/xt7+1jGrbdu25cyZoisMv/76K/Hx8bRt2/aW49atW0ffvn15+OGHOXLkCFu3bqV169b6+/Pz83nvvfeIiIhg1apVREVF6QtMPz8/fv31VwDOnDlDfHw8n3/++W2f87Bhwzh48CBr1qxhz5496HQ6Hn744WJFd1ZWFjNmzGDJkiXs3LmT6Ohoxo8ff9v2RPWj/rUSIW5oNQL2zgEMt5e2qDgL/RqgTT2udozbuugJxtjZO/Cv2SQGjyM/V1tBZ1C4Gt8avxArYo4bd8xhWdzYrtPXpwFtAnpjFmec81oFOWPhdWvBWBrnzp1Dp9PRoEGDUj3u7bff1v/d39+f8ePHs2zZMiZMmABAdHQ0r7/+ur7doKAg/fHR0dE89thjhISEABAYGHjbc1haWuovr7u6uuLldfuxrdOnT2fQoEFMmzZNf1vTpk31fx8+fLj+74GBgcyePZtWrVqRkZGBvb09rq6uAHh4eNyx2D579ixr1qxh165d+gJ46dKl+Pn5sWrVKvr37w8UFbpff/01derUAWDMmDG8++67t21TVD/S8ylMh1sdCOykdgpRAlecfPg9/bTaMe7osHOaUc5jEX2GYJuKn+mfdLkptZo8XOHnMZTYuNOs2PUxxyx2GWW7ToeOfuVuQ6cr24fe5cuX065dO7y8vLC3t+ftt98mOjpaf/+rr77KyJEj6datGx999BHnz5/X3/fyyy/z/vvv065dO6ZMmcKxY8fK9RyOHj1K165d73j/oUOH6NWrF7Vq1cLBwYGOHTsCFMt7L5GRkZibm3Pffffpb3Nzc6N+/fpERv57JcTW1lZfeAJ4e3vrL/ELIcWnMC3tXlY7gSiB7wObk6/Nv/eBKtljFQvmxrmw47bmf9TwqPhzJcY0oFbTvqCY+EKbN4n8+69/tus8XWHbdVr4OWBd17nc7QQFBaEoCqdPl/xD1Z49exgyZAgPP/wwa9eu5ciRI0yaNIm8vDz9MVOnTuXkyZM88sgjbNu2jYYNG/Lbb78BMHLkSC5cuMBTTz3F8ePHCQ0N5Ysvvijzc7C5y0S7zMxMunfvjqOjI0uXLuXAgQP6HDfnNZQbQwtuUBSlzAW+qHqk+BSmpU4XqNVG7RTiLtJtXViRcf7eB6ooS5MPtY2zYLuiLaT+qSVojDDOMTE6gFqN+6NoKtdb9/6jq/nl+CckVsB2nY4dDTO5zNXVle7du/PVV1+RmXnr5hb/XX4IYPfu3dSuXZtJkyYRGhpKUFAQly5duuW4evXq8corr/DHH3/Qr18/Fi5cqL/Pz8+P559/npUrV/Laa68xb968Mj+HJk2a3HFJqNOnT5OcnMxHH31E+/btadCgwS09kZaWlgAUFhbe8RzBwcEUFBSwb9+/E+6Sk5M5c+YMDRs2LHN2Ub1UrncwUT10fkvtBOIufgxqQ1ZBltox7umav/H297Y68Rf13FOMcq7EyzXxbfgEZkbq2TWUwsI8tu9bzO+XDLddp7mHDdaNDPdz/uqrrygsLKR169b8+uuvnD17lsjISGbPnk2bNrd+KA4KCiI6Opply5Zx/vx5Zs+ere9NBMjOzmbMmDGEh4dz6dIldu3axYEDBwgODgZg3LhxbNq0iYsXL3L48GG2b9+uv68spkyZwk8//cSUKVOIjIzUT2ICqFWrFpaWlnzxxRdcuHCBNWvW3LJ2Z+3atVEUhbVr15KUlERGRsZtn3Pv3r0ZNWoUf/31FxERETz55JP4+vrSu3fvMmcX1YsUn8L0BHQA//ZqpxC3kW1py085MWrHKJFoT+PuwuO15iMcXYyz6PrVWA886w3B3NLKKOczpMzMf7brTF1Gnm9huXaLcujgp18H0xACAwM5fPgwnTt35rXXXqNx48Y8+OCDbN26lblz595y/KOPPsorr7zCmDFjaNasGbt372by5Mn6+83MzEhOTubpp5+mXr16DBgwgB49eugnBBUWFjJ69GiCg4MJCwujXr16zJkzp8z5O3XqxC+//MKaNWto1qwZXbp0Yf/+/QC4u7uzaNEifvnlFxo2bMhHH33EjBkzij3e19eXadOm8cYbb+Dp6cmYMWNue56FCxfSsmVLevbsSZs2bdDpdKxfv/6WS+1C3Imik0EYwhRd2gMLw9ROIf5jaUgYH2WcUjtGifS9Xo/BXxo3a1a7vuy16Ga08zl7pJMas4zcrFsvE1cWAf7NaeXdAyXhzpd6b8fMyQqvCaEoZtKHIkRlI/9qhWmq3aZo/KcwGfkaCxYXJqkdo8R22cYZfXKO7a7fCPAx/OSNO0lLdMLJZwg2DpV3g4aLUUf+2a7zcKm267Rv7yuFpxCVlPzLFaar8yS1E4ibrK/fgfjsylN8JpploPiUf6/v0qq16ROs7Yw3HvNasj127k9g52KMlU0rTsTJzSw//AGX3aLuuV2nxt7CIPu4CyHUIcWnMF01QyHoIbVTCECHwgLzbLVjlFqmf8Xve/1fZsnxNCo8ZNRzZqTaYOkwACeP8u3yozadTsuug8tZeeYzUr1TUKxu/1+UY9da5d5KUwihHik+hWmTme8mYVvQA1zIuKx2jFKL91FnQo7Txm/x8TbuJf/s69Zg8RguPrWNet6KkJeXxR+757EhfgHZvrnFtus0c7OWXk8hKjkpPoVp82kO9R9RO0W1t8C2cvYynaqRo9q56+z5CnNL477F5mZZUqjrTY1aQfc+uBJIT7/Cmr9m8Vf2Kgp9im5zeqi2jPUUopKTf8HC9HV+i3KtxyLKZb9/K45dq/gtJCvCXrsrqp3bIuokwfYl37bQUPJyzMnOeRjPwEZGP3dFubFd50Xfv7Fp4q52HCFEOUnxKUyfV2No+KjaKaqt+S7Oakcos7MWySiuLqqdv8bqT3Ezwt7m/1WYb8b1aw/iXa+F0c9dkQK63mfQdT2FEOqQ4lNUDp0ngUYWMDa2Uz6N2JN2Ru0Y5ZJbx0e1cyuFBTT4+ycUFd5ptYUaUq92xDe4amxXG9S6LTUbNlY7hhDCAKT4FJWDe31o97LaKaqd+V611I5Qbok+tqqe3yoinHoe19Q5uU7hasL9+DXupM75DcTM3JwOQ55RO4YQwkCk+BSVR4cJ4FpH7RTVRpR7HbamRaodo9zOupdu55yK4L32I+yd1Om5V1BIim1BrSbdVTm/ITTv8SjOXpV7GSkhxL+k+BSVh4U19JqldopqY6FfA7Q6rdoxyu2A41W1I6DJSKdRymZVMyTGNKJWk8o3dtrR3ZO2/Z9QO4YQwoCk+BSVS0AHaPak2imqvEQnb35PP612DIM4ZBmHYqvupXcAuz9/wd+nQNUMiTF1qdXkcRQ1BqGW0YMjX8TCylrtGEIIA6o870BC3PDQe2Any61UpO8DW5KvzVc7hkHoFCisU1PtGADU3jITK5XXTE2MqUXNxgPRmBl/Fn5pNWjXEf9mLdWOIYQwMCk+ReVj6wphH6mdospKt3Hml4xzascwqGQ/J7UjAGCWGE0jjqkdg6TL3ng3eAJzS0u1o9yRtb0DnYc9q3YMIUQFkOJTVE4hj0PdB9VOUSX9VK8tWQVZascwqAseOrUj6Dmvn4O3t/pvvclxNXAPfBILaxu1o9xWx6dGYOtoGh8ahBCGpf47oBBl1fN/YGGndooqJdvSlh9zYtSOYXBHnNLUjlBMnf1zMbNQ/+039YozrrWexNreQe0oxdRq3ITGnbqpHUMIUUHUf/cToqyca0GXSWqnqFJW1u9Aal662jEMbrfNZTA3nTGOluePEewUq3YMANKTHLD3HIKts6vaUQAwt7Ck26gxascQQlQgKT5F5Xbf8+DTXO0UVUK+xoLFhUlqx6gQOUoBBJjGpKMbaqz6BJcaprFrV0aKLTbOg3Co4al2FNoOfBIXL/V2pRJCVDwpPkXlpjGDXrNBYzq9WpXV+vodiM+umsUnQHptN7UjFKMpyCP4/HJMZavyzHRrzKz74+ylXpFeq3FTQnv2Ve38QgjjkOJTVH7eTYp2PxJlpkNhgXm22jEqVLSXuksc3Y71ka0EeWaoHUMvJ9OSQk1f3GoGGv3c1vYOhI1+BcVUqnEhRIWR4lNUDR1eh7oyQaGstgU9wIWMy2rHqFDHnFXaX/0efNZ+iJ2j6fTc52dbkJvfCw//YKOe96FnX8LBtYZRzymEUIcUn6Jq0Gig3zxwqqV2kkppgcoLnxvDLrt4TOYa9000GWk0urZD7RjFFOSZkZHZHa+6TY1yvpCu3Qm6r61RziWEUJ8Un6LqsHWFAYvBzErtJJXKfv9WHLt2Qe0YFe6qJhPF11vtGLdlH/4jtXy0ascoRlugIT2lC74NWlfoeWrU8pfF5IWoZqT4FFWLbwvo8bHaKSqV+S7Oakcwmkx/D7Uj3JH/9plY2phWD7ROp5B8pR01G3WokPYtrKzpOXYiFpbygVGYHn9/f2bNmqV2DIPq1KkT48aN03+v1nOU4lNUPaHPQLMhaqeoFE75NGJP2hm1YxhNrI/pbidpHh9FI7NTase4DYWrcaHUCjH8jmJdR7yAW02/Mj9+2LBhKIqCoihYWFjg6enJgw8+yIIFC9BqTasnuSItWrQIZ2dngx2ntmHDhtGnT597HvffQuqGyvI8oeTPtaIcOHCAZ581/pUHKT5F1fTITPAKUTuFyfvOq3qNkT3lZtoz+p3XfYGnCc7KB0i8HEKtJj0N1l6z7j1p1LFrudsJCwsjPj6eqKgoNmzYQOfOnRk7diw9e/akoKDAAEmFKJu8vDy1I9yTu7s7tra2Rj+vFJ+iarKwgQFLwFr2hr6TSzUC2ZIWqXYMo9pjn6B2hLtSdDqCDs/HzNz0JkYBJMbUo1aTfuWeuFUrpBmdh40ySCYrKyu8vLzw9fWlRYsWvPXWW6xevZoNGzawaNEi/XHR0dH07t0be3t7HB0dGTBgAFeuXCnW1u+//06rVq2wtramRo0a9O3775qjiqKwatWqYsc7OzvrzxEVFYWiKPz888+0b98eGxsbWrVqxd9//82BAwcIDQ3F3t6eHj16kJRUfD3d+fPnExwcjLW1NQ0aNGDOnDn6+260u3LlSjp37oytrS1NmzZlz549AISHh/PMM8+Qnp6u7wWeOnVqmV7LjRs38sADD+Ds7Iybmxs9e/bk/Pnz+vu7dOnCmDHFd59KSkrC0tKSrVu3ApCamsrTTz+Ni4sLtra29OjRg7Nnz+qPnzp1Ks2aNSvWxqxZs/D399ffv3jxYlavXq1/PuHh4WV6Pjfc6F2cMWMG3t7euLm5MXr0aPLz8/XHJCYm0qtXL2xsbAgICGDp0qW3tJOWlsbIkSNxd3fH0dGRLl26EBERcctzmz9/PgEBAVhbWwOwYsUKQkJCsLGxwc3NjW7dupGZmXnX5zpx4kTq1auHra0tgYGBTJ48uVjeG+dasmQJ/v7+ODk5MWjQIK5fv64/JjMzk6effhp7e3u8vb2ZOXPmLc/pv5fdFUVh/vz59O3bF1tbW4KCglizZk2xx6xZs4agoCCsra3p3LkzixcvRlEU0tLSSvwzkeJTVF2uAdD3G8A0/yNX28JawWh11efSJMAF81QUN9PYRvJOLP8+SAOXK/c+UCWJMf74hQxAY1a2HloXbx96jXsDjabieni7dOlC06ZNWblyJQBarZbevXuTkpLCjh072Lx5MxcuXGDgwIH6x6xbt46+ffvy8MMPc+TIEbZu3Urr1qWfbDVlyhTefvttDh8+jLm5OU888QQTJkzg888/588//+TcuXO88847+uOXLl3KO++8w/Tp04mMjOSDDz5g8uTJLF68uFi7kyZNYvz48Rw9epR69eoxePBgCgoKaNu2LbNmzcLR0ZH4+Hji4+MZP358mV63zMxMXn31VQ4ePMjWrVvRaDT07dtXP4Rh5MiR/Pjjj+Tm5uof88MPP+Dr60uXLl2AokLv4MGDrFmzhj179qDT6Xj44YeLFU53M378eAYMGKDv0Y6Pj6dt2/KvhLB9+3bOnz/P9u3bWbx4MYsWLSr24WTYsGHExMSwfft2VqxYwZw5c0hMTCzWRv/+/UlMTGTDhg0cOnSIFi1a0LVrV1JSUvTHnDt3jl9//ZWVK1dy9OhR4uPjGTx4MMOHDycyMpLw8HD69euHTqe763N1cHBg0aJFnDp1is8//5x58+bx2WefFctz/vx5Vq1axdq1a1m7di07duzgo48+0t//+uuvs2PHDlavXs0ff/xBeHg4hw8fvudrNW3aNAYMGMCxY8d4+OGHGTJkiP45Xrx4kccff5w+ffoQERHBc889x6RJpd/m2nQWlxOiItTvAe1fhT9v/cRXnSU6ebMm/bTaMVSRG+iDZXLKvQ9Ukcfqj4ntNZu05JL9h21sSTG++AQPJv7McgpLWFQAWNnZ0WfCO1jb21dguiINGjTg2LFjAGzdupXjx49z8eJF/PyKxph+//33NGrUiAMHDtCqVSumT5/OoEGDmDZtmr6Npk1Lv9TU+PHj6d69OwBjx45l8ODBbN26lXbt2gEwYsSIYkXPlClTmDlzJv369QMgICCAU6dO8c033zB06NBi7T7yyCNAUXHQqFEjzp07R4MGDXByckJRFLy8vEqd92aPPfZYse8XLFiAu7s7p06donHjxvTr148xY8awevVqBgwYABSNr7wx9vbs2bOsWbOGXbt26YuopUuX4ufnx6pVq+jfv/89M9jb22NjY0Nubm65n8/NXFxc+PLLLzEzM6NBgwY88sgjbN26lVGjRvH333+zYcMG9u/fT6tWrQD47rvvCA7+d63bv/76i/3795OYmIiVVdEEuRkzZrBq1SpWrFihHzeZl5fH999/j7u7OwCHDx+moKCAfv36Ubt2bQBCQv4dEnan5/r222/r/+7v78/48eNZtmwZEyb8u6GKVqtl0aJFODg4APDUU0+xdetWpk+fTkZGBt999x0//PADXbsWDW9ZvHgxNWveewezYcOGMXjwYAA++OADZs+ezf79+wkLC+Obb76hfv36fPrppwDUr1+fEydOMH369Hu2ezPp+RRVX+e3IbCT2ilMyveBLcnXmmZhU9Gu1DT++KbSUvJyCI5eadKd9ldjPfAMGoLFP5cW70XRaOj58gRcfYyzfadOp9PvlhQZGYmfn5++8ARo2LAhzs7OREYWDT05evSo/j/p8mjSpIn+756enkDxYsPT01Pfo5aZmcn58+cZMWIE9vb2+q/333+/2OXu/7br7V20ZNh/e+bK6+zZswwePJjAwEAcHR31l8Kjo6MBsLa25qmnnmLBggVAUWF14sQJhg0bBhS9zubm5tx33336Nt3c3Khfv77+dVZLo0aNMLupt97b21v/+t3I3bJlS/39DRo0KDZpKSIigoyMDNzc3Ir9rC5evFjsZ1W7dm194QlFH2C6du1KSEgI/fv3Z968eaSmpt4z7/Lly2nXrh1eXl7Y29vz9ttv638ON/j7++sLz/8+p/Pnz5OXl1fsZ+Hq6kr9+vXvee6bf9fs7OxwdHTUt3vmzBl9gX5DWa4QSM+nqPo0GnhsASx4CJLPqZ1Gdek2zvySUX1fh79r5FP2+dXGY3NgI3UHd+VcvOkWyynxrrjVHkLq5eXkZt59m9COTw7Hv1nLux5jSJGRkQQEBJT4eBsbm7verygKOp2u2G23u5RsYWFR7DG3u+3GZeyMjKLXbN68ecWKBKBYoXSndg09o79Xr17Url2befPm4ePjg1arpXHjxsUmzowcOZJmzZpx+fJlFi5cSJcuXfQ9eiWh0WhK9DqWhKOjI+np6bfcnpaWhpNT8fH+N79+UPznUBIZGRl4e3vfdvzpzUWqnZ1dsfvMzMzYvHkzu3fv5o8//uCLL75g0qRJ7Nu3746/n3v27GHIkCFMmzaN7t274+TkxLJly24Zs1ne53QnFdXuzaTnU1QPdm7w5EqwN9xlnMrqp3ptySrIUjuGavY7XlU7QonVXPcRtg6m3UeQluiEk/cQbByd73hM484P0vKRPkbLtG3bNo4fP66/jBwcHExMTAwxMTH6Y06dOkVaWhoNGzYEinp7bkyauR13d3fi4+P13589e5asrPL9O/L09MTHx4cLFy5Qt27dYl+lKZwtLS0pLCwsV5bk5GTOnDnD22+/TdeuXQkODr5tD11ISAihoaHMmzePH3/8keHDh+vvCw4OpqCggH379t3S7o3X2d3dnYSEhGIF6NGjR8v0fOrXr3/bMYyHDx+mXr1693z8DQ0aNKCgoIBDhw7pbztz5kyxCTQtWrQgISEBc3PzW35WNWrcfVtYRVFo164d06ZN48iRI1haWvLbb78Bt3+uu3fvpnbt2kyaNInQ0FCCgoK4dOlSiZ8PQJ06dbCwsCj2s0hNTeXvv/8uVTv/Vb9+fQ4ePFjstgMHDpS6HdN+VxPCkFxqw1MrYWEPyLn103J1kG1py485Mfc+sAo7ahGPYmeHLjNT7Sj3pLmWTKOsXRzgvnsfrKJryXbYuQ7GzHwFGSnFZ3L7N2tJt5GjK+zcubm5JCQkUFhYyJUrV9i4cSMffvghPXv25OmnnwagW7duhISEMGTIEGbNmkVBQQEvvvgiHTt2JDQ0FCgae9m1a1fq1KnDoEGDKCgoYP369UycOBEomsT05Zdf0qZNGwoLC5k4ceItPURlMW3aNF5++WWcnJwICwsjNzeXgwcPkpqayquvvlqiNvz9/cnIyGDr1q00bdoUW1vbOy6fU1hYeEuxZ2VlRf369XFzc+Pbb7/F29ub6Oho3njjjdu2MXLkSMaMGYOdnV2xFQGCgoLo3bs3o0aN4ptvvsHBwYE33ngDX19fevfuDRStzZmUlMQnn3zC448/zsaNG9mwYQOOjo7Fns+mTZs4c+YMbm5uODk53fa1fuGFF/jyyy95+eWXGTlyJFZWVqxbt46ffvqJ33//vUSvHRQVVGFhYTz33HPMnTsXc3Nzxo0bV6w3vFu3brRp04Y+ffrwySefUK9ePeLi4vQT1W78Hv3Xvn372Lp1Kw899BAeHh7s27ePpKQk/XjS2z3XoKAgoqOjWbZsGa1atWLdunX6YrWk7O3tGTFiBK+//jpubm54eHgwadIkNJry9Tk+99xz/O9//2PixImMGDGCo0eP6scwK6VYBUN6PkX14tkIBi8D85KNU6tqVtbvQGpe9Sy8b9ApUBBonHGHhuCw9Xv8fHT3PlBlmek2WNgPwMnDR3+bd70GPPrqm5iZV1w/x8aNG/H29sbf35+wsDC2b9/O7NmzWb16tf7StaIorF69GhcXFzp06EC3bt0IDAxk+fLl+nY6derEL7/8wpo1a2jWrBldunRh//79+vtnzpyJn58f7du354knnmD8+PEGWR9x5MiRzJ8/n4ULFxISEkLHjh1ZtGhRqXo+27Zty/PPP8/AgQNxd3fnk08+ueOxGRkZNG/evNhXr1690Gg0LFu2jEOHDtG4cWNeeeUV/aSS/xo8eDDm5uYMHjxYv5zQDQsXLqRly5b07NmTNm3aoNPpWL9+vb54DA4OZs6cOXz11Vc0bdqU/fv33zI7f9SoUdSvX5/Q0FDc3d3ZtWvXbXMEBgayc+dOTp8+Tbdu3bjvvvv4+eef+eWXXwgLCyvx63cjt4+PDx07dqRfv348++yzeHj8uyOaoiisX7+eDh068Mwzz1CvXj0GDRrEpUuX9GN7b8fR0ZGdO3fy8MMPU69ePd5++21mzpxJjx497vhcH330UV555RXGjBlDs2bN2L17N5MnTy7V8wH49NNPad++Pb169aJbt2488MADxca1lkVAQAArVqxg5cqVNGnShLlz5+pnu9+YiFUSiu6/gy+EqA5Or4flT4KufJeqKpN8jQWPNGhCfHbSvQ+u4r481hyPdaW/VKSWAt+67A0ZT16O6f++WtnmY8bvaDQ6Bk792Cgz24VxRUVFUadOHQ4cOECLFi3UjiNUNn36dL7++utiw1ruRXo+RfXU4GF4dDYmPZ3YwNbX7yCF5z8ueFSuz9zmsedoaFW+sVrGkptlgbXzAPq99a4UnlVMfn4+CQkJvP3229x///1SeFZTc+bM4cCBA1y4cIElS5bw6aefFlsWrCSk+BTVV/Mn4ZEZaqcwCh0KC8xNe2tJYzrsdO+lTkyNy++f4+5pmltv3szexYpHXmyFg6ub2lGEge3atQtvb28OHDjA119/rXYcoZKzZ8/Su3dvGjZsyHvvvcdrr71W6l215LK7EHvmwKY31U5RobYGPcC4guh7H1hNWOnMWDJTC2Vc4kUtecH3sdtnKNpC03zbtnG0pN9rLXD2NN3loYQQ6pOeTyHavAhdp6idokItsJWFLW6WqxSCf+WZdHSDZeQ+GriZ5tAJa3sLeo9tJoWnEOKeKl3xqSgKq1atMnqbw4YNo0+fPgY9b1U3depUmjVrdtdjoqKiUBTllqU/jK79q9BxoroZKsh+/1Ycu3ZB7RgmJ622ae/xfieeqz/CybX8S/wYkp2TJX1fa4Gbr4zxFELcW6mKzzsVYOHh4SiKUmxB1sokPj5ev+zBnYqhzz//vNh+vBXlxh65iqJgYWFBQEAAEyZMICcnp8LPbWjjx48vtmjz7X5//Pz8iI+Pp3HjxkZOdxud36qSBeh3Ls5qRzBJ0V6V7rM3AEpuNg1j15jMXDlHdxv6vd4SV2+7ex8shBBUwp5PQ7qxZZiXl9c916dycnIqtoVWRQoLCyM+Pp4LFy7w2Wef8c033zBlSuW7LGxvb4+b290nHZiZmeHl5YV5Ba4DWCqd34Kes0Ax/YkdJXHKpxG7086oHcMkHXO5+3aQpsxm31rqeKv/gdTVx45+41vgWOPuW1MKIcTNKqT4TE5OZvDgwfj6+mJra0tISAg//fRTsWM6derEyy+/zIQJE3B1dcXLy+uW2VJnz56lQ4cOWFtb07BhQzZv3lzs/scff5wxY8bovx83bhyKonD69GmgqLi0s7Njy5Yt+nOOGTOGcePGUaNGDbp37w4Uv+x+Y2Hf5s2boygKnTp1Am7ttStJ/tOnT/PAAw/o82/ZsqVEl/itrKzw8vLCz8+PPn360K1bt2LPXavV8uGHHxIQEICNjQ1NmzZlxYoVxdo4efIkPXv2xNHREQcHB9q3b8/58+f1j3/33XepWbMmVlZWNGvWjI0bNxZ7/O7du2nWrBnW1taEhoayatWqYj3CN3q7t27dSmhoKLa2trRt25YzZ/4tdG6+7D516lQWL17M6tWr9T274eHht+1p3rFjB61bt8bKygpvb2/eeOMNCgoKSvXal0voMzD4J7Co/D0533nVUjuCydplGwul2JHD1NTc+Ak29up9aPOo7UDf11pg51TyhaWFEAIqqPjMycmhZcuWrFu3jhMnTvDss8/y1FNPFdstAmDx4sXY2dmxb98+PvnkE9599119kaXVaunXrx+Wlpbs27ePr7/+Wr/N2Q0dO3YkPDxc//2OHTuoUaOG/rYDBw6Qn59P27Zti53T0tKSXbt23XapiBsZt2zZQnx8PCtXrrzj87xb/sLCQvr06YOtrS379u3j22+/1e8CUBonTpxg9+7dWFpa6m/78MMP+f777/n66685efIkr7zyCk8++SQ7duwAIDY2lg4dOmBlZcW2bds4dOgQw4cP1xdwn3/+OTNnzmTGjBkcO3aM7t278+ijj3L27FkArl27Rq9evQgJCeHw4cO89957t7z2N0yaNImZM2dy8OBBzM3Ni+3ze7Px48czYMAAfa9ufHx8sZ/LDbGxsTz88MO0atWKiIgI5s6dy3fffcf7779f7Li7vfYGUa87DFsLdu6Ga9PILtUIZEtapNoxTFayJgulps+9DzRRZilXaJS7794HVgDfes70fqU51namNfZUCFE5lPpj89q1a7H/z8LBhYXFd93w9fUttl3WSy+9xKZNm/j5559p3bq1/vYmTZroLycHBQXx5ZdfsnXrVh588EG2bNnC6dOn2bRpEz4+Rf9BfPDBB/qxmVDUAzZ27FiSkpIwNzfn1KlTTJ48mfDwcJ5//nnCw8Np1apVsS3QgoKC7rr1mLt7UbHh5uaGl5fXXV+Lu+XfvHkz58+fJzw8XN/O9OnTefDBB+/aJvz7GhcUFJCbm4tGo+HLL78EivYx/uCDD9iyZQtt2rQBirYY++uvv/jmm2/o2LEjX331FU5OTixbtky/pVm9evX07c+YMYOJEycyaNAgAD7++GO2b9/OrFmz+Oqrr/jxxx9RFIV58+bpe21jY2MZNWrULVmnT59Ox44dAXjjjTd45JFHyMnJuWXLNXt7e2xsbMjNzb3r6zpnzhz8/Pz48ssvURSFBg0aEBcXx8SJE3nnnXf0+9Le7bU3GN8WMGIzLH0cks8Zrl0jWVgrGG3qcbVjmLQM/xrYxcSqHaPMHDcvwPeJlsTGGe+c/k1q0H1UI8wtqsbQFCGE8ZW6+OzcuTNz584tdtu+fft48skn9d8XFhbywQcf8PPPPxMbG0teXh65ubm37IPbpEmTYt97e3uTmJgIQGRkJH5+fvrCE9AXWzc0btwYV1dXduzYgaWlJc2bN6dnz5589dVXQFFP6I3L5jeUd1/TkuY/c+YMfn5+xQqtmwvvu7nxGmdmZvLZZ59hbm7OY489BsC5c+fIysq6pcjKy8ujefPmABw9epT27dvrC8+bXbt2jbi4ONq1a1fs9nbt2hEREaHP3qRJk2IF5J2y3/waeHt7A5CYmEitWmW73BsZGUmbNm1Qbroc2q5dOzIyMrh8+bK+3bu99gblGlBUgP40CGLU6WUqi0Qnb9akn1Y7hsmL87EiSO0Q5RT412wSg8eRn6ut8HM1fMCHjoProTGr1tMFhBDlVOri087Ojrp16xa77fLly8W+//TTT/n888+ZNWsWISEh2NnZMW7cOP0Enxv+WxwpioJWW/I3UEVR6NChA+Hh4VhZWdGpUyeaNGlCbm6u/nL1zT2wN/IbSnnz38nNr/GCBQto2rQp3333HSNGjCAjo2iSxLp16/D19S32uBuTpmxsjDf4/+bX4EbBaIjXoDTnvXHuCjuvrSs8vQZWjoTI3yvmHAb2fWBL8tOOqR3D5J10y670xadF9BmCW1zgWK5/hZ1DUaDtY3Vp1k3GEAshyq9CPr7u2rWL3r178+STT9K0aVMCAwP5++/S7UscHBxMTEwM8fHx+tv27t17y3E3xn2Gh4fTqVMnNBoNHTp04NNPPyU3N/eWHr57uTG28r9DCUqrfv36xMTEcOXKFf1tBw4cKHU7Go2Gt956i7fffpvs7GwaNmyIlZUV0dHR1K1bt9iXn58fUNQr+Oeff5J/m91bHB0d8fHxYdeuXcVu37VrFw0bNtRnP378OLm5ueXK/l+Wlpb3fF2Dg4PZs2cPN2+8tWvXLhwcHKhZU8VFwS2sof/30Po59TKUULqNM79kVL5hAmrYbRd/74MqAbc1/6OGR8VMPrKwMqPHC02k8BRCGEyFFJ9BQUFs3ryZ3bt3ExkZyXPPPVesCCuJbt26Ua9ePYYOHUpERAR//vnnbSfsdOrUiVOnTnHy5EkeeOAB/W1Lly4lNDS01D2dHh4e2NjYsHHjRq5cuUJ6enqpHn/Dgw8+SJ06dRg6dCjHjh1j165dvP322wDFLimXRP/+/TEzM+Orr77CwcGB8ePH88orr7B48WLOnz/P4cOH+eKLL1i8eDEAY8aM4dq1awwaNIiDBw9y9uxZlixZop+J/vrrr/Pxxx+zfPlyzpw5wxtvvMHRo0cZO3YsAE888QRarZZnn32WyMhINm3axIwZM8qU/Wb+/v4cO3aMM2fOcPXq1dsWxy+++CIxMTG89NJLnD59mtWrVzNlyhReffVV/XhP1Wg08PAn8ND7mMwii7fxU722ZBVkqR2jUogyT0NTo/LvQa5oC6l/agkajWF/Lx3crOn3eksCmtQwaLtCiOqtQv43f/vtt2nRogXdu3enU6dOeHl5lXp3II1Gw2+//UZ2djatW7dm5MiRTJ8+/ZbjQkJCcHZ2plmzZvqJUJ06daKwsPCW8Z4lYW5uzuzZs/nmm2/w8fGhd+/epW4DitavXLVqFRkZGbRq1YqRI0fqi+f/TsYpSaYxY8bwySefkJmZyXvvvcfkyZP58MMPCQ4OJiwsjHXr1umXiXJzc2Pbtm1kZGTQsWNHWrZsybx58/SXql9++WVeffVVXnvtNUJCQti4cSNr1qwhKKjoAqSjoyO///47R48epVmzZkyaNIl33nmnTNlvNmrUKOrXr09oaCju7u639L5C0WS19evXs3//fpo2bcrzzz/PiBEj9IW7SWj7Ejy+wCSXYsq2tOXHnBi1Y1Qq2YHeakcwCKsTf1HPPcVg7fnWd2HAm62oUVN2LRJCGJaiu/n6pqhQu3bt4oEHHuDcuXPUqVNH7TilsnTpUp555hnS09ONOqbUpF09CyuegQTTmVG+tHF3PsqU5ZVKY8aZFtRauf/eB1YCWht7Dof9j2upt15VKI0mnWvS7vG6MrFICFEhTGRbmarpt99+w97enqCgIM6dO8fYsWNp165dpSg8v//+ewIDA/H19SUiIoKJEycyYMAAKTxvViMIRm6FzVNg39x7H1/B8jUWLNZeVTtGpfO3Wx5VZTSjJjuDhokb2GvRrUyPt7Qxp9MT9Qlq5WngZEII8S/5WFuBrl+/zujRo2nQoAHDhg2jVatWrF69Wu1YJZKQkMCTTz5JcHAwr7zyCv379+fbb79VO5bpMbeCHh/BEz+Drbrj4tbX70B8dpKqGSqj/Y5Vq2C33fUbAT559z7wP7wCHRk4qZUUnkKICieX3YUwlOsJsPJZuLjD6KfWodA3pA3nMy7f+2BRjKKDn7+0QpeRqXYUgyl082bf/VPJySy457GKAi2616Z1rwC5zK4yf39/xo0bx7hx49SOUm5Tp05l1apVxbZOvpuoqCgCAgI4cuSIfltmUXXJO40QhuLgBU+tgq5TQGPcES3bgtpJ4VlGOgUK6qi4jFcFMEuOp1HhoXseZ+dkyaNjm3F/nzoGKTyHDRuGoigoioKFhQWenp48+OCDLFiwwCjr/5bEsGHDSj0B1lgOHDjAs88+W+Hn8ff3R1EUli1bdst9jRo1QlEUFi1aVOE5RPUlxacQhqTRQPtXYfgmcPE32mkX2Mrw7fK4WtNR7QgG57TxW3y877z0kn+IGwMnt6ZmA1eDnjcsLIz4+HiioqLYsGEDnTt3ZuzYsfTs2ZOCgnv3xFZFt1tW7nbc3d1v2Qmwovj5+bFw4cJit+3du5eEhASDbsYixO1I8SlERagZCs/9CSH9K/xU+/1bcezahQo/T1V23sM0euUMrc6erzC3LP42b25lRvuB9XhkdFNs7C0Nfk4rKyu8vLzw9fWlRYsWvPXWW6xevZoNGzYU601LS0tj5MiRuLu74+joSJcuXfRb/N6wevVqWrRogbW1NYGBgUybNq1YAasoCnPnzqVHjx7Y2NgQGBjIihUrypX/xIkT9OjRA3t7ezw9PXnqqae4evXfccEbN27kgQcewNnZGTc3N3r27Mn58+f190dFRaEoCsuXL6djx45YW1uzdOlSfY/rjBkz8Pb2xs3NjdGjRxcrTP39/Zk1a1ax5zd//nz69u2Lra0tQUFBrFmzpljeG8vkWVtb07lzZxYvXoyiKKSlpd31eQ4ZMoQdO3YQE/Pv0mwLFixgyJAhmJsX/zAbHR1N7969sbe3x9HRkQEDBtyydvdHH32Ep6cnDg4OjBgxgpycnFvOOX/+fIKDg7G2tqZBgwbMmTPnjvlSU1MZMmQI7u7u2NjYEBQUdEuxLCovKT6FqCjWjvDYfOgzFywdKuw037k4V1jb1cVhp1S1I1QIi6iTBNtH67+v2cCFwZNb06SzcYcZdOnShaZNm7Jy5Ur9bf379ycxMZENGzZw6NAhWrRoQdeuXUlJKVqr9M8//+Tpp59m7NixnDp1im+++YZFixbdst7z5MmTeeyxx4iIiGDIkCEMGjSIyMiyLTeWlpZGly5daN68OQcPHtRvNjJgwAD9MZmZmbz66qscPHiQrVu3otFo6Nu37y3DCt544w3Gjh1LZGQk3bt3B2D79u2cP3+e7du3s3jxYhYtWnTPy9vTpk1jwIABHDt2jIcffpghQ4boX6OLFy/y+OOP06dPHyIiInjuueduuxnL7Xh6etK9e3f95iRZWVksX76c4cOHFztOq9XSu3dvUlJS2LFjB5s3b+bChQsMHDhQf8zPP//M1KlT+eCDDzh48CDe3t63FJZLly7lnXfeYfr06URGRvLBBx8wefJk/fn/a/LkyZw6dYoNGzYQGRnJ3LlzqVFDNjuoKuRanRAVrdkTUKcLbH4Hji03aNOnfBqxO+2MQdusjvZZxzLGwgJKeHm0Mqmx+lO8B8+lQedAGrbzUS1HgwYNOHbsGAB//fUX+/fvJzExESsrKwBmzJjBqlWrWLFiBc8++yzTpk3jjTfeYOjQoQAEBgby3nvvMWHCBKZMmaJvt3///owcORKA9957j82bN/PFF1/ctVftTr788kuaN2/OBx98oL9twYIF+Pn58ffff1OvXj0ee+yxYo9ZsGAB7u7unDp1isaNG+tvHzduHP369St2rIuLC19++SVmZmY0aNCARx55hK1btzJq1Kg7Zho2bBiDBw8G4IMPPmD27Nns37+fsLAwvvnmG+rXr8+nn34KFG2NfOLEidtuyHI7w4cP57XXXmPSpEmsWLGCOnXq3DLZZ+vWrRw/fpyLFy/qt3D+/vvvadSoEQcOHKBVq1bMmjWLESNGMGLECADef/99tmzZUqz3c8qUKcycOVP/mgQEBOg/VNz4Gd8sOjqa5s2bExoaChT1CouqQ3o+hTAGBy/o923RWFCvEIM1+51XVVmhUl25SiG6QD+1Y1QIpx5hPPpcfVULTwCdTqffnjciIoKMjAzc3Nywt7fXf128eFF/CTsiIoJ333232P2jRo0iPj6erKx/t49t06ZNsfO0adOmzD2fERERbN++vdg5GzRoAKDPdfbsWQYPHkxgYCCOjo76oig6OrpYWzeKpps1atQIMzMz/ffe3t4kJibeNVOTJk30f7ezs8PR0VH/mDNnztCqVatix7du3bqEzxYeeeQRMjIy2LlzJwsWLLil1xMgMjISPz8/feEJ0LBhQ5ydnfWvc2RkJPfdd1+xx938c8nMzOT8+fOMGDGi2Gv7/vvvFxuycLMXXniBZcuW0axZMyZMmMDu3btL/LyE6ZOeTyGMqdb98OxOOLQAtr0P2WW/3HupRiBb0mQ3I0NJr+WCcxXqRLYMCMDrncnY/ac4U0tkZKR+C+CMjAy8vb0JDw+/5ThnZ2f9MdOmTbul9xDKt83v3WRkZNCrVy8+/vjjW+7z9i7ahrVXr17Url2befPm4ePjg1arpXHjxuTlFV9b9XaTdm5scXyDoij3XAWgLI8pKXNzc5566immTJnCvn37+O233wzS7n9lZGQAMG/evFuK1JuL8Zv16NGDS5cusX79ejZv3kzXrl0ZPXo0M2bMqJCMwrik+BTC2DQaaDUSGvWDre/C4cWgK/1/JgtrBaNNNZ2tPSu7KE8NzdQOYQCKtTU1nn8Ot+HDUSwNP6GoLLZt28bx48d55ZVXAGjRogUJCQmYm5vf8XJqixYtOHPmDHXr1r1r23v37uXpp58u9n3z5s3LlLNFixb8+uuv+Pv73zLpBiA5OZkzZ84wb9482rdvDxQNIVBL/fr1Wb9+fbHbDhw4UKo2hg8fzowZMxg4cCAuLi633B8cHExMTAwxMTH63s9Tp06RlpZGw4YN9cfs27fvlp/DDZ6envj4+HDhwgWGDBlS4mzu7u4MHTqUoUOH0r59e15//XUpPqsIKT6FUIutK/SaBS2HwfrX4XLJ9xdPdPJmTfrpCotWHUW4XK/0xad9ly54vvUmljXVW7c0NzeXhIQECgsLuXLlChs3buTDDz+kZ8+e+uKkW7dutGnThj59+vDJJ59Qr1494uLiWLduHX379iU0NJR33nmHnj17UqtWLR5//HE0Gg0RERGcOHGC999/X3++X375hdDQUB544AGWLl3K/v37+e677+6aMT09/ZbFz2/MPp83bx6DBw9mwoQJuLq6cu7cOZYtW8b8+fNxcXHBzc2Nb7/9Fm9vb6Kjo3njjTcM/hqW1HPPPcf//vc/Jk6cyIgRIzh69Kh+AtONIQ73EhwczNWrV++4xFO3bt0ICQlhyJAhzJo1i4KCAl588UU6duyoH1owduxYhg0bRmhoKO3atWPp0qWcPHmSwMBAfTvTpk3j5ZdfxsnJibCwMHJzczl48CCpqam8+uqrt5z3nXfeoWXLljRq1Ijc3FzWrl1LcHBwKV8hYapkzKcQavNpBiP+KJoVb+dRood8H9iCfG3Vmxyjpt22sUW90pWQ3QMP4P/zcvzmfKVq4QlFSxF5e3vj7+9PWFgY27dvZ/bs2axevVp/iVVRFNavX0+HDh145plnqFevHoMGDeLSpUt4ehZt79m9e3fWrl3LH3/8QatWrbj//vv57LPPqF27drHzTZs2jWXLltGkSRO+//57fvrpJ32P3J2Eh4fTvHnzYl/Tpk3Dx8eHXbt2UVhYyEMPPURISAjjxo3D2dkZjUaDRqNh2bJlHDp0iMaNG/PKK6/oJ/uoISAggBUrVrBy5UqaNGnC3Llz9bPdb0zkKgk3NzdsbGxue5+iKKxevRoXFxc6dOhAt27dCAwMZPnyfydPDhw4kMmTJzNhwgRatmzJpUuXeOGFF4q1M3LkSObPn8/ChQsJCQmhY8eOLFq0SD8U478sLS158803adKkCR06dMDMzOy2i+KLykm21xTClORcg/CPYP+3cIfiMt3GmYdqepFVkHXb+0XZ/bLUE110rNoxSsz2/vtxf/klbFu0UDuKKhRF4bfffjPZHYvUMH36dL7++uti63cKYWrksrsQpsTaEcI+gPufh78+gyM/QGHxiQw/1WtL1rUTKgWs2jL8PbCrBMWnTWhL3F9+GbtSzGwWVdOcOXNo1aoVbm5u7Nq1i08//ZQxY8aoHUuIu5LiUwhT5FwLen4G7cfDrs+LJiUV5JBtacuPOdKjUVEu+1hQX+0Qd2HTtCk1Xn4J+3bt1I4iTMTZs2d5//33SUlJoVatWrz22mu8+eabascS4q7ksrsQlcH1K7B7NkuzL/FResS9jxdlMii9Af3mmF6vsnXjxri//BL2HTqoHUUIIcpNik8hKpG0nDSWnVnGT6d/IiUnRe04VU6tQmdmfHL13gcaiVWDBri//BIOXbqoHUUIIQxGik8hKqHcwlzWnF/D9ye/J+palNpxqpRf5jujS1KxADU3x6FzZ1yeGGwyC8QLIYQhSfEpRCWm0+nYcXkHv/79K3/F/kWBrkDtSJXekm3BWO0z/uL95u7uOPfvj/PAAVj8s9yQEEJURVJ8ClFFXM2+ytrza1l9fjXn0s6pHafS+vRMC2qvLPmC/+WiKNjefx8uAwfi0K0bym121RFCiKpGik8hqqATV0+w6twq1l9cz/W862rHqVRGJjfmoW+PVug5LGrWxKlvH5z79MHC17dCzyWEEKZGik8hqrDcwly2RW9j9bnV7Infg7YMe8hXNyF5HkyeGWfwdhVbWxwfeginfn2xbdWqxNsfCiFEVSPFpxDVREJmAusurGPH5R0cSzpGoa5Q7Ugm65cvbdBdL3+Psbm7O3Yd2mPfsSP27dqhsbMzQDohhKjcpPgUohpKz01nV+wudsbuZHfsblJzU9WOZFJ+Wl8Xs4jTpX+gRoNNkybYd+yAfceOWAUHSw+nEEL8hxSfQlRzWp2W41ePs/PyTv68/CenU06jo3q/LXxxvDmeaw+U6FgzJyfs2rfHvmMH7B54AHMXlwpOJ4QQlZsUn0KIYpKykvgr9i/+jP2TiKQIErMS1Y5kdGOvNKXdgkN3vN8qOLiod7NDR2yaNkExMzNiOiGEqNyk+BRC3FVSVhInrp7gRPIJTl49yYnkE6Tnpqsdq0I9kOPHy59dBIrGbVqHhGAT0hjrxiFYN24kvZtCCFEOUnwKIUot5npMUSH6T1EamRxJVkGW2rHKRUHB196Xei71CHauxxPJ9bBp0qTKLfiuKAq//fYbffr0ueMxw4YNIy0tjVWrVhkt1w2dOnWiWbNmzJo1q0THh4eH07lzZ1JTU3F2dq7QbMLw/P39GTduHOPGjSt3W2r+3orSkRWNhRCl5ufgh5+DH2EBYUDRuNHY67FEX48m5npM0Z/XYoi5HkNcZhzZBdkqJy5ibWaNp50nnraeeNl54Wnria+9L0EuQdR1routha3aEYsZNmwYixcv5rnnnuPrr78udt/o0aOZM2cOQ4cOZdGiRWVqPyoqioCAAI4cOUKzZs30t3/++edUpX4Jf39/Ll26BIC1tTWenp60bt2a559/ni5duqicznhKW5zt2bOHBx54gLCwMNatW1ex4Qygqv3eVmVSfAohyk2jaPBz9MPP0e+296fnppOQmcCVrCv6PzPyMsgqyCIrP0v/Z3ZBNlkFWWTmZ+q//+/kJ0uNJVZmVliZW2FlZoWlmWWxP63MrHC1dtUXlzf/6WTlZIyXw6D8/PxYtmwZn332GTY2NgDk5OTw448/UqtWrQo5p5NT5Xud7uXdd99l1KhR5OXlERUVxQ8//EC3bt147733mDRpktrxTNJ3333HSy+9xHfffUdcXBw+Pj5qR7qrqvh7W2XphBDCRGm1Wl1mXqbuWu41XU5Bjk6r1aodyaiGDh2q6927t65x48a6H374QX/70qVLdU2aNNH17t1bN3ToUP3ttWvX1n322WfF2mjatKluypQp+u8B3W+//ab/+81fHTt2LHbeGzp27KgbM2aMbuzYsTpnZ2edh4eH7ttvv9VlZGTohg0bprO3t9fVqVNHt379+mLnDg8P17Vq1UpnaWmp8/Ly0k2cOFGXn5+vvz8jI0P31FNP6ezs7HReXl66GTNm6Dp27KgbO3as/pjvv/9e17JlS529vb3O09NTN3jwYN2VK1f092/fvl0H6FJTU+/4Ot7uddHpdLp33nlHp9FodKdPny5x5sLCQt3HH3+sq1Onjs7S0lLn5+ene//99++Y5ciRIzpAd/HiRZ1Op9MtXLhQ5+TkpPv999919erV09nY2Ogee+wxXWZmpm7RokW62rVr65ydnXUvvfSSrqCgQN9OTk6O7rXXXtP5+PjobG1tda1bt9Zt375df/+Ndjdu3Khr0KCBzs7OTte9e3ddXFycTqfT6aZMmXLLz/vmx//X9evXdfb29rrTp0/rBg4cqJs+fXqx+2881y1btuhatmyps7Gx0bVp06bYa3nu3Dndo48+qvPw8NDZ2dnpQkNDdZs3b77jz+aZZ57RPfLII8Xuz8vL07m7u+vmz5+v0+l0ul9++UXXuHFjnbW1tc7V1VXXtWtXXUZGhk6nu/X39m7HCnVpVKl4hRCiBBRFwdbCFgdLB6zMrKrtmpnDhw9n4cKF+u8XLFjAM888U+529+8v2sN+y5YtxMfHs3Llyjseu3jxYmrUqMH+/ft56aWXeOGFF+jfvz9t27bl8OHDPPTQQzz11FNkZRWN/Y2NjeXhhx+mVatWREREMHfuXL777jvef/99fZuvv/46O3bsYPXq1fzxxx+Eh4dz+PDhYufNz8/nvffeIyIiglWrVhEVFcWwYcPK/dwBxo4di06nY/Xq1SXO/Oabb/LRRx8xefJkTp06xY8//ohnKccFZ2VlMXv2bJYtW8bGjRsJDw+nb9++rF+/nvXr17NkyRK++eYbVqxYoX/MmDFj2LNnD8uWLePYsWP079+fsLAwzp49W6zdGTNmsGTJEnbu3El0dDTjx4//f3t3GxRV9ccB/MvT8rTgBiwPYg6OCGIBg9oDIMNMMC2JG4QlEUkMUFCZNkEhgahTTZouoxEvGkOo8WHQmeqFKUnMkA4SCoNazQLJgGhDQQwFaxYPnv8Lh/tnYVcWsWvY9zOzM9x7D+ecvfe++O3v3nMOACAvLw/r1q1DXFwcenp60NPTg4iICLN9PHLkCJYuXYrAwEA8//zz2L9/v8lH2oWFhdDpdGhqaoKtrS0yMjKkYwaDAatXr0ZtbS1aWloQFxcHrVaL7u5uk21mZWWhuroaPT090r5jx47hzz//RHJyMnp6epCSkoKMjAzo9XrU1dUhKSnJZL9mUpbugrsb+xIRkTnjmZze3l5hb28vurq6RFdXl3BwcBB9fX2zznx2dnYKAKKlpcVku+Oio6PFqlWrpO3R0VHh7Ows1q9fL+3r6ekRAERDQ4MQQoi3335bBAYGGmWry8rKhFKpFGNjY2JoaEgoFApx5MgR6Xh/f79wdHQ0ynxOdu7cOQFADA0NCSFml/kUQggvLy/x8ssvW9TnwcFBYW9vL/bt22eyLksznwDEpUuXpDLZ2dnCyclJ+k5CCKHRaER2drYQQojLly8LGxsb8fPPPxu1FxMTIwoKCszWW1ZWJry8vKTtydf1ViIiIsSePXuEEEKMjIwIDw8Po0zpxMznuK+++koAENevXzdb7wMPPCBKS0ul7cnXZtmyZWLnzp3StlarFenp6UIIIZqbmwUA0dXVZbLuid9vurJ0dzHzSUT0L6dWqxEfH4/KykpUVFQgPj4eHh4esvYhJCRE+tvGxgbu7u4IDg6W9o1n/3p7b84Lq9frER4ebpStjoyMhMFgwNWrV9HR0YHh4WE88sgj0nE3NzcEBgYatdvc3AytVouFCxfCxcUF0dHRAGA2ezZTQgipj9P1Wa/X4++//0ZMTMys2nRycsLixYulbS8vL/j5+UGpVBrtGz+X33//PcbGxhAQEAClUil9vv32W3R0dJit18fHR6pjJtra2nD27FmkpKQAAGxtbZGcnIzy8vIpZSfeFz4+PgD+fw8YDAbk5eUhKCgIKpUKSqUSer3+ltcuKytLyvL/+uuvOHHihJRNDQ0NRUxMDIKDg/HMM89g3759GBgwvTrbTMqS/DjgiIhoDsjIyMCGDRsAAGVlZSbLWFtbT3msODIyckfat7OzM9q2srIy2jcesN24ceOOtAcA165dg0ajgUajwcGDB6FWq9Hd3Q2NRoPh4eFZ19/f34++vj4sWrTIovLjA77Msba+mc+ZeA1Mnf/pzuX4vvFzaTAYYGNjg+bmZthMWtBgYsBqqo7J94MlysvLMTo6ajTASAgBe3t7fPTRR0YDe251D+Tl5aGmpga7d++Gv78/HB0d8fTTT9/y2qWlpWHz5s1oaGjAmTNnsGjRIkRFRQG4+aOnpqYGZ86cwcmTJ1FaWorCwkI0NjZOuYYzKUvyY+aTiGgOiIuLw/DwMEZGRqDRaEyWUavVRu/LDQ4OorOz02ydCoUCADA2NnZnOwsgKCgIDQ0NRsFPfX09XFxcsGDBAixevBh2dnZobGyUjg8MDKC9vV3abm1tRX9/P3bs2IGoqCgsXbr0tjJ55uzduxfW1tbSnKfT9XnJkiVwdHREbW2tyfrUajUAGF2D8+fPz7qfYWFhGBsbQ29vL/z9/Y0+3t7eFtejUCimvdajo6P47LPPoNPpcP78eelz4cIFzJ8/H4cPH7a4vfr6eqSnp+Opp55CcHAwvL290dXVdcv/cXd3R2JiIioqKlBZWTnl3WYrKytERkZi+/btaGlpgUKhwBdffGGyrpmUJXkx80lENAfY2NhAr9dLf5vy2GOPobKyElqtFiqVCsXFxWbLAoCnpyccHR1RXV2NBQsWwMHB4Y5NV/PKK69gz549eO2117Bhwwa0tbVh69ateOONN2BtbQ2lUonMzEy8+eabcHd3h6enJwoLC6XsIQAsXLgQCoUCpaWlyMnJwQ8//IB33nnntvozNDSEX375BSMjI+js7MSBAwfwySef4P3334e/v79FfXZwcEB+fj7eeustKBQKREZGoq+vDz/++CMyMzPh7++P+++/H9u2bcN7772H9vZ26HS6WZ/LgIAApKamIi0tDTqdDmFhYejr60NtbS1CQkIQHx9vUT1+fn74+uuv0dbWBnd3d8ybN29KtvTYsWMYGBhAZmbmlHth7dq1KC8vR05OjkXtLVmyBJ9//jm0Wi2srKywZcsWizLjWVlZWLNmDcbGxvDCCy9I+xsbG1FbW4vHH38cnp6eaGxsRF9fH4KCgqbUMZOyJD9mPon+Q/z8/KZdOcbKyoorhPxLubq6wtXV1ezxgoICREdHY82aNYiPj0diYqLRO4CT2dra4sMPP8THH3+M+fPnIyEh4Y711dfXF8ePH8fZs2cRGhqKnJwcZGZmoqioSCqza9cuREVFQavVIjY2FqtWrcKKFSuk42q1GpWVlTh69CiWLVuGHTt2YPfu3bfVn+LiYvj4+MDf3x/r16/HH3/8gdraWuTn58+oz1u2bEFubi6Ki4sRFBSE5ORkKRtrZ2eHw4cPo7W1FSEhIdi5c6fRSPnZqKioQFpaGnJzcxEYGIjExEScO3duRnO9vvjiiwgMDMTKlSuhVqtRX18/pUx5eTliY2NN/ghZu3YtmpqacPHiRYvaKykpwX333YeIiAhotVpoNBosX7582v+LjY2Fj48PNBqN0aN/V1dXnDp1CqtXr0ZAQACKioqg0+nwxBNPTKljJmVJflxek2gOmW6qoa1bt2Lbtm1mj1uylJ0lyy8SEf1TDAYDfH19UVFRgaSkpLvdHfoH8LE70Rwy8V2yqqoqFBcXo62tTdo3cfABEdFccuPGDfz222/Q6XRQqVR48skn73aX6B/Cx+5Ec4i3t7f0mTdvHqysrKTta9euITU1FV5eXlAqlXjooYfwzTffTKljaGgIKSkpcHZ2hq+vr9mR0+OuXLmCdevWQaVSwc3NDQkJCUaDBurq6vDwww/D2dkZKpUKkZGR0jraRESW6u7uhpeXFw4dOoT9+/fD1pb5sXsVg0+ie4Slq4ns2rULoaGhaGlpwebNm7Fp0ybU1NSYrHN8ZLWLiwtOnz6N+vp6KJVKaeT16OgoEhMTER0djYsXL6KhoQEvvfTSf3YlIiK6fX5+fhBC4MqVK7OeS5X+3fjOJ9EcVVlZiddffx2///672TIPPvggcnJypPkh/fz8EBQUhBMnTkhlnn32WQwODuL48eMAjN/5PHDgAN59913o9XopoBweHoZKpcKXX36JlStXwt3dHXV1ddLk30RERLfCzCfRPcLS1UTCw8OnbI9P4TPZhQsXcOnSJbi4uEirqri5ueGvv/5CR0cH3NzckJ6eDo1GA61Wi7179xq9l0pERDQZX6ggukfczmoi0zEYDFixYgUOHjw45dj4hNoVFRXYuHEjqqurUVVVhaKiItTU1ODRRx+97XaJiOjexeCT6B4xcTUR4GbgaGo1ke+++27KtrmJl5cvX46qqip4enrecn7JsLAwhIWFoaCgAOHh4Th06BCDTyIiMomP3YnuEeOriYwvhffcc8+ZXE2kvr4eH3zwAdrb21FWVoajR49i06ZNJutMTU2Fh4cHEhIScPr0aXR2dqKurg4bN27E1atX0dnZiYKCAjQ0NODy5cs4efIkfvrpJ64iQkREZjHzSXSPKCkpQUZGBiIiIuDh4YH8/HwMDg5OKZebm4umpiZs374drq6uKCkpMbtWuJOTE06dOoX8/HwkJSVhaGgIvr6+iImJgaurK65fv47W1lZ8+umn6O/vh4+PD1599VVkZ2f/01+XiIjmKI52JyIiIiLZ8LE7EREREcmGwScRERERyYbBJxERERHJhsEnEREREcmGwScRERERyYbBJxERERHJhsEnEREREcmGwScRERERyYbBJxERERHJhsEnEREREcmGwScRERERyYbBJxERERHJhsEnEREREcmGwScRERERyYbBJxERERHJhsEnEREREcmGwScRERERyYbBJxERERHJhsEnEREREcmGwScRERERyYbBJxERERHJhsEnEREREcmGwScRERERyYbBJxERERHJhsEnEREREcmGwScRERERyYbBJxERERHJhsEnEREREcnmf6DCrqcqsRGsAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot it as a bar\n",
        "df['label'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "sMns6L5vTkxa",
        "outputId": "447e6161-b30e-4562-dc13-a94637df6403"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='label'>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Putting everything together\n",
        "Lastly we need to gather all our results and organize them in a json file"
      ],
      "metadata": {
        "id": "JMxdH2TnEGTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Initialize a dictionary for storing categorized data\n",
        "output_json = {\n",
        "    \"Tables\": [],\n",
        "    \"Classification\": [],\n",
        "    \"Key Information Extraction\": [],\n",
        "    \"Optical Character Recognition\": [],\n",
        "    \"Datasets\": [],\n",
        "    \"Document Layout Understanding\": [],\n",
        "    \"Others\": [],\n",
        "    \"Deep Learning Models\": [],\n",
        "    \"Multimodal Document Analysis\": [],\n",
        "    \"Handwriting Recognition\": []\n",
        "}\n",
        "\n",
        "# Function to clean the authors string and remove empty/space-only values\n",
        "def clean_authors(authors_string):\n",
        "    # Remove non-alphabetic characters, except for commas\n",
        "    cleaned_string = re.sub(r'[^a-zA-Z, ]+', '', authors_string)\n",
        "    # Split by commas and strip any leading/trailing whitespace from each name\n",
        "    authors_list = [author.strip() for author in cleaned_string.split(',')]\n",
        "    # Remove any empty strings or strings that are just spaces\n",
        "    authors_list = [author for author in authors_list if author]\n",
        "    return authors_list\n",
        "\n",
        "# Iterate through the DataFrame and populate the corresponding category\n",
        "for _, row in df.iterrows():\n",
        "    paper_info = {\n",
        "        \"originalFileName\": row['id'],\n",
        "        \"title\": row['title'],\n",
        "        # Clean and split the authors string\n",
        "        \"authors\": clean_authors(row['authors'])\n",
        "    }\n",
        "\n",
        "    # Append the paper info to the corresponding category\n",
        "    if row['label'] in output_json:\n",
        "        output_json[row['label']].append(paper_info)\n",
        "    else:\n",
        "        output_json['Others'].append(paper_info)  # Default to 'Others' if label is not predefined\n",
        "\n",
        "# Output the JSON structure\n",
        "output_json_str = json.dumps(output_json, indent=4)\n",
        "print(output_json_str)\n",
        "\n",
        "# Save the output to a JSON file\n",
        "with open('categorized_papers.json', 'w') as f:\n",
        "    f.write(output_json_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax_-9NsoENLr",
        "outputId": "71ec05aa-db5a-4a73-8aac-6115d476ccd8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Tables\": [\n",
            "        {\n",
            "            \"originalFileName\": \"0049.pdf\",\n",
            "            \"title\": \"Synthesizing Realistic Data for Table Recognition\",\n",
            "            \"authors\": [\n",
            "                \"Qiyu Houl\",\n",
            "                \"Jun wangl  X PA Meixuan\",\n",
            "                \"Qiao\",\n",
            "                \"and Lujun Tianl\",\n",
            "                \"iWudao Tech\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0027.pdf\",\n",
            "            \"title\": \"Towards EndtoEnd SemiSupervised Table Detection with Semantic Aligned Matching Transformer\",\n",
            "            \"authors\": [\n",
            "                \"Tahira Shehzadi\",\n",
            "                \"X Shalini Sarode\",\n",
            "                \"Didier Stricker\",\n",
            "                \"and Muhammad Zeshan Afza\",\n",
            "                \"Department of Computer Science\",\n",
            "                \"Technical University of Kaiserslautern\",\n",
            "                \"Germany\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0060.pdf\",\n",
            "            \"title\": \"MultiCell Decoder and Mutual Learning for Table Structure and Character Recognition\",\n",
            "            \"authors\": [\n",
            "                \"Takaya Kawakatsui\",\n",
            "                \"Preferred Networks\",\n",
            "                \"Inc\",\n",
            "                \"Otemachi\",\n",
            "                \"Chiyoda\",\n",
            "                \"Tokyo\",\n",
            "                \"Japan\",\n",
            "                \"katniiacjpgmailcom\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0095.pdf\",\n",
            "            \"title\": \"ClusterTabNet Supervised clustering method for table detection and table structure\",\n",
            "            \"authors\": [\n",
            "                \"recognition\",\n",
            "                \"Marek Polewczykl and Marco Spinacil\",\n",
            "                \"SAP Business AT marek  polewczyk\",\n",
            "                \"marco  spinaci sap  com\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0082.pdf\",\n",
            "            \"title\": \"WikiDT Visualbased Table Recognition and Question Answering Dataset\",\n",
            "            \"authors\": [\n",
            "                \"Hui Shit\",\n",
            "                \"Yusheng Xie\",\n",
            "                \"Luis Goncalves\",\n",
            "                \"Sicun Gaol\",\n",
            "                \"and Jishen Zhao\",\n",
            "                \"University of California San Diego\",\n",
            "                \"Amazon AGI\"\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"Classification\": [\n",
            "        {\n",
            "            \"originalFileName\": \"0094.pdf\",\n",
            "            \"title\": \"Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting\",\n",
            "            \"authors\": [\n",
            "                \"Omar Hamedl\",\n",
            "                \"Souhail Bakkali\",\n",
            "                \"Matthew\",\n",
            "                \"Blaschko     X\",\n",
            "                \"Sien Moens\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0018.pdf\",\n",
            "            \"title\": \"A Multiclass Imbalanced Dataset Classification of Symbols from Piping and Instrumentation\",\n",
            "            \"authors\": [\n",
            "                \"Diagrams\",\n",
            "                \"Laura Jamiesonl\",\n",
            "                \"Carlos Francisco MorenoGarciai\",\n",
            "                \"and\",\n",
            "                \"Eyad Elyanl\"\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"Key Information Extraction\": [\n",
            "        {\n",
            "            \"originalFileName\": \"0075.pdf\",\n",
            "            \"title\": \"ConClue Conditional Clue Extraction for Multiple Choice Question Answering\",\n",
            "            \"authors\": [\n",
            "                \"Wangli Yang\",\n",
            "                \"Jie Yangl\",\n",
            "                \"Wanqing Li\",\n",
            "                \"and Yi Guo\",\n",
            "                \"School of Computing and Information Technology\",\n",
            "                \"University of Wollongong\",\n",
            "                \"Australia\"\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"Optical Character Recognition\": [],\n",
            "    \"Datasets\": [\n",
            "        {\n",
            "            \"originalFileName\": \"0036.pdf\",\n",
            "            \"title\": \"A Historical Handwritten Dataset for Ethiopic OCR with Baseline Models and Humanlevel\",\n",
            "            \"authors\": [\n",
            "                \"Performance\",\n",
            "                \"Birhanu Hailu Belays\",\n",
            "                \"Isabelle\",\n",
            "                \"Guyon\",\n",
            "                \"Tadele Mengiste\"\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"Document Layout Understanding\": [\n",
            "        {\n",
            "            \"originalFileName\": \"0045.pdf\",\n",
            "            \"title\": \"UniVIE A Unified Label Space Approach to Visual Information Extraction from Formlike\",\n",
            "            \"authors\": [\n",
            "                \"Documents\",\n",
            "                \"Kai Hul t\",\n",
            "                \"Jiawei Wang\",\n",
            "                \"Weihong Line\",\n",
            "                \"Zhuoyao Zhong\",\n",
            "                \"Lei Sung\",\n",
            "                \"and Qiang Huo\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0059.pdf\",\n",
            "            \"title\": \"GraphMLLM A Graphbased Multilevel Layout Languageindependent Model for\",\n",
            "            \"authors\": [\n",
            "                \"Document Understanding\",\n",
            "                \"HeSen Dai\",\n",
            "                \"XiaoHui Lie\",\n",
            "                \"Fei Yin\",\n",
            "                \"Xudong Yana\",\n",
            "                \"Shuqi Mei\",\n",
            "                \"and\",\n",
            "                \"ChengLin Liu\"\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"Others\": [\n",
            "        {\n",
            "            \"originalFileName\": \"0020.pdf\",\n",
            "            \"title\": \"Improving Automatic Text Recognition with Language Models in the PyLaia OpenSource\",\n",
            "            \"authors\": [\n",
            "                \"Library\",\n",
            "                \"Solene Tarride\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0089.pdf\",\n",
            "            \"title\": \"GlobalSEG Text Semantic Segmentation Based on Global Semantic Pair Relations\",\n",
            "            \"authors\": [\n",
            "                \"Wenjun Sunl\",\n",
            "                \"Hanh Thi Hong\",\n",
            "                \"Tran\",\n",
            "                \"CarlosEmiliano\",\n",
            "                \"GonzalezGallardol\",\n",
            "                \"mithael\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0083.pdf\",\n",
            "            \"title\": \"ImpressionCLIP Contrastive ShapeImpression Embedding for Fonts\",\n",
            "            \"authors\": [\n",
            "                \"Yugo Kubota\",\n",
            "                \"Daichi Haraguchi\",\n",
            "                \"and\",\n",
            "                \"Seiichi Uchida\",\n",
            "                \"Kyushu University\",\n",
            "                \"Fukuoka\",\n",
            "                \"Japan yugo  kubotahuman  ait  kyushuu  ac  j p\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0051.pdf\",\n",
            "            \"title\": \"SourceFree Domain Adaptation for Optical Music Recognition\",\n",
            "            \"authors\": [\n",
            "                \"Adrian Rose\",\n",
            "                \"Eliseo FuentesMartinezi\",\n",
            "                \"Maria AlfaroContreras\",\n",
            "                \"David\",\n",
            "                \"Rizo\",\n",
            "                \"and Jorge CalvoZaragozal\",\n",
            "                \"Pattern Recognition and Artificial Intelligence Group\",\n",
            "                \"University of Alicante\",\n",
            "                \"Spain\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0042.pdf\",\n",
            "            \"title\": \"LMTextSpotter Towards Better Scene Text Spotting with Language Modeling in Transformer\",\n",
            "            \"authors\": [\n",
            "                \"Xin Xia\",\n",
            "                \"Guodong Ding\",\n",
            "                \"and Siyuan Lie\",\n",
            "                \"College of Computer Science and Technology\",\n",
            "                \"Zhejiang University\",\n",
            "                \"HiThink RoyalFlush Information Network Co Ltd\",\n",
            "                \"China\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0043.pdf\",\n",
            "            \"title\": \"Dynamic Relation Transformer for Contextual Text Block Detection\",\n",
            "            \"authors\": [\n",
            "                \"Jiawei Wang t\",\n",
            "                \"Shunchi Zhang\",\n",
            "                \"t\",\n",
            "                \"Kai Hul t\",\n",
            "                \"Chixiang Ma\",\n",
            "                \"Zhuoyao Zhong\",\n",
            "                \"Lei Sun\",\n",
            "                \"and Qiang Huo\",\n",
            "                \"University of Science and Technology of China\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0069.pdf\",\n",
            "            \"title\": \"MOoSE MultiOrientation Sharing Experts for Openset Scene Text Recognition\",\n",
            "            \"authors\": [\n",
            "                \"Chang Liu\",\n",
            "                \"Simon Corbille\",\n",
            "                \"and Elisa H Barney Smith\",\n",
            "                \"Machine Learning Group\",\n",
            "                \"Lulea University of Technology\",\n",
            "                \"Sweden\",\n",
            "                \"changliultuse\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0092.pdf\",\n",
            "            \"title\": \"More and Less Enhancing Abundance and Refining Redundancy for TextPriorGuided\",\n",
            "            \"authors\": [\n",
            "                \"Scene Text Image SuperResolution\",\n",
            "                \"Wei Yang\",\n",
            "                \"Yihong Luo G\",\n",
            "                \"Mayire\",\n",
            "                \"r and Askar Hamdulla\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0055.pdf\",\n",
            "            \"title\": \"End to End Table Transformer Yun Young Choi X, Taehoon Kim  \",\n",
            "            \"authors\": [\n",
            "                \"Namwook Kim   Taehee Lee\",\n",
            "                \"and\",\n",
            "                \"Seongho Joe\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0084.pdf\",\n",
            "            \"title\": \"PrivacyAware Document Visual Question Answering\",\n",
            "            \"authors\": [\n",
            "                \"Ruben Tito\",\n",
            "                \"Khanh Nguyen\",\n",
            "                \"Marlon Tobaben\",\n",
            "                \"Raouf Kerkouche\",\n",
            "                \"Mohamed Ali Souibguil\",\n",
            "                \"Kangsoo June\",\n",
            "                \"Joonas Ja\",\n",
            "                \"Vincent Poulain\",\n",
            "                \"DAndecy\",\n",
            "                \"Aurelie Josephs\",\n",
            "                \"Lei Kang\",\n",
            "                \"Ernest Valvenyl\",\n",
            "                \"Antti Honkela\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0054.pdf\",\n",
            "            \"title\": \"A regionbased approach for layout analysis of music score images in scarce data scenarios\",\n",
            "            \"authors\": [\n",
            "                \"Francisco J Castellanos  Juan P\",\n",
            "                \"MartinezEsteso  Alejandro\",\n",
            "                \"GalanCuenca\",\n",
            "                \"and Antonio Javier\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0081.pdf\",\n",
            "            \"title\": \"Multitask Learning for License Plate Recognition in Unconstrained Scenarios\",\n",
            "            \"authors\": [\n",
            "                \"ZhenLun Mot\",\n",
            "                \"SongLu Chenl\",\n",
            "                \"Qi Liu\",\n",
            "                \"Feng Chen\",\n",
            "                \"and XuCheng Yinl\",\n",
            "                \"University of Science and Technology Beijing\",\n",
            "                \"Beijing\",\n",
            "                \"China\",\n",
            "                \"songluchen\",\n",
            "                \"xuchengyinOustbeducn\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0016.pdf\",\n",
            "            \"title\": \"A RealTime Scene Uyghur Text Detection Network Based on Feature Complementation\",\n",
            "            \"authors\": [\n",
            "                \"Mayire Ibrayim\",\n",
            "                \"Mengmeng Chen\",\n",
            "                \"Askar Hamdulla\",\n",
            "                \"X\",\n",
            "                \"Jianjun Kang\",\n",
            "                \"and Chunhu\",\n",
            "                \"Zhang\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0004.pdf\",\n",
            "            \"title\": \"SAGHOG SelfSupervised Autoencoder for Generating HOG Features for Writer Retrieval\",\n",
            "            \"authors\": [\n",
            "                \"Marco PeerO\",\n",
            "                \"Florian Klebere\",\n",
            "                \"and Robert Sablatnigwo\",\n",
            "                \"Computer Vision Lab\",\n",
            "                \"TU Wien\",\n",
            "                \"mpeer\",\n",
            "                \"kleber\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0077.pdf\",\n",
            "            \"title\": \"Progressive Evolution from SinglePoint to Polygon for Scene Text\",\n",
            "            \"authors\": [\n",
            "                \"Linger Denglt\",\n",
            "                \"Mingxin Huang t\",\n",
            "                \"Xudong Xie\",\n",
            "                \"Yuliang Liul\",\n",
            "                \"Lianwen Jin\",\n",
            "                \"and Xiang Bail\",\n",
            "                \"Huazhong University of Science and Technology\",\n",
            "                \"WuHan\",\n",
            "                \"China\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0038.pdf\",\n",
            "            \"title\": \"Selfsupervised Pretraining of Text Recognizers Martin Kimi  \\u2014  and Michal Hradigi  X\",\n",
            "            \"authors\": [\n",
            "                \"Faculty of Information Technology\",\n",
            "                \"Brno University of Technology\",\n",
            "                \"Brno\",\n",
            "                \"Czech Republic\",\n",
            "                \"ikiss\",\n",
            "                \"hradisfitvutbrcz\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0037.pdf\",\n",
            "            \"title\": \"Drawing the Line Deep Segmentation for Extracting Art from Ancient Etruscan Mirrors\",\n",
            "            \"authors\": [\n",
            "                \"Rafael SterzingerC\",\n",
            "                \"Simon BrennerO\",\n",
            "                \"and Robert Sablatnige\",\n",
            "                \"Computer Vision Lab\",\n",
            "                \"TU Wien\",\n",
            "                \"Vienna\",\n",
            "                \"AUT\",\n",
            "                \"Ifirstnamelastnameltuwienacat\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0064.pdf\",\n",
            "            \"title\": \"Learning to Kern Setwise Estimation of Optimal Letter Space\",\n",
            "            \"authors\": [\n",
            "                \"Kei Nakatsuru and Seiichi Uchida\",\n",
            "                \"Kyushu University\",\n",
            "                \"Fukuoka\",\n",
            "                \"Japan\",\n",
            "                \"uchidaaitkyushuuacjp\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0074.pdf\",\n",
            "            \"title\": \"Oneshot Transformerbased Framework for VisuallyRich Document Understanding\",\n",
            "            \"authors\": [\n",
            "                \"Huynh Vu The  x\",\n",
            "                \"Van Pham Hoail\",\n",
            "                \"and Jeff yang\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0032.pdf\",\n",
            "            \"title\": \"AltChart Enhancing VLMbased Chart Summarization Through MultiPretext Tasks\",\n",
            "            \"authors\": [\n",
            "                \"Omar Moured\",\n",
            "                \"Jiaming Zhang\",\n",
            "                \"M Saquib Sarfrazl\",\n",
            "                \"and Rainer\",\n",
            "                \"Stiefelhagen\",\n",
            "                \"CVHCI lab\",\n",
            "                \"Karlsruhe Institute of Technology\",\n",
            "                \"Germany\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0079.pdf\",\n",
            "            \"title\": \"Font Impression Estimation in the Wild Kazuki Kitajima, Daichi Haraguchi  and\",\n",
            "            \"authors\": [\n",
            "                \"Seiichi Uchida\",\n",
            "                \"Kyushu University\",\n",
            "                \"Fukuoka\",\n",
            "                \"Japan\",\n",
            "                \"uchidaaitkyushuuacjp\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0019.pdf\",\n",
            "            \"title\": \"LightWeight MultiModality Feature Fusion Network for VisuallyRich Document\",\n",
            "            \"authors\": [\n",
            "                \"Understanding\",\n",
            "                \"Jeff yangi\",\n",
            "                \"Huynh Vu Thei  X   and\",\n",
            "                \"Hai Luu Tu  X\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0034.pdf\",\n",
            "            \"title\": \"Mining and Analyzing Statistical Information from Untranscribed Form Images\",\n",
            "            \"authors\": [\n",
            "                \"E\",\n",
            "                \"Jose Andresl\",\n",
            "                \"Alejandro H\",\n",
            "                \"Toselli\",\n",
            "                \"and Enrique Vida\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0031.pdf\",\n",
            "            \"title\": \"Geometricaware Control in Diffusion Model for Handwritten Chinese Font Generation\",\n",
            "            \"authors\": [\n",
            "                \"yao\",\n",
            "                \"Gang\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0022.pdf\",\n",
            "            \"title\": \"CREPE CoordinateAware EndtoEnd Document Parser\",\n",
            "            \"authors\": [\n",
            "                \"Yamato Okamoto\",\n",
            "                \"t\",\n",
            "                \"Youngmin Baek\",\n",
            "                \"Geewook Kiml\",\n",
            "                \"Ryota Nakao\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0080.pdf\",\n",
            "            \"title\": \"Typographic Text Generation with OfftheShelf Diffusion Model\",\n",
            "            \"authors\": [\n",
            "                \"KhayTze Peong\",\n",
            "                \"Seiichi Uchida\",\n",
            "                \"and Daichi\",\n",
            "                \"HaraguchiP\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0047.pdf\",\n",
            "            \"title\": \"Weakly Supervised Training for Hologram Verification in Identity Documents\",\n",
            "            \"authors\": [\n",
            "                \"Glen Pouliquen\",\n",
            "                \"Guillaume Chironle\",\n",
            "                \"Joseph Chazalon G\",\n",
            "                \"Thierry Geraud\",\n",
            "                \"and Ahmad Montaser Awalle\",\n",
            "                \"IDnow AI  ML Center of Excellence\",\n",
            "                \"CessonSevigne\",\n",
            "                \"France\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0050.pdf\",\n",
            "            \"title\": \"GDP Generic Document Pretraining to Improve Document Understanding\",\n",
            "            \"authors\": [\n",
            "                \"Akkshita Trivedil\",\n",
            "                \"Akarsh Upadhyayl\",\n",
            "                \"Rudrabha Mukhopadhyay\",\n",
            "                \"and Santanu\",\n",
            "                \"Chaudhury\"\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"Deep Learning Models\": [\n",
            "        {\n",
            "            \"originalFileName\": \"0044.pdf\",\n",
            "            \"title\": \"  DLAFormer An EndtoEnd Transformer For\",\n",
            "            \"authors\": [\n",
            "                \"Document Layout Analysis\",\n",
            "                \"Jiawei Wang\",\n",
            "                \"t\",\n",
            "                \"Kai Hui\",\n",
            "                \"t\",\n",
            "                \"and Qiang Huo\",\n",
            "                \"Department of EEIS\",\n",
            "                \"University of Science and Technology of China\",\n",
            "                \"Hefei\",\n",
            "                \"China\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0071.pdf\",\n",
            "            \"title\": \"Deep LearningDriven Innovative Model for Generating Functional Knowledge Units\",\n",
            "            \"authors\": [\n",
            "                \"Pan Qiangangl\",\n",
            "                \"Hu Yahongl\",\n",
            "                \"Xie Youbai\",\n",
            "                \"Meng Xianghui\",\n",
            "                \"and Zhang\",\n",
            "                \"Yilun\",\n",
            "                \"Zhejiang University of Technology\",\n",
            "                \"Hangzhou\",\n",
            "                \"Zhejiang\",\n",
            "                \"China\"\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"Multimodal Document Analysis\": [\n",
            "        {\n",
            "            \"originalFileName\": \"0007.pdf\",\n",
            "            \"title\": \"Binarizing Documents by Leveraging both Space and Frequency\",\n",
            "            \"authors\": [\n",
            "                \"Fabio Quattrini\",\n",
            "                \"Vittorio Pippip l\",\n",
            "                \"Silvia Cascianelli\",\n",
            "                \"and Rita Cucchiara X\",\n",
            "                \"University of Modena and Reggio Emilia\",\n",
            "                \"Modena\",\n",
            "                \"Italy\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0025.pdf\",\n",
            "            \"title\": \"   \",\n",
            "            \"authors\": [\n",
            "                \"A Hybrid Approach for Document Layout\",\n",
            "                \"Analysis in Document images\",\n",
            "                \"Tahira Shehzadi\",\n",
            "                \"X  Didier Stricker\",\n",
            "                \"and\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0046.pdf\",\n",
            "            \"title\": \"The Socface Project LargeScale Collection, Processing, and Analysis of a Century of French\",\n",
            "            \"authors\": [\n",
            "                \"Censuses\",\n",
            "                \"Melodie Boilletl\",\n",
            "                \"Solene Tarridei\",\n",
            "                \"Yoann Schneiders\",\n",
            "                \"Bastien Abadiel\",\n",
            "                \"Lionel Kesztenbaum\",\n",
            "                \"and Christopher\"\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"Handwriting Recognition\": [\n",
            "        {\n",
            "            \"originalFileName\": \"0086.pdf\",\n",
            "            \"title\": \"Handwritten Document Recognition Using Pretrained Vision Transformers\",\n",
            "            \"authors\": [\n",
            "                \"Daniel Parresl\",\n",
            "                \"Dan Aniteii\",\n",
            "                \"and\",\n",
            "                \"Roberto Paredes\",\n",
            "                \"PRHLT Research Center\",\n",
            "                \"Universitat Politecnica Valencia\",\n",
            "                \"Valencia\",\n",
            "                \"Spain\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0030.pdf\",\n",
            "            \"title\": \"Visual Prompt Learning for Chinese Handwriting Recognition\",\n",
            "            \"authors\": [\n",
            "                \"Gang yao\",\n",
            "                \"Ning Ding\",\n",
            "                \"Tianqi\",\n",
            "                \"Zhao\",\n",
            "                \"Kemeng Zhao\",\n",
            "                \"pei\",\n",
            "                \"Tang\",\n",
            "                \"Yao Ta    and Liangrui\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0013.pdf\",\n",
            "            \"title\": \"Revisiting NGram Models Their Impact in Modern Neural Networks for Handwritten Text\",\n",
            "            \"authors\": [\n",
            "                \"Recognition\",\n",
            "                \"Solene Tarride  and Christopher\",\n",
            "                \"Kermorvant\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0058.pdf\",\n",
            "            \"title\": \"Analysis of the Calibration of Handwriting Text Recognition Models\",\n",
            "            \"authors\": [\n",
            "                \"Eric Ayllon\",\n",
            "                \"Francisco J Castellanos\",\n",
            "                \"and Jorge CalvoZaragoza\",\n",
            "                \"Pattern Recognition and Artificial Intelligence Group\",\n",
            "                \"University of Alicante\",\n",
            "                \"Spain\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0078.pdf\",\n",
            "            \"title\": \"Test Time Augmentation as a Defense Against Adversarial Attacks on Online Handwriting\",\n",
            "            \"authors\": [\n",
            "                \"Yoh Yamashita and Brian Kenji Iwanaww\",\n",
            "                \"Graduate School of Information Science and Electrical Engineering\",\n",
            "                \"Kyushu University\",\n",
            "                \"Fukuoka\",\n",
            "                \"Japan\"\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"originalFileName\": \"0097.pdf\",\n",
            "            \"title\": \"Reading Order Independent Metrics for Information Extraction in Handwritten\",\n",
            "            \"authors\": [\n",
            "                \"Documents\",\n",
            "                \"David VillanovaAparisii\",\n",
            "                \"Solene\",\n",
            "                \"Tarride\",\n",
            "                \"CarlosD\"\n",
            "            ]\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}